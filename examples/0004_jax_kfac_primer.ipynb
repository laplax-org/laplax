{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a049c892",
   "metadata": {},
   "source": [
    "## KFAC in laplax\n",
    "\n",
    "In the [Laplace Redux](https://arxiv.org/abs/2106.14806) paper, the authors share their laplace approximation library for second order optimization and Bayesian Machine Learning. The corresponding framework [laplace-torch](https://github.com/AlexImmer/Laplace) gives PyTorch implementations for all common curvature matrix- and approximation-types.\n",
    "\n",
    "The goal of the library [laplax](https://github.com/laplax-org/laplax) is to provide the same functionality in pure jax. Additionally, as there isn't one prominent frontend library for deep learning in jax (flax.linen, flax.nnx, equinox, haiku), laplax targets all of these frontends by relying on a purely functional style, in which the user simply specifies a model function $f : \\mathcal D \\times \\mathcal \\Theta \\to \\mathcal Y$, which maps a tuple of inputs and parameters to an output. Further, laplax relies on Matrix-Vector functions, instead of storing matrices in memory. This harmonizes well with jax's functional paradigm. A drawback of this approach becomes apparent when we need to condition on structure of the given model in order to compute approximations. Especially, the Kronecker Factored Approximation to Curvature (KFAC) needs to access layers, store intermediate results and compute gradients w.r.t. those intermediates. This becomes a challenge due to the purely functional nature of laplax. Libraries like [kfac-jax](https://github.com/google-deepmind/kfac-jax) try to infer as much information as possible from the given model, however their approach is for one tailored towards KFAC-Optimizers and on the other hand it's necessary to conform to special (admittedly unavoidable) constraints in order to write a training loop which incorporates their optimizer.\n",
    "\n",
    "We'd like a more lightweight and flexible method of just obtaining the KFAC blocks - or even just the intermediate activations and gradients - without having to add too many dependencies and convolute the solution. In a perfect world, we could write code that magically just does the job:\n",
    "\n",
    "```python\n",
    "from module import intermediates_and_gradients\n",
    "from models import MLP\n",
    "from data import make_dataset\n",
    "\n",
    "model = MLP(din=10, dhidden=5, dout=2)\n",
    "x, y = make_dataset()\n",
    "\n",
    "# computes the pre-relu activations and the gradients of the loss w.r.t. layer outputs\n",
    "activations, grads = intermediates_and_gradients(model, x, y, loss_fn='celoss')\n",
    "```\n",
    "\n",
    "#### What this notebook isn't\n",
    "This notebook is a work in progress, names, locations and functionality will change. It's not a demo of laplax library code. The notebook is supposed to be a jumpstart for everyone interested in writing a stable KFAC implementation for laplax, it's not a polished solution.\n",
    "\n",
    "In case you aren't using laplax and just need the intermediates and the gradients of the intermediate values, take a look at [this](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/module.html#flax.nnx.Module.perturb) documentation page, which gives a method to collect intermediate gradients safely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa35388",
   "metadata": {},
   "source": [
    "## Problem definition - KFAC basics\n",
    "\n",
    "I'll brush over what is a really well explained in Felix Dangel's [kfac-tutorial](https://github.com/f-dangel/kfac-tutorial).\n",
    "\n",
    "Assume a Loss function $\\mathcal L : \\mathcal Y \\times \\mathcal Y \\to \\mathbb R$, and a prediction model (Neural Network) $f_{\\theta} : \\mathcal X \\to \\mathcal Y$. We'd like to use local curvature information around a point in the parameter space to use in our method. However, when the number of parameters is large, computing the exact curvature becomes impossible due to time and memory constraints - we need a good approximation to the curvature. A diagonal approximation - computing the covariance of each parameter with itself - is a good start. However for some applications it may be beneficial to trade off computation for a more accurate curvature approximation. Besides a low-rank approximation to curvature, KFAC offers this.  \n",
    "\n",
    "The KFAC algorithm we'll discuss computes the layerwise covariances. For an MLP $f = A_n \\circ \\phi_{n-1} \\circ A_{n-1} \\circ ... \\circ \\phi_1 \\circ A_1$, where $A_i$ are the linear layers and $\\phi_i$ are the activation functions, it computes the Curvature blocks $C(\\theta^{(i)})$ which relates all parameters *within* a layer. To obtain these approximations, we'll need the inputs to the $i$'th layer $x_i$ and the gradients of the loss w.r.t. the pre-activations $z_i := A_i x_i$ which we'll denote as $g_i := \\frac{\\partial \\mathcal L}{\\partial z_i}$.\n",
    "\n",
    "Once we have those, we can compute the $i$'th layer's curvature for $N$ datapoints as $C(\\theta^{(i)}) \\approx A_i \\otimes B_i$ for which $A_i = \\sum_j^N x_{i,j} x_{i,j}^\\top, \\quad B_i = \\frac{1}{N}\\sum_j^N g_{i,j} g_{i,j}^\\top$ and $\\otimes$ denotes the Kronecker product of two matrices.\n",
    "\n",
    "To categorize the KFAC-flavour we're aiming for: In the notation of F. Dangel, we're computing $\\text{KFAC}_{\\text{exp}}^E(\\text{rvec}(\\tilde W))$ - the KFAC-expand of the Empirical Fisher for the augmented weight vector $\\tilde W$ with the vectorization scheme $\\text{rvec}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213c67c",
   "metadata": {},
   "source": [
    "## Jaxprs and writing Jax Interpreters\n",
    "\n",
    "What makes jax so unique is that its allowing for the composition of function transformations. You might have come across \n",
    "```python \n",
    "jax.jit(jax.vmap(model))(xs) # jit-of-vmap\n",
    "jax.grad(jax.grad(f)) # second derivative\n",
    "```\n",
    "where the vmap and jit function transformations are concatenated. Another view on function transformations is program interpretation. Each function transformation interprets a sub-program to yield another program. The interpretation of a program is a program itself, so a function transformation of a function transformation is possible.\n",
    "\n",
    "A special function transformation which lets us a) inspect the underlying computation invoked by a function and b) lets us access the computation graph is the `jax.make_jaxpr` function transformation. It traces (jax-lingo for interprets) the function you pass it and constructs a sequence of equations which make up a program.\n",
    "\n",
    "A prerequisite to obtaining the necessary values from the jax computation graph is to get an understanding of how to handle jax expressions. [Jaxpr](https://docs.jax.dev/en/latest/jaxpr.html) is a jax-internal expression language for side-effect-less programs which take in and return vectors. Here is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def f(x):\n",
    "    x = x + 1\n",
    "    x = x * 2\n",
    "    x = jax.numpy.sin(x)\n",
    "    print('This is a side-effect!')\n",
    "    return x\n",
    "\n",
    "print(jax.make_jaxpr(f)(jax.numpy.arange(10).astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a2489",
   "metadata": {},
   "source": [
    "What jax is doing under the hood is first tracing, i.e. interpreting the supplied function. While tracing, the function is also executed, which triggers the print statement. In the final jaxpr however we cannot find any print statements. Note that this is also what happens when we jit-compile a function which contains a print statement. During compile-time, the function is traced and the print statement is executed once. Later, when you execute the compiled function, the print statements aren't part of the staged program any more. \n",
    "\n",
    "For the rest of this section, we'll write programs which take the computation tree defined by parsing the function into a jaxpr, then operating on it and inserting operations at the correct stage. Helpful resources for a more in-depth understanding on the inner workings of jax are: [Writing a jax interpreter](https://docs.jax.dev/en/latest/notebooks/Writing_custom_interpreters_in_Jax.html), [Autodidax](https://docs.jax.dev/en/latest/autodidax.html) and [Autodidax 2](https://docs.jax.dev/en/latest/autodidax2_part1.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75da35",
   "metadata": {},
   "source": [
    "\n",
    "### Obtaining layer inputs ($x_{i,j}$)\n",
    "\n",
    "To get a grip of the computation graph, lets first print the jaxpr of a forward pass of an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    def __init__(self, din=5, dmid=4, dout=3, rngs=nnx.Rngs(0)):\n",
    "        super().__init__()\n",
    "        self.lin1 = nnx.Linear(in_features=din, out_features=dmid, rngs=rngs)\n",
    "        self.lin2 = nnx.Linear(in_features=dmid, out_features=dout, rngs=rngs)\n",
    "    def __call__(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "x = jnp.arange(5).astype(float)\n",
    "print(jax.make_jaxpr(model)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3b02f",
   "metadata": {},
   "source": [
    "In this jaxpr, the inputs to the layers are values of the variables e and h. Note that the input variables a to d are the weights and biases of the network. The `custom_jvp_call` corresponds to the relu operation. Next, we'll write a function which can evaluate any given jaxpr. We can show that the jaxprs from the regular function $f$ and the function which evaluates the jaxpr of $f$ are equivalent. We obtain a way of manually traversing the computation graph. This will come in handy when trying to extract the values e and h (or possibly many more values) from the computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56623a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing Jax functions useful for tracing/interpreting.\n",
    "from functools import wraps\n",
    "\n",
    "from jax import lax\n",
    "from jax.extend import core\n",
    "from jax._src.util import safe_map\n",
    "\n",
    "def eval_jaxpr(jaxpr, consts, *args):\n",
    "  # Mapping from variable -> value\n",
    "  env = {}\n",
    "\n",
    "  def read(var):\n",
    "    # Literals are values baked into the Jaxpr\n",
    "    if type(var) is core.Literal:\n",
    "      return var.val\n",
    "    return env[var]\n",
    "\n",
    "  def write(var, val):\n",
    "    env[var] = val\n",
    "\n",
    "  # Bind args and consts to environment\n",
    "  safe_map(write, jaxpr.invars, args)\n",
    "  safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "  for i, eqn in enumerate(jaxpr.eqns):\n",
    "    invals = safe_map(read, eqn.invars)\n",
    "    \n",
    "    if \"call_jaxpr\" in eqn.params: # handle custom definitions (i.e. jax.nn.relu)\n",
    "        subjaxpr = eqn.params[\"call_jaxpr\"]\n",
    "        sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "        \n",
    "        if type(subjaxpr) is core.ClosedJaxpr:\n",
    "            subjaxpr = subjaxpr.jaxpr\n",
    "            sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "\n",
    "        outvals = eval_jaxpr(subjaxpr, sub_consts, *invals)\n",
    "    else:\n",
    "        outvals = eqn.primitive.bind(*invals, **eqn.params)\n",
    "\n",
    "    if not eqn.primitive.multiple_results:\n",
    "        outvals = [outvals]\n",
    "\n",
    "    safe_map(write, eqn.outvars, outvals)\n",
    "  # Read the final result of the Jaxpr from the environment\n",
    "  return safe_map(read, jaxpr.outvars)\n",
    "\n",
    "print('Regular evaluation: ', model(x))\n",
    "closed_jaxpr = jax.make_jaxpr(model)(x)\n",
    "print('Evaluation of jaxpr: ', eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed325",
   "metadata": {},
   "source": [
    "This simple example is taken from [this](https://docs.jax.dev/en/latest/notebooks/Writing_custom_interpreters_in_Jax.html) source. It's the foundation of the implementations that follow, as now we're able to insert custom (and conditional) code into the flow of the program. First, lets make sure that by evaluating the jaxpr, we're evaluating the true function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6746f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the jaxpr of the function which evaluates the jaxpr!\n",
    "f = lambda x : eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, x)[0]\n",
    "print(jax.make_jaxpr(f)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cb57d",
   "metadata": {},
   "source": [
    "Seems alright. Now, lets get to actually collecting the intermediate values we're interested in. For this, all we need to do is realize that the current primitive operation we're applying is a relu (or other non-linearity) and collect the outputs of the current Jaxpr equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def log_jaxpr(jaxpr, consts, *args):\n",
    "    \"\"\"\n",
    "        Given a jax expression which contains dot-products between nn inputs and\n",
    "        weights, log the activations of that computation.\n",
    "\n",
    "        Returns:\n",
    "            accumulator containing the intermediate expressions.\n",
    "    \"\"\"\n",
    "    # Mapping from variable -> value\n",
    "    env = {}\n",
    "    activations = []\n",
    "\n",
    "    def read(var):\n",
    "        # Literals are values baked into the Jaxpr\n",
    "        if type(var) is core.Literal:\n",
    "            return var.val\n",
    "        return env[var]\n",
    "    \n",
    "    def write(var, val):\n",
    "        env[var] = val\n",
    "\n",
    "    # Bind args and consts to environment\n",
    "    safe_map(write, jaxpr.invars, args)\n",
    "    safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "    for i, eqn in enumerate(jaxpr.eqns):\n",
    "        invals = safe_map(read, eqn.invars)\n",
    "        \n",
    "        if \"call_jaxpr\" in eqn.params: # handle custom definitions (i.e. jax.nn.relu)\n",
    "            subjaxpr = eqn.params[\"call_jaxpr\"]\n",
    "            sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "            \n",
    "            if type(subjaxpr) is core.ClosedJaxpr:\n",
    "                subjaxpr = subjaxpr.jaxpr\n",
    "                sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "\n",
    "            outvals, _ = log_jaxpr(subjaxpr, sub_consts, *invals)\n",
    "            activations.append(outvals[0])\n",
    "\n",
    "        else:\n",
    "            outvals = eqn.primitive.bind(*invals, **eqn.params)\n",
    "\n",
    "        if not eqn.primitive.multiple_results:\n",
    "            outvals = [outvals]\n",
    "        \n",
    "        safe_map(write, eqn.outvars, outvals)\n",
    "\n",
    "    return safe_map(read, jaxpr.outvars), activations\n",
    "\n",
    "def log_activations(fun):\n",
    "    @wraps(fun)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n",
    "        flatargs = list(\n",
    "            chain.from_iterable([jax.tree.flatten(arg)[0] for arg in args])\n",
    "        )\n",
    "        out = log_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *flatargs)\n",
    "        return out\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b66429",
   "metadata": {},
   "source": [
    "Lets assert that we're collecting the right values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_with_intermediate(nnx.Module):\n",
    "    \"\"\"For testing purposes\"\"\"\n",
    "    def __init__(self, rngs=nnx.Rngs(0)):\n",
    "        super().__init__()\n",
    "        self.lin1 = nnx.Linear(in_features=5, out_features=4, rngs=rngs)\n",
    "        self.lin2 = nnx.Linear(in_features=4, out_features=3, rngs=rngs)\n",
    "    def __call__(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = nnx.relu(x)\n",
    "        int1 = x.copy()\n",
    "        x = self.lin2(x)\n",
    "        return x, int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLP_with_intermediate(rngs=nnx.Rngs(0))\n",
    "model_with_log = log_activations(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, acts = mlp2(x)\n",
    "logits_log, acts_log = model_with_log(x)\n",
    "\n",
    "print(acts)\n",
    "print(acts_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc32b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the nice thing about this new transformation is that it's (sort-of) compatible with\n",
    "# other jax transformations like vmap and jit.\n",
    "jit_model = jax.jit(jax.vmap(model_with_log))\n",
    "xs = jnp.arange(25).reshape(5, 5)\n",
    "logits, acts = jit_model(xs)\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca347fc8",
   "metadata": {},
   "source": [
    "Thats it for the layer inputs. Next, we'll take a look at how to obtain the gradients of the loss w.r.t. the pre-activations.\n",
    "\n",
    "### Obtaining pre-activation gradients ($g_{i,j}$)\n",
    "\n",
    "To get the pre-activation gradients, we use a commonly used autodiff trick. We're attaching zeros to the computation at the right positions, then letting the gradient flow back to the target positions via regular reverse-mode-autodiff. Here is an example taken from [this](https://github.com/jax-ml/jax/discussions/5336#discussioncomment-269983) post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98024fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  y = jnp.sin(x)\n",
    "  return y ** 2\n",
    "\n",
    "print(jax.grad(f)(3.))  # -0.2794155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_perturbed(x, delta_y):\n",
    "  y = jnp.sin(x)\n",
    "  y = y + delta_y\n",
    "  return y ** 2\n",
    "\n",
    "print(jax.grad(f_perturbed, (0, 1))(3., 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingrad, intergrad = jax.grad(f_perturbed, (0, 1))(3., 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45a991",
   "metadata": {},
   "source": [
    "What we've hardcoded in the cell above we'll implement for any given function $f$. Essentially, we're going to employ the same trick - just augmenting the computation graph by adding new \"perturbation\" input variables. These variables do not contribute to the result, but capture the same gradient as we execute the reverse mode autodiff. The challenge now becomes transforming a jaxpr into a form which adds extra input variables of the right shape, at the right position before executing the `jax.grad` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(jaxpr, consts, perturbations, *args):\n",
    "    \"\"\"\n",
    "        To be differentiated w.r.t. perturbations.\n",
    "\n",
    "        We're running through the same computation graph as `flat_inject` did,\n",
    "        but we've already recorded the shapes of the perturbations and added them\n",
    "        to be invars of this function. This lets us differentiate w.r.t. them.\n",
    "    \"\"\"\n",
    "\n",
    "    env = {}\n",
    "    def read(var):\n",
    "        if type(var) is core.Literal:\n",
    "            return var.val\n",
    "        return env[var]\n",
    "    def write(var, val):\n",
    "        env[var] = val\n",
    "\n",
    "    # Bind args and consts to environment\n",
    "    safe_map(write, jaxpr.invars, args)\n",
    "    safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "    for i, eqn in enumerate(jaxpr.eqns):\n",
    "        invals = safe_map(read, eqn.invars)\n",
    "        if \"call_jaxpr\" in eqn.params:              # if its a relu, record the activation\n",
    "            subjaxpr = eqn.params[\"call_jaxpr\"]\n",
    "            sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "            if type(subjaxpr) is core.ClosedJaxpr:\n",
    "                subjaxpr = subjaxpr.jaxpr\n",
    "                sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "            outvals = eval_jaxpr(subjaxpr, sub_consts, *invals) # assuming that we're just encountering relus\n",
    "        else:                                       \n",
    "            outvals = eqn.primitive.bind(*invals, **eqn.params)\n",
    "        if not eqn.primitive.multiple_results:\n",
    "            outvals = [outvals]\n",
    "        if eqn.primitive.name == 'add':\n",
    "            pert = perturbations.pop(0)\n",
    "            outvals[0] = outvals[0] + pert    \n",
    "        safe_map(write, eqn.outvars, outvals)\n",
    "    return safe_map(read, jaxpr.outvars)[0]\n",
    "\n",
    "\n",
    "def flat_inject(jaxpr, consts, *args):\n",
    "    \"\"\"\n",
    "        We step through the jaxpr and collect post-relu activations.\n",
    "        Additionally, we construct another jaxpr in which we perturb the weights.\n",
    "        This jaxpr is interpreted as a function depending on the perturbation inputs and\n",
    "        can be called with jax.grad to obtain the intermediate gradients.\n",
    "    \"\"\"\n",
    "    \n",
    "    env = {}\n",
    "    perturbations, activations = [], []\n",
    "\n",
    "    def read(var):\n",
    "        if type(var) is core.Literal:\n",
    "            return var.val\n",
    "        return env[var]\n",
    "\n",
    "    def write(var, val):\n",
    "        env[var] = val\n",
    "\n",
    "    # Bind args and consts to environment\n",
    "    safe_map(write, jaxpr.invars, args)\n",
    "    safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "    for eqn in jaxpr.eqns:\n",
    "        invals = safe_map(read, eqn.invars)\n",
    "        if \"call_jaxpr\" in eqn.params:              # if its a relu, record the activation\n",
    "            subjaxpr = eqn.params[\"call_jaxpr\"]\n",
    "            sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "            if type(subjaxpr) is core.ClosedJaxpr:\n",
    "                subjaxpr = subjaxpr.jaxpr\n",
    "                sub_consts = subjaxpr.consts if hasattr(subjaxpr, 'consts') else ()\n",
    "            outvals = eval_jaxpr(subjaxpr, sub_consts, *invals) # assuming that we're just encountering relus\n",
    "            activations.append(outvals[0])\n",
    "        else:                                       \n",
    "            outvals = eqn.primitive.bind(*invals, **eqn.params)\n",
    "        if not eqn.primitive.multiple_results:\n",
    "            outvals = [outvals]\n",
    "        if eqn.primitive.name == 'add':\n",
    "            perturbations.append(jnp.zeros_like(outvals[0]))\n",
    "\n",
    "        safe_map(write, eqn.outvars, outvals)\n",
    "\n",
    "    perturbed_fn = lambda perts, *args : perturb(jaxpr, consts, perts, *args)\n",
    "    \"\"\"\n",
    "        In jax we can differentiate w.r.t. a pytree. This comes in handy, as we can simply\n",
    "        call grad and diff w.r.t. the list of perturbations.\n",
    "    \"\"\"\n",
    "    grads = jax.grad(perturbed_fn)(perturbations, *args)\n",
    "\n",
    "    return activations, grads\n",
    "\n",
    "def intergrad(fun):\n",
    "    \"\"\"\n",
    "        Flattens the input values, then delegates.\n",
    "    \"\"\"\n",
    "    @wraps(fun)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n",
    "        flatargs = list(\n",
    "            chain.from_iterable([jax.tree.flatten(arg)[0] for arg in args])\n",
    "        )\n",
    "        out = flat_inject(closed_jaxpr.jaxpr, closed_jaxpr.literals, *flatargs)\n",
    "        return out\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f94a09",
   "metadata": {},
   "source": [
    "What I've added here is already the collection of the activations and the gradients combined, as we have to run through the program once to observe the shapes of the pre-activations anyways. We're left with a jax transformation which yields back the intermediate layer-inputs and the pre-activation gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607104c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "_graph, _params = nnx.split(model)\n",
    "\n",
    "x, y = jnp.ones(5), jax.nn.one_hot(1, num_classes=3)\n",
    "\n",
    "def model_fn(params, x):\n",
    "    return nnx.merge(_graph, params)(x)\n",
    "\n",
    "def celoss(params, x, y):\n",
    "    ypred = jax.nn.log_softmax(model_fn(params, x), axis=-1)\n",
    "    loss = -(y * ypred).mean()\n",
    "    return loss\n",
    "\n",
    "acts, grads = intergrad(celoss)(_params, x, y)\n",
    "shape = lambda x: jax.tree.map(lambda y: y.shape, x)\n",
    "print(shape(acts), shape(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf7da0",
   "metadata": {},
   "source": [
    "Seems to work. Lets make sure that the gradients we're getting are correct. For this, I'll refer to the `test_intergrad.py` module in which I compare this implementation to the `Module.perturb` functionality provided by nnx.\n",
    "\n",
    "## Constructing the KFAC blocks\n",
    "\n",
    "Next up, lets demonstrate that the KFAC blocks we're computing coincide with the true Empirical Fisher. We'll train a model on a subset of MNIST, then compute the Empirical Fisher and the KFAC Blocks, then compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.1)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "xtrain, xtest = jnp.array(xtrain, dtype=jnp.float32), jnp.array(xtest, dtype=jnp.float32)\n",
    "ytrain, ytest = jax.nn.one_hot(ytrain, num_classes=2), jax.nn.one_hot(ytest, num_classes=2)\n",
    "\n",
    "model = MLP(din=2, dmid=5, dout=2, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, tx=optax.adam(learning_rate=0.01))\n",
    "\n",
    "def dataloader(x, y, batchsize=32, key=None):\n",
    "    \"\"\"Yield batches of data.\"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    indices = jnp.arange(n_samples)\n",
    "    if key is not None:\n",
    "        indices = jax.random.permutation(key, indices)\n",
    "\n",
    "    for start in range(0, n_samples, batchsize):\n",
    "        end = min(start + batchsize, n_samples)\n",
    "        batch_indices = indices[start:end]\n",
    "        yield x[batch_indices], y[batch_indices]\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model, x, y):\n",
    "        ypred = jax.nn.log_softmax(model(x), axis=-1)\n",
    "        loss = -(ypred * y).mean()\n",
    "        return loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n",
    "    optimizer = optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "key=jax.random.PRNGKey(0)\n",
    "for epoch in range(1, 50 + 1):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    for xs, ys in dataloader(xtrain, ytrain, key=subkey):\n",
    "        loss = train_step(model, optimizer, xs, ys)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: \", epoch, f\" Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3977d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the kfac blocks\n",
    "_graph, _params = nnx.split(model)\n",
    "\n",
    "num_samples = 1\n",
    "xb, yb = xtrain[:num_samples], ytrain[:num_samples]\n",
    "\n",
    "def model_fn(params, x):\n",
    "    return nnx.merge(_graph, params)(x)\n",
    "\n",
    "def celoss(params, x, y):\n",
    "    ypred = jax.nn.log_softmax(model_fn(params, x), axis=-1)\n",
    "    loss = -(y * ypred).mean()\n",
    "    return loss\n",
    "\n",
    "acts, grads = jax.vmap(intergrad(celoss), in_axes=(None, 0, 0))(_params, xb, yb)\n",
    "acts = [xb] + acts # prepend the original inputs to the activations.\n",
    "acts = jax.tree.map( # prepend a column of ones to the activations (for the bias)\n",
    "    lambda a : jnp.concatenate([jnp.ones((a.shape[0], 1)), a], axis=-1),\n",
    "    acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(acts), shape(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "As, Bs = jax.tree.map(\n",
    "    lambda x: x.T @ x, (acts, grads)\n",
    ")\n",
    "Bs = jax.tree.map(lambda x: x / num_samples , Bs) # normalize the Bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = jax.tree.map(\n",
    "    lambda a, b: jnp.kron(a, b),\n",
    "    As, Bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Empirical Fisher\n",
    "grads = jax.vmap(nnx.grad(celoss), in_axes=(None, 0, 0))(_params, xb, yb) # has shapes [b, d] and [b, d1, d2]\n",
    "grads = jax.tree.map(lambda x: x.reshape(x.shape[0], -1), jax.tree.leaves(grads))\n",
    "grads = jnp.concat(grads, axis=-1)\n",
    "EFisher = grads.T @ grads / grads.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.imshow(EFisher)\n",
    "ax1.set_title('Empirical Fisher')\n",
    "\n",
    "kfac = jnp.zeros_like(EFisher)\n",
    "idx = 0\n",
    "for b in blocks:\n",
    "    kfac = kfac.at[idx: idx+b.shape[0], idx: idx+b.shape[1]].set(b)\n",
    "    idx += b.shape[0]\n",
    "ax2.imshow(kfac)\n",
    "ax2.set_title('KFAC Blocks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfe237",
   "metadata": {},
   "source": [
    "We can see that as described in F. Dangel's tutorial, for a single Datapoint, the blocks of the Fisher and the KFAC conincide. If we repeat for more datapoints, we start to see differences.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "A limitation of the current implementation of `intergrad` is that we assume the network only contains Linear layers with bias. There isn't a mechanism to \"parse\" out layers for which KFAC is supported like in `kfac-jax` or `laplace-torch` for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362c8ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
