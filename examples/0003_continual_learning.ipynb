{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a660b69",
   "metadata": {},
   "source": [
    "#### Continual Learning Tutorial\n",
    "\n",
    "In this notebook, we'll show how to use `laplax` in a simple continual learning context. We'll first show that by just continuing to train on new data distributions, we \"forget\" past distributions. Then, we'll briefly introduce a certain flavour of curvature matrix, before giving a concise implementation. Finally, we'll discuss the trade-off between stability and plasticity which is offered by the simple regularization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9297e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.util.datasets import mnist, minimnist, permute, DataLoader, collect\n",
    "from typing import *\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    We're training on minimnist, which is a small subset of MNIST.\n",
    "    If you want to use the full MNIST dataset, you can use the `mnist` function instead.\n",
    "\"\"\"\n",
    "\n",
    "trainloader, testloader, num_train, num_test = minimnist()\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    def __init__(self, rngs : nnx.Rngs, in_size: int= 28*28, hidden_size: int=10, num_classes:int=10):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nnx.Linear(in_features=in_size, out_features=hidden_size, rngs=rngs)\n",
    "        self.hidden2 = nnx.Linear(in_features=hidden_size, out_features=hidden_size, rngs=rngs)\n",
    "        self.hidden3 = nnx.Linear(in_features=hidden_size, out_features=num_classes, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.hidden1(x))\n",
    "        x = nnx.relu(self.hidden2(x))\n",
    "        x = self.hidden3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree as jt\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def update(model, optimizer, x, y):\n",
    "    def cross_entropy(model, x, y):\n",
    "        logits = model(x)\n",
    "        ypred = jax.nn.log_softmax(logits)\n",
    "        loss = -(ypred * y).mean()\n",
    "        return loss\n",
    "    loss, grads = nnx.value_and_grad(cross_entropy)(model, x, y)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "def evaluate(model : nnx.Module, _loader : DataLoader, seeds : Sequence[int]):\n",
    "    \"\"\"\n",
    "        Evaluates a model on a set of permuted dataloaders. Note: seed=0 is the non-permuted dataloader.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for seed in seeds:\n",
    "        correct = 0\n",
    "        loader = permute(_loader, seed=seed)\n",
    "        for x, y in loader:\n",
    "            logits = model(x)\n",
    "            preds = jnp.argmax(logits, axis=-1)\n",
    "            correct += jnp.sum(jnp.argmax(y, axis=-1) == preds)\n",
    "        ret.append(correct / loader.num_samples)\n",
    "    return ret\n",
    "\n",
    "def train_epoch(model, optimizer, trainloader):\n",
    "    loss_history = []\n",
    "    for x, y in trainloader:\n",
    "        loss = update(model, optimizer, x, y)\n",
    "        loss_history.append(loss)\n",
    "    return loss_history\n",
    "\n",
    "def train(model, optimizer, trainloader, testloader, eval_seeds, num_epochs=10, evaluate=False):\n",
    "    \"\"\"\n",
    "        Train the model for a number of epochs, evaluating on the test set for eval_seeds at the end of each epoch.\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        losses = train_epoch(model, optimizer, trainloader)\n",
    "        history.extend(losses)\n",
    "        if evaluate and epoch % 10 == 0:\n",
    "            test_accuracy = evaluate(model, testloader, seeds=eval_seeds)\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, \",\n",
    "                  f\"Test Accuracy: {test_accuracy}\")\n",
    "            \n",
    "    return model, optimizer, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model=model, tx = optax.adamw(learning_rate=1e-4, weight_decay=5e-4))\n",
    "model, optimizer, history = train(model, optimizer, trainloader, testloader, \n",
    "                                  num_epochs=100, eval_seeds=[0], evaluate=False)\n",
    "accuracy = evaluate(model, testloader, seeds=[0])\n",
    "print(f\"Test-Accuracy on MNIST without permutation: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b50451",
   "metadata": {},
   "source": [
    "Sanity check passed. We trained on a subset of mnist and got ~87% accuracy with a three-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c8dd2a",
   "metadata": {},
   "source": [
    "#### Continual Learning\n",
    "\n",
    "We're permuting the inputs while keeping the labels static:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple plot which shows one sample of the training data (MNIST number) and its permutation in the next row\n",
    "import matplotlib.pyplot as plt\n",
    "def split_loader(loader):\n",
    "    X, y = loader.X, jnp.argmax(loader.y, axis=-1)\n",
    "    xs = [X[y==i] for i in range(10)]\n",
    "    return xs\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "imgs = []\n",
    "for seed in seeds:\n",
    "    xs = split_loader(permute(testloader, seed=seed))\n",
    "    imgs.extend([x[0].reshape(28, 28) for x in xs]) # take first sample\n",
    "\n",
    "fig, axs = plt.subplots(3, 10, figsize=(10, 3), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5211669",
   "metadata": {},
   "source": [
    "#### Continual Learning Primer\n",
    "\n",
    "In the MNIST Permute setup, we're continuously training on permuted samples from MNIST. Whilst the training\n",
    "data distribution changes, we're keeping track of performance on past distributions to measure the *plasticity* - i.e. the ability to adapt and learn new information over time - and the *stability* - i.e. the ability to retain information about past training data - of the model. \n",
    "\n",
    "In this notebook, we want to understand the influence of regularizers on the stability and plasticity of Neural Networks. First, let us see the influence on the stability with no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_seeds = jnp.arange(5) # we want 5 different \"tasks\" - they correspond to permutations of the initial MNIST\n",
    "model = MLP(rngs=nnx.Rngs(0)) # initilalize the model once\n",
    "optimizer = nnx.Optimizer(model=model, tx = optax.adamw(learning_rate=1e-4, weight_decay=5e-4))\n",
    "loss_history, previous_accuracies = [], []\n",
    "\n",
    "for i, seed in enumerate(task_seeds):\n",
    "    cur_trainloader = permute(trainloader, seed=seed)\n",
    "    model, optimizer, history = train(model, optimizer, cur_trainloader, testloader, \n",
    "              num_epochs=100, eval_seeds=task_seeds[:i+1], evaluate=False)\n",
    "    acc = evaluate(model, testloader, seeds=task_seeds[:i+1])\n",
    "    loss_history.extend(history); previous_accuracies.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f59eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def plot(accuracies, history, title, task_seeds=None):\n",
    "    tmp = deepcopy(accuracies)\n",
    "    num_tasks = 5 if task_seeds is None else len(task_seeds)\n",
    "    transposed_accuracies = [[l.pop(0) for l in tmp if len(l) > 0] for _ in range(num_tasks)]\n",
    "\n",
    "    # Use higher DPI for better quality\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), dpi=150)\n",
    "\n",
    "    # --- Loss plot ---\n",
    "    ax1.plot(history, color='black')\n",
    "    ax1.set_title(\"Loss over Time / Different Tasks\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # --- Accuracy plot ---\n",
    "    markers = [\"o\", \"v\", \"^\", \"s\", \"P\"]\n",
    "    colors = plt.colormaps.get_cmap('tab10')\n",
    "\n",
    "    for i, task_accs in enumerate(transposed_accuracies):\n",
    "        xpos = list(range(i, len(transposed_accuracies)))\n",
    "        ax2.plot(xpos, task_accs, color=colors(i), alpha=0.5)\n",
    "        ax2.scatter(xpos, task_accs, marker=markers[i], facecolor=colors(i), edgecolor='black', linewidth=0.8)\n",
    "\n",
    "    # Custom legend\n",
    "    handles = [\n",
    "        plt.Line2D(\n",
    "            [0], [0], marker=markers[i], color='w', label=f'Task {i}',\n",
    "            markerfacecolor=colors(i), markeredgecolor='black', markersize=9\n",
    "        ) for i in range(num_tasks)\n",
    "    ]\n",
    "    ax2.legend(handles=handles, loc='lower left')\n",
    "\n",
    "    ax2.grid(True, axis='y')\n",
    "    ax2.set_title(title)\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.set_xlabel(\"Training Task\")\n",
    "\n",
    "    # Set xticks according to the number of tasks\n",
    "    xticks = list(range(len(transposed_accuracies)))\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels([f'Task {i}' for i in xticks])\n",
    "    ax2.set_ylim(0.5, 0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot(previous_accuracies, loss_history, title='Accuracy per Task Over Continual Training (No Regularization)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f9fcb",
   "metadata": {},
   "source": [
    "#### Discussion and where to go next \n",
    "\n",
    "We can see that the performance for previous tasks when continuously training the model without any regularization drops. The current task however is always learned. We can notice that we might be able to trade off plasticity for stability - this is what we'll try to do next: We will approximate the curvature around the found mode using the empirical Fisher and use this information as a regularizer for the downstream tasks. We will show that even with a simple diagonal approximation to curvature we can retain most of the plasticity while gaining a lot of stability in our setup.\n",
    "\n",
    "The new loss formulation is:\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\theta; \\theta_{i-1}, x, y) = \\texttt{CE}(\\hat y, y) + (\\theta_{i - 1} - \\theta)^\\top \\Psi (\\theta_{i - 1} - \\theta)\n",
    "$$\n",
    "\n",
    "The regularization term re-scales the distance between the current and the previous sets of parameters by the curvature $\\Psi$. Changes in the directions in parameter space with high curvature will incur a high loss - hence the regularization term induces a bias towards keeping a certain set of high curvature directions more static then others. After converging on each training task $i$, we will compute a function $\\texttt{inner\\_fn}$ which takes in the distance $d := \\theta_{i - 1} - \\theta_{i}$ and computes $d^\\top\\hat \\Psi d$ where $\\hat \\Psi$ is an approximation to the true curvature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f556339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.util.tree import sub, dot   # pytree utiliy functions\n",
    "\n",
    "def update(model, optimizer, x, y, mode, inner_fn, reg_lambda=1e-4):\n",
    "    def cross_entropy(model, x, y):\n",
    "        logits = model(x)\n",
    "        ypred = jax.nn.log_softmax(logits)\n",
    "        loss = -(ypred * y).mean()\n",
    "        return loss\n",
    "    \n",
    "    def regularization(model):\n",
    "        if not inner_fn:\n",
    "            return 0.0\n",
    "        \n",
    "        _, params = nnx.split(model)\n",
    "        d = sub(mode, params)\n",
    "        reg = inner_fn(d) # this computes the scaled inner product\n",
    "        return reg\n",
    "    \n",
    "    def total_loss(model, x, y):\n",
    "        ce_loss = cross_entropy(model, x, y)\n",
    "        reg_loss = regularization(model)\n",
    "        return ce_loss + reg_lambda * reg_loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(total_loss)(model, x, y)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, trainloader, testloader, \n",
    "          eval_seeds, mode, inner_fn, \n",
    "          reg_lambda=1e-4, num_epochs=100, evaluate=False):\n",
    "    \"\"\"\n",
    "        Train the model for a number of epochs, evaluating on the test set for eval_seeds at the end of each epoch.\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    # recompile the update function when inner_fn changes\n",
    "    jit_update = nnx.jit(update, static_argnames=('inner_fn')) \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for x, y in trainloader:\n",
    "            loss = jit_update(model=model, \n",
    "                               optimizer=optimizer, \n",
    "                               x=x, y=y,\n",
    "                               mode=mode,\n",
    "                               inner_fn=inner_fn, reg_lambda=reg_lambda)\n",
    "            \n",
    "            history.append(loss)\n",
    "\n",
    "        if evaluate and epoch % 10 == 0:\n",
    "            test_accuracy = evaluate(model, testloader, seeds=eval_seeds)\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, \",\n",
    "                  f\"Test Accuracy: {test_accuracy}\")\n",
    "            \n",
    "    return model, optimizer, history\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153aae16",
   "metadata": {},
   "source": [
    "#### Defining the Inner Product $(\\theta_{i - 1} - \\theta)^\\top \\Psi (\\theta_{i - 1} - \\theta)$\n",
    "Lets define a factory function, which given a current model mode ($\\theta_i$) and a set of training samples, computes the $\\texttt{inner\\_fn}$ we will use in the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f026f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_fisher_inner(model, trainloader, maxsamples, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes the empirical Fisher information inner product function for a model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        trainloader: DataLoader providing training data.\n",
    "        maxsamples: Number of samples to use from the loader.\n",
    "        *args, **kwargs: Additional arguments (unused).\n",
    "\n",
    "    Returns:\n",
    "        inner: A function that computes the empirical Fisher inner product with a vector v.\n",
    "    \"\"\"\n",
    "    def cross_entropy(model, x, y):\n",
    "        log_y_pred = jax.nn.log_softmax(model(x))\n",
    "        return -(log_y_pred * y).mean()\n",
    "\n",
    "    xb, yb = collect(trainloader, maxsamples=maxsamples)\n",
    "    grads = nnx.grad(cross_entropy)(model, xb, yb)\n",
    "    sqgrads = jax.tree.map(lambda x: jnp.mean(x**2, axis=0), grads)  # square and mean\n",
    "\n",
    "    def inner(v, sqgrads=sqgrads):\n",
    "        return dot(v, jax.tree.map(lambda x, y: x * y, v, sqgrads))\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   NOTE: In case you're running out of memory, use jax.clear_caches() in the loop \n",
    "\"\"\" \n",
    "model = MLP(rngs=nnx.Rngs(0))  # initialize the model once\n",
    "optimizer = nnx.Optimizer(model=model,\n",
    "                          tx=optax.adamw(learning_rate=1e-4, weight_decay=5e-4))\n",
    "\n",
    "# these numbers I've found to be a good trade-off in a gridsearch\n",
    "CURV_APPROX_SAMPLES = 128\n",
    "LAMBDA = 1000\n",
    "mode, inner_fn = nnx.split(model)[1], None\n",
    "task_seeds = jnp.arange(5)\n",
    "previous_accuracies, loss_history = [], []\n",
    "for i, seed in enumerate(task_seeds):\n",
    "   cur_trainloader = permute(trainloader, seed=seed)\n",
    "   model, optimizer, history = \\\n",
    "   train(model=model,\n",
    "         optimizer=optimizer,\n",
    "         trainloader=cur_trainloader,\n",
    "         testloader=testloader,\n",
    "         eval_seeds=None,\n",
    "         evaluate=False,\n",
    "         mode=mode,\n",
    "         inner_fn=inner_fn,\n",
    "         reg_lambda=LAMBDA)\n",
    "   acc = evaluate(model, testloader, seeds=task_seeds[:i+1])\n",
    "   print(f\"Task {i}, Test Accuracy: {acc}\")\n",
    "   previous_accuracies.append(acc)\n",
    "   loss_history.extend(history)\n",
    "\n",
    "   # post training: get new mode and compute the inner_fn\n",
    "   mode = nnx.split(model)[1]\n",
    "   inner_fn = emp_fisher_inner(model, cur_trainloader, \n",
    "                        maxsamples=CURV_APPROX_SAMPLES)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48675afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(previous_accuracies, loss_history, title='Accuracy per Task Over Continual Training (Empirical Fisher Diagonal)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d3844",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "As we can see by comparing both plots, the Empirical-Fisher-regularized version does perform quite well.\n",
    "\n",
    "Currently, we're using a diagonal approximation to the Fisher, as building the full matrix may be quite expensive. We can however try to find an approximation which lies between the diagonal and full matrix, trading off computation and memory for performance. Common choices are a low-rank approximation or a Kroenecker Factored approximation to curvature (KFAC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.experimental.innerprods import kfac_inner_fn\n",
    "\n",
    "model = MLP(rngs=nnx.Rngs(0))  # initialize the model once\n",
    "optimizer = nnx.Optimizer(model=model,\n",
    "                          tx=optax.adamw(learning_rate=1e-4, weight_decay=5e-4))\n",
    "graph_def, _ = nnx.split(model)\n",
    "\n",
    "def model_fn(params, input):\n",
    "    return nnx.call((graph_def, params))(input)[0]\n",
    "\n",
    "# these numbers I've found to be a good trade-off in a gridsearch\n",
    "CURV_APPROX_SAMPLES = 128\n",
    "LAMBDA = 3500\n",
    "mode, inner_fn = nnx.split(model)[1], None\n",
    "task_seeds = jnp.arange(5)\n",
    "previous_accuracies, loss_history = [], []\n",
    "for i, seed in enumerate(task_seeds):\n",
    "   cur_trainloader = permute(trainloader, seed=seed)\n",
    "   model, optimizer, history = \\\n",
    "   train(model=model,\n",
    "         optimizer=optimizer,\n",
    "         trainloader=cur_trainloader,\n",
    "         testloader=testloader,\n",
    "         eval_seeds=None,\n",
    "         evaluate=False,\n",
    "         mode=mode,\n",
    "         inner_fn=inner_fn,\n",
    "         reg_lambda=LAMBDA)\n",
    "   acc = evaluate(model, testloader, seeds=task_seeds[:i+1])\n",
    "   print(f\"Task {i}, Test Accuracy: {acc}\")\n",
    "   previous_accuracies.append(acc)\n",
    "   loss_history.extend(history)\n",
    "\n",
    "   # post training: get new mode and compute the inner_fn\n",
    "   _, mode = nnx.split(model)\n",
    "   x, y = collect(cur_trainloader, maxsamples=CURV_APPROX_SAMPLES)\n",
    "   data = {'input' : x, 'target' : y}\n",
    "   inner_fn = kfac_inner_fn(params=mode, model_fn=model_fn, data=data)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(previous_accuracies, loss_history , title='Accuracy per Task Over Continual Training (KFAC-emp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9948e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
