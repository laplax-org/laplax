{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Laplace Approximations in JAX.</p> <p>The <code>laplax</code> package provides a performant, minimal, and practical implementation of Laplace approximation techniques in <code>jax</code>. The package is designed to support a wide range of scientific libraries, initially focusing on compatibility with popular neural network libraries such as <code>equinox</code>, <code>flax.linen</code>, and <code>flax.nnx</code>. Our goal is to create a flexible tool for both practical applications and research, enabling rapid iteration and comparison of new approaches.</p>"},{"location":"#installation","title":"Installation","text":"<p>Use <code>pip install laplax</code>.</p>"},{"location":"#minimal-example","title":"Minimal example","text":"<p>The following tiny <code>laplax</code> example shows how to use the <code>laplax</code> package to perform a linearized Laplace approximation on a two-parameter ReLU network \\(\\mathcal{D}(x,\\theta)=\\theta_2\\,\\text{ReLU}(\\theta_1 x+1)\\) for \\(\\mathcal{D}=\\{(1,-1),(-1,-1)\\}\\) and visualize the weight space uncertainty in the loss landscape.</p> <p>Gray contours: energy with square loss; black dot: optimum \\(\\theta^*\\); green ellipses: \\(1\\sigma\\) and \\(2\\sigma\\) levels of the Laplace approximation.</p> <pre><code>from jax.nn import relu\nfrom jax.numpy import array\nfrom laplax import laplace\nfrom plotting import plot_figure_1\n\n# You need a model...\ndef model_fn(input, params):\n    return relu(params[\"theta1\"] * input - 1) * params[\"theta2\"]\nparams = { # optimized weights,\n    \"theta1\": array(1.6556547), \"theta2\": array(1.0420421)\n}\ndata = {  # and training data.\n    \"input\": array([1., -1.]), \"target\": array([1., -1.])\n}\n\n# ... then apply laplax ...\nposterior_fn, _ = laplace(\n    model_fn, params, data, loss_fn=\"mse\", curv_type=\"full\",\n)\ncurv = posterior_fn({\"prior_prec\": 0.2}).state['scale']\n\n# ... to get Figure 1.\nplot_figure_1(model_fn, params, curv)\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>We provide a high-level interface for performing Laplace approximation on a model and expose additional is low-level building blocks. As working examples, we include both approaches as tutorials:</p> <ul> <li>Tiny example (cf. plot above)</li> <li>Laplax for regression</li> <li>Laplax on MNIST</li> </ul> <p>Both APIs and all available options are documented in the Manual. For each submodule, we provide a short overview as well as a comprehensive list of all available functions.</p> <p>For a general starting point for details on Laplace approximation and our notations, we refer to the Background section. Most of the documentation follows our recent workshop paper.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>laplax</code> in your research, please cite for now:</p> <pre><code>@software{laplax,\n    author = {Tobias Weber, B\u00e1lint Mucs\u00e1nyi, Lenard Rommel, Thomas Christie, Lars Kas\u00fcschke, Marvin Pf\u00f6rtner, Philipp Hennig},\n    title = {Laplax: Laplace Approximations in JAX},\n    year = {2025},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/laplax-org/laplax}},\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the Laplax API reference.</p>"},{"location":"api/#package-overview","title":"Package Overview","text":"<p>The <code>laplax</code> package contains a high-level API, that is designed to be used out-of-the box, and a modular low-level API, which provides exposes all essential building blocks for the high-level API and can be used for fast experimentations. The low-level API is organized into the following modules:</p> <ul> <li><code>laplax.curv</code>: Tools for computing and approximating curvature information</li> <li><code>laplax.eval</code>: Evaluation metrics and utilities for assessing predictive uncertainty</li> <li><code>laplax.util</code>: Various utilities for working with PyTrees, DataLoaders, and other common utilities.</li> </ul>"},{"location":"api/#main-design-decisions","title":"Main design decisions","text":""},{"location":"api/#model-function-signature","title":"Model function signature","text":"<p><code>laplax</code> operates by taking an arbitrary <code>model_fn</code> with (key-word) signature <code>model_fn(input, params)</code>. This allows for a wide range of JAX-based neural network libraries to be used. For <code>flax.nnx</code> and <code>equinox</code>, this would look like:</p> flax.nnxequinox <pre><code>from flax import nnx\n\nmodel = ...\n\ngraph_def, params = nnx.split(model)\n\ndef model_fn(input, params):\n    return nnx.call((graph_def, params))(input)[0]\n</code></pre> <pre><code>from equinox import filter_jit\n\nmodel = ...\n\nparams, static = eqx.partition(model, eqx.is_array)\n\ndef model_fn(input, params):\n    model = eqx.combine(params, static)\n    return model(input)\n</code></pre>"},{"location":"background/","title":"Turning neural networks Bayesian","text":""},{"location":"background/#probabilistic-perspective-on-supervised-learning","title":"Probabilistic perspective on supervised learning","text":"<p>Given labelled training data \\(\\mathcal{D} = \\{(x_n, y_n)\\}_{n=1}^N\\), loss function \\(\\ell\\) and regularizer \\(\\Omega\\), the parameters \\(\\theta\\) of a neural network \\(f_\\theta\\) are typically obtained by minimising the regularised empirical risk \\(\\mathcal{L}(\\mathcal{D}, f_\\theta)\\). From a probabilistic perspective, this procedure corresponds to finding a maximum a posteriori (MAP) estimate of the weights under a likelihood and prior. Formally, both views lead to the following optimisation problem:</p> \\[ \\begin{aligned} \\theta^* &amp;= \\operatorname*{arg\\,min}_{\\theta} \\mathcal{L} (\\theta; \\mathcal{D}) \\\\          &amp;= \\operatorname*{arg\\,min}_{\\theta} \\underbrace{\\sum_{n=1}^{N} \\ell(f_\\theta(x_n), y_n) + \\Omega(\\theta)}_{\\mathcal{L}(\\mathcal{D}, f_\\theta)} \\\\          &amp;= \\operatorname*{arg\\,max}_{\\theta} \\sum_{n=1}^{N} \\log p(y_n \\vert f_\\theta(x_n)) + \\log p(\\theta)\\, . \\end{aligned} \\] <p>The weight-space uncertainty is then described by the posterior distribution given the training data:</p> \\[ p(\\theta \\vert \\mathcal{D}) = \\frac{ p(\\mathcal{D} \\vert \\theta)\\, p(\\theta)}{\\int p(\\mathcal{D} \\vert \\theta)\\,p(\\theta)\\,d\\theta}\\; . \\] <p>However, for deep neural networks, the integral in the denominator is usually intractable. The Laplace approximation circumvents this by utilising a Gaussian distribution to approximate the posterior. To this end, we apply a second-order Taylor approximation to the negative log-posterior loss \\(\\mathcal{L}\\) around the MAP estimate \\(\\theta^*\\), which yields</p> \\[ \\mathcal{L}(\\mathcal{D}, f_\\theta) \\approx \\mathcal{L}(\\mathcal{D}, f_{\\theta^*}) + \\nabla_\\theta \\mathcal{L}(\\mathcal{D}, f_{\\theta^*})^\\top (\\theta - \\theta^*) + \\frac{1}{2} (\\theta - \\theta^*)^\\top\\nabla^2_{\\theta \\theta} \\mathcal{L}(\\mathcal{D}, f_{\\theta^*}) (\\theta - \\theta^*), \\] <p>where the first-order term vanishes due to the assumed local optimality of \\(\\theta^*\\). Negation and exponentiation yield</p> \\[ p(\\theta \\vert \\mathcal{D}) \\approx \\mathcal{N}\\Bigl(\\theta^*, \\mathbf{H}(\\mathcal{D}, f_{\\theta^*})^{-\\frac{1}{2}}\\Bigr) \\] <p>with \\(\\mathbf{H} = \\nabla^2_{\\theta \\theta} \\mathcal{L}(\\mathcal{D}, f_{\\theta^*})\\) being the posterior precision.</p>"},{"location":"background/#linearised-laplace-approximation","title":"Linearised Laplace approximation","text":"<p>To obtain predictive uncertainty estimates, the weight space uncertainty is pushed forward into the neural network's output space. This is either done via sampling a set of \\(S\\) weights from the approximate posterior and using these in the neural network forward pass to obtain \\(S\\) predictions, or by linearising the network around the MAP estimate as</p> \\[ f_{\\theta^{\\text{lin}}}(\\cdot, \\theta) = f_{\\theta^*}(\\cdot, \\theta^*) + \\mathcal{J}_{\\theta^*}(\\cdot)(\\theta - \\theta^*) \\] <p>and using the linear closure of Gaussian distributions<sup>2</sup>, yielding closed-form output-space uncertainty.</p> <p>The linearised approach is guaranteed to yield positive-definite weight-space covariance matrices for a strictly convex regulariser \\(\\Omega\\) at any weight configuration \\(\\theta\\), not just at MAP estimates (that are hard to obtain exactly in deep learning settings). Usually, further approximations are needed to reduce the computational and memory requirements of the curvature. These are discussed in Curvature.</p>"},{"location":"background/#marginal-log-likelihood","title":"Marginal log-likelihood","text":"<p>An important Bayesian tool for model selection is the marginal log-likelihood given by</p> \\[ \\log p(\\mathcal{D} \\vert \\mathcal{M}) \\approx \\log p(\\mathcal{D}, \\theta^* \\vert \\mathcal{M}) - \\frac{1}{2} \\log \\left\\vert \\frac{1}{2\\pi} \\mathbf{H}(\\mathcal{D}, f_{\\theta^*}) \\right\\vert. \\] <p>This term is often used for the selection of the model hyperparameters \\(\\mathcal{M}\\) via maximization <sup>3</sup>, since it represents an analytic trade-off between complexity and expressivity -- the so-called Occam's razor <sup>4</sup>. Tractability and scalability depend on the structure of the estimated \\(\\mathbf{H}(\\mathcal{D}, f_{\\theta^*})\\), but compared to the predictive uncertainty above (cf. Weight posterior), no inversion is needed.</p> <ol> <li> <p>For classification, the logit-space uncertainty is analytic, but the predictive distribution has to be approximated, e.g., through Monte Carlo sampling and averaging the softmax probabilities.\u00a0\u21a9</p> </li> <li> <p>A. Immer, M. Korzepa, and M. Bauer, \"Improving predictions of Bayesian neural nets via local linearization.\" 2021. Available: http://arxiv.org/abs/2008.08400 \u21a9</p> </li> <li> <p>A. Immer, M. Bauer, V. Fortuin, G. R\u00e4tsch, and M. E. Khan, \"Scalable marginal likelihood estimation for model selection in deep learning.\" 2021. Available: http://arxiv.org/abs/2104.04975 \u21a9</p> </li> <li> <p>C. Rasmussen and Z. Ghahramani, \"Occam's razor,\" Advances in Neural Information Processing Systems, vol. 13, 2000.\u00a0\u21a9</p> </li> </ol>"},{"location":"design_philosophy/","title":"Design Philosophy of <code>laplax</code>","text":"<p>The development of <code>laplax</code> is guided by the following principles:</p> <ul> <li> <p>Minimal Dependencies: The package only depends on <code>jax</code>, ensuring compatibility and ease of integration.</p> </li> <li> <p>Matrix-Free Linear Algebra: The core of our implementation revolves around efficient matrix-vector products. By passing around Python <code>Callable</code> objects, we maintain a loose coupling between components, allowing for easy interaction with various other packages, including linear operator libraries in <code>jax</code>.</p> </li> <li> <p>Performance and Practicality: We prioritize a performant and minimal implementation that serves practical needs. The package offers a simple API for basic use cases while primarily serving as a reference implementation for researchers to compare new methods or iterate quickly over experiments.</p> </li> <li> <p>PyTree-Centric Structure: Internally, the package is structured around PyTrees. This design choice allows us to defer materialization until necessary, optimizing performance and memory usage.</p> </li> </ul>"},{"location":"design_philosophy/#roadmap-and-contributions","title":"Roadmap and Contributions","text":"<p>We're developing this package in public. The roadmap and feature priorities are discussed in the Issues of the repository. If you're interested in contributing or want to see what's planned for the future, please check them out.</p>"},{"location":"_examples/0000_tiny_laplax/","title":"Tiny illustration of Laplace approximations","text":"<p>This script is a super tiny illustration of a Laplace approximation - one where curvature approximation is tractable and can be easy visualised</p> <pre><code>import jax.numpy as jnp\nfrom jax.nn import relu\nfrom plotting import plot_figure_1\n\nfrom laplax import laplace\n\n# You need optimized parameters,\nbest_params = {\"theta1\": jnp.array(1.6546547), \"theta2\": jnp.array(1.0420421)}\n\n\ndef model_fn(input, params):\n    return relu(params[\"theta1\"] * input - 1) * params[\"theta2\"]\n\n\ndata = {  # and training data.\n    \"input\": jnp.array([1.0, -1.0]).reshape(2, 1),\n    \"target\": jnp.array([1.0, -1.0]).reshape(2, 1),\n}\n\n# Then apply laplax\nposterior_fn, _ = laplace(\n    model_fn,\n    best_params,\n    data,\n    loss_fn=\"mse\",\n    curv_type=\"full\",\n)\ncurv = posterior_fn({\"prior_prec\": 0.2}).state[\"scale\"]\n\n# to get figure 1.\nplot_figure_1(best_params, curv, save_fig=False)\n</code></pre> <pre><code>\u001b[32m2025-11-18 16:22:50.894\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlaplax.api\u001b[0m:\u001b[36mlaplace\u001b[0m:\u001b[36m669\u001b[0m - \u001b[34m\u001b[1mCreating curvature MV - factor = 1/1 = 1.0\u001b[0m\n\n\n\u001b[32m2025-11-18 16:22:50.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlaplax.api\u001b[0m:\u001b[36m_maybe_wrap_loader_or_batch\u001b[0m:\u001b[36m179\u001b[0m - \u001b[34m\u001b[1mUsing *single batch* curvature evaluation.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:22:51.272\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlaplax.api\u001b[0m:\u001b[36mlaplace\u001b[0m:\u001b[36m695\u001b[0m - \u001b[34m\u001b[1mCurvature estimated: full\u001b[0m\n\n\n\u001b[32m2025-11-18 16:22:51.272\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlaplax.api\u001b[0m:\u001b[36mlaplace\u001b[0m:\u001b[36m704\u001b[0m - \u001b[34m\u001b[1mPosterior callable constructed.\u001b[0m\n\n\n/home/runner/work/laplax/laplax/examples/plotting.py:545: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\n(&lt;Figure size 325x200.861 with 1 Axes&gt;,\n &lt;Axes: xlabel='$\\\\theta_1$', ylabel='$\\\\theta_2$'&gt;)\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/","title":"Introduction to <code>laplax</code> for regression tasks","text":"<p>This tutorial follows the <code>laplace-torch</code> regression tutorial and provides a quick overview of the different functionalities which are currently supported by <code>laplax</code>.</p> <p>For the dataset we consider sinus as our target with additional observation noise \\(\\sigma^2 = 0.3\\). To make the task harder, we only consider training and validation data on a few subintervals.</p> <pre><code>from functools import partial\n\nimport ipywidgets as widgets\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nimport torch\nfrom flax import nnx\nfrom helper import DataLoader, get_sinusoid_example\nfrom IPython.display import display\nfrom matplotlib.colors import SymLogNorm\nfrom plotting import plot_regression_with_uncertainty, plot_sinusoid_task, print_results\nfrom skerch import linops\nfrom skerch.decompositions import seigh\nfrom tqdm.auto import tqdm\n\nfrom laplax.curv import create_ggn_mv, create_posterior_fn, estimate_curvature\nfrom laplax.curv.cov import set_posterior_fn\nfrom laplax.curv.utils import LowRankTerms, get_matvec\nfrom laplax.eval import evaluate_for_given_prior_arguments, marginal_log_likelihood\nfrom laplax.eval.calibrate import optimize_prior_prec\nfrom laplax.eval.metrics import (\n    DEFAULT_REGRESSION_METRICS,\n    chi_squared_zero,\n    nll_gaussian,\n)\nfrom laplax.eval.pushforward import (\n    lin_pred_mean,\n    lin_pred_std,\n    lin_pred_var,\n    lin_setup,\n    nonlin_pred_mean,\n    nonlin_pred_std,\n    nonlin_pred_var,\n    nonlin_setup,\n    set_lin_pushforward,\n    set_nonlin_pushforward,\n    set_posterior_gp_kernel,\n)\nfrom laplax.eval.utils import evaluate_metrics_on_dataset\nfrom laplax.register import register_curvature_method\nfrom laplax.types import DType\nfrom laplax.util.flatten import flatten_function\nfrom laplax.util.loader import input_target_split\nfrom laplax.util.mv import to_dense\nfrom laplax.util.tree import get_size\n\nn_epochs = 1000\nkey = jax.random.key(0)\n\n# Sample toy data example\nnum_training_samples = 150\nnum_calibration_samples = 50\nnum_test_samples = 150\n\nbatch_size = 20\nX_train, y_train, X_valid, y_valid, X_test, y_test = get_sinusoid_example(\n    num_train_data=num_training_samples,\n    num_valid_data=num_calibration_samples,\n    num_test_data=num_test_samples,\n    sigma_noise=0.3,\n    intervals=[(0, 2), (4, 5), (6, 8)],\n    rng_key=jax.random.key(0),\n)\ntrain_loader = DataLoader(X_train, y_train, batch_size)\n\nfig = plot_sinusoid_task(X_train, y_train, X_test, y_test)\n</code></pre> <p></p>"},{"location":"_examples/0001_laplax_for_regression/#training-for-the-map","title":"Training for the MAP","text":"<p>In this tutorial, we use <code>flax.nnx</code> for setting up neural networks in <code>jax</code>, but other libraries (e.g., <code>equinox</code> or <code>flax.linen</code>) should also work out of the box since we will only require a split into <code>model_fn</code> and <code>params</code> for <code>laplax</code>, which all of them provide.</p> <p>From a Bayesian perspective supervised learning can be seen as finding the maximum-a-posteriori estimate of the joint log likelihood:</p> \\[ \\text{arg}\\max_{\\theta_\\in\\mathbb{R}^{P}} = \\sum_{n=1}^{N} \\log p(y_n \\vert f(x_n, \\theta) )+ \\log p(\\theta) \\] <p>where: - \\(f\\) is the neural network, - \\(\\theta \\in \\mathbb{R}^{P}\\) its parameters, and - \\(\\mathcal{D} := \\{(x_n, y_n)\\}_{n=1}^{N}\\) the labelled dataset.</p> <pre><code># Create and train MAP model\nclass Model(nnx.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, rngs):\n        self.linear1 = nnx.Linear(in_channels, hidden_channels, rngs=rngs)\n        self.linear2 = nnx.Linear(hidden_channels, out_channels, rngs=rngs)\n\n    def __call__(self, x):\n        x = self.linear2(nnx.tanh(self.linear1(x)))\n        return x\n\n\n@nnx.jit\ndef train_step(model, optimizer, x, y):\n    def loss_fn(model):\n        y_pred = model(x)  # Call methods directly\n        return jnp.sum((y_pred - y) ** 2)\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(grads)  # Inplace updates\n\n    return loss\n\n\ndef train_model(model, n_epochs, lr=1e-3):\n    # Create optimizer\n    optimizer = nnx.Optimizer(model, optax.adamw(lr))  # Reference sharing\n\n    # Train epoch\n    for epoch in range(n_epochs):\n        for x_tr, y_tr in train_loader:\n            loss = train_step(model, optimizer, x_tr, y_tr)\n\n        if epoch % 100 == 0:\n            print(f\"[epoch {epoch}]: loss: {loss:.4f}\")\n\n    print(f\"Final loss: {loss:.4f}\")\n    return model\n</code></pre> <pre><code># Initialize model\nmodel = Model(in_channels=1, hidden_channels=64, out_channels=1, rngs=nnx.Rngs(0))\n\n# Train model\nmodel = train_model(model, n_epochs=1000)\n\nX_pred = jnp.linspace(0.0, 8.0, 200).reshape(200, 1)\ny_pred = jax.vmap(model)(X_pred)\n\n_ = plot_sinusoid_task(X_train, y_train, X_test, y_test, X_pred, y_pred)\n</code></pre> <pre><code>[epoch 0]: loss: 6.2626\n\n\n[epoch 100]: loss: 4.2591\n\n\n[epoch 200]: loss: 1.3689\n\n\n[epoch 300]: loss: 1.5673\n\n\n[epoch 400]: loss: 0.3712\n\n\n[epoch 500]: loss: 0.6099\n\n\n[epoch 600]: loss: 1.7761\n\n\n[epoch 700]: loss: 0.5140\n\n\n[epoch 800]: loss: 0.5963\n\n\n[epoch 900]: loss: 0.6607\n\n\nFinal loss: 0.8118\n</code></pre> <p></p>"},{"location":"_examples/0001_laplax_for_regression/#estimating-the-curvature","title":"Estimating the curvature","text":"<p>We are now interested in finding a normal distribution that describes the uncertainty in the weight space with respect to the loss and the data: $$ p(\\theta \\vert \\mathcal{D}) = \\frac{p(\\mathcal{D}, \\theta)}{p(\\mathcal{D})} = \\frac {p(\\mathcal{D} \\vert \\theta) p(\\theta)}{\\int p(\\mathcal{D} \\vert \\theta) p(\\theta) d\\theta}. $$</p> <p>Our tool of choice is the Laplace approximation --- motivated via a second-order Taylor expansion, where the first-order term disappears due to the assumption of having reached a local minimum of the loss. Following these steps we get the following normal distribution approximating the true posterior: $$ \\theta \\sim \\mathcal{N}(\\theta_{\\text{MAP}}, [\\nabla^2_{\\theta\\theta} \\log p (\\theta \\vert \\mathcal{D}) \\vert_{\\theta = \\theta_{\\text{MAP}}} ]^{-1}). $$</p> <p>We usually assume the prior to be an isotropic Gaussian distribution, hence the expensive part remains mainly the loss hessian. Due to various reasons (positive definiteness or/and a linearized perspective of the neural network) we usually consider instead of the true Hessian the so-called Generalized-Gauss Newton matrix:</p> \\[ \\text{GGN}(f, \\theta, \\mathcal{D}) = \\sum_{n=1}^{N} \\mathcal{J}_{\\theta} f(x_n) ^\\top \\nabla^2_{\\theta\\theta}\\ell(f_\\theta(x_n), y_n) \\mathcal{J}_{\\theta} f(x_n). \\] <p>We start by splitting the <code>flax.nnx</code> model into <code>model_fn</code> and <code>params</code>. Important The signature of the <code>model_fn</code> needs to be <code>input</code> and <code>params</code>, since we strongly depend on the key word arguments in <code>laplax</code>.</p> <pre><code># Create GGN\ngraph_def, params = nnx.split(model)\n\n\ndef model_fn(input, params):\n    return nnx.call((graph_def, params))(input)[0]\n\n\ntrain_batch = {\"input\": X_train, \"target\": y_train}\n\nggn_mv = create_ggn_mv(\n    model_fn,\n    params,\n    train_batch,\n    loss_fn=\"mse\",\n)\n</code></pre> <p>In this small toy example, we can dense the curvature matrix-vector product. We start by wrapping the matrix-vector product to accept normal 1D vectors of size \\(P\\). This will help us visualize the GGN.</p> <pre><code>ggn_mv_wrapped = flatten_function(ggn_mv, layout=params)\narr = to_dense(ggn_mv_wrapped, layout=get_size(params))\n\n\nplt.imshow(arr, norm=SymLogNorm(linthresh=1e-2, linscale=1))\nplt.colorbar()\nplt.show()\n</code></pre> <p></p>"},{"location":"_examples/0001_laplax_for_regression/#curvature-estimators","title":"Curvature estimators","text":"<p>In practice, we can not afford to dense and continue computations with the GGN. Therefore, various strategies for estimating the curvature exist. Within this package we have: <code>full</code> (obvious), <code>diagonal</code> and low_rank. For the latter, we support finding the low rank representation using <code>lanczos</code> or <code>lobpcg</code>.</p> <pre><code># Create dropdown for library selection.\nlib_dropdown = widgets.Dropdown(\n    options=[\"full\", \"diagonal\", \"lanczos\", \"lobpcg\"],\n    value=\"full\",\n    description=\"Curv. est.:\",\n)\ndisplay(lib_dropdown)\n</code></pre> <pre><code>Dropdown(description='Curv. est.:', options=('full', 'diagonal', 'lanczos', 'lobpcg'), value='full')\n</code></pre> <pre><code>print(f\"Curvature will be estimated using a {lib_dropdown.value} approximation.\")\ncurv_type = lib_dropdown.value\nlow_rank_args = {\n    \"key\": jax.random.key(20),\n    \"rank\": 50,\n    \"mv_jit\": True,\n}\ncurv_args = {} if curv_type in {\"full\", \"diagonal\"} else low_rank_args\n\ncurv_estimate = estimate_curvature(\n    curv_type=curv_type,\n    mv=ggn_mv,\n    layout=params,\n    **curv_args,\n)\n</code></pre> <pre><code>Curvature will be estimated using a full approximation.\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#create-a-posterior_fn","title":"Create a posterior_fn","text":"<p>We can now create a <code>posterior_fn</code> that takes <code>prior_arguments</code> and returns a posterior distribution over the weights. This includes adding the prior precision \\(\\tau\\) and inverting the combined expression in a memory-efficient way: $$ \\text{posterior_fn}(\\tau) = \\big( GGN + \\tau I_{P\\times P} \\big)^{-1} $$ If we have already an estimation of the curvature, then we can directly set the posterior function using the estimate. Otherwise both functions can also be executed at once using the <code>laplax.curv.create_posterior_fn</code>.</p> <pre><code>posterior_fn = set_posterior_fn(curv_type, curv_estimate, layout=params)\n\n# # Alternatively, we can create the posterior function from scratch, if no curvature\n# # estimation is available.\n# # Create Posterior\n# posterior_fn = create_posterior_fn(\n#     curv_type=curv_type,\n#     mv=ggn_mv,\n#     layout=params,\n#     **curv_args,\n# )\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#how-to-pushforward-the-weight-space-uncertainty","title":"How to pushforward the weight space uncertainty?","text":"<p>There are two ideas for pushing forward weight space uncertainty.</p> <ol> <li> <p>Sample-based pushforward via the neural network $$ f(x_n, \\theta_s), \\quad \\theta_s \\sim \\mathcal{N}\\bigg(\\theta_{MAP}, \\Sigma\\bigg)$$</p> </li> <li> <p>Linearized pushforward $$ f(x_n, \\theta) \\sim \\mathcal{N}\\bigg(f(x_n, \\theta_{MAP}), \\mathcal{J}{\\theta}(f (x_n, \\theta{\\text{MAP}}))\\Sigma \\mathcal{J}{\\theta}(f(x_n, \\theta{\\text{MAP}})) ^\\top\\bigg)$$</p> </li> </ol> <p>Recommendation: Play around with the prior precision to see its strong modeling impact. Also check out larger intervals to see the uncertainty structure outside of the training domain.</p>"},{"location":"_examples/0001_laplax_for_regression/#sample-based-pushforward-via-the-neural-network","title":"Sample-based pushforward via the neural network","text":"<pre><code># Setup linearized pushforward\nset_nonlin_prob_predictive = partial(\n    set_nonlin_pushforward,\n    model_fn=model_fn,\n    mean_params=params,\n    posterior_fn=posterior_fn,\n    pushforward_fns=[nonlin_setup, nonlin_pred_mean, nonlin_pred_var, nonlin_pred_std],\n    key=jax.random.key(42),\n    num_samples=10000,\n)\nprior_arguments = {\"prior_prec\": 40.0}  # Choose any prior precision.\nprob_predictive = set_nonlin_prob_predictive(\n    prior_arguments=prior_arguments,\n)\n\nX_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\npred = jax.vmap(prob_predictive)(X_pred)\n\n_ = plot_regression_with_uncertainty(\n    X_train=train_batch[\"input\"],\n    y_train=train_batch[\"target\"],\n    X_pred=X_pred,\n    y_pred=pred[\"pred_mean\"][:, 0],\n    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n)\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#linearized-pushforward","title":"Linearized pushforward","text":"<pre><code># Setup linearized pushforward\nset_prob_predictive = partial(\n    set_lin_pushforward,\n    model_fn=model_fn,\n    mean_params=params,\n    posterior_fn=posterior_fn,\n    pushforward_fns=[\n        lin_setup,\n        lin_pred_mean,\n        lin_pred_var,\n        lin_pred_std,\n    ],\n)\nprior_arguments = {\"prior_prec\": 1.0}  # Choose any prior precision.\nprob_predictive = set_prob_predictive(\n    prior_arguments=prior_arguments,\n)\n\nX_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\npred = jax.vmap(prob_predictive)(X_pred)\n\n_ = plot_regression_with_uncertainty(\n    X_train=train_batch[\"input\"],\n    y_train=train_batch[\"target\"],\n    X_pred=X_pred,\n    y_pred=pred[\"pred_mean\"][:, 0],\n    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n)\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#calibration","title":"Calibration","text":"<p>When playing around we see that it is non-trivial of how to choose the prior \\ precision. To do so with an heuristic we need to optimize some objective. There are two common strategies: either optimize for a downstream metric (e.g. Negative-Log Likelihood or average calibration (\\(\\chi^2\\))) or target the marginal log-likelihood. Later is a common objective for even more general model selection (see below) and is given by:</p> \\[ \\log p(\\mathcal{D}\\vert\\mathcal{M}) = \\log p(\\mathcal{D} \\vert \\theta_{*}, \\mathcal {M}) + \\log p(\\mathcal{\\theta_*} \\vert \\mathcal{M}) - \\frac{1}{2} \\log \\vert \\frac{1} {2\\pi} \\mathrm{H}_{\\theta_*}\\vert \\] <p>where \\(\\mathrm{H}_{\\theta_*}\\) is the posterior precision and \\(\\mathcal{M}\\) other model parameters, such as the network architecture. We note that no inversion is needed compute the marginal log likelihood when updating the prior arguments. However, in practice optimizing for downstream metrics will also lead to better downstream metrics.</p> <p>For optimization we can choose either grid search or gradient descent.</p>"},{"location":"_examples/0001_laplax_for_regression/#partially-initializing-set_prob_predictive","title":"Partially initializing <code>set_prob_predictive</code>","text":"<p>We always start by partially initializing the <code>set_prob_predictive</code>, such that it only misses the <code>prior_arguments</code>, which we will use for optimizing a chosen objective.</p> <pre><code>set_prob_predictive = partial(\n    set_lin_pushforward,\n    model_fn=model_fn,\n    mean_params=params,\n    posterior_fn=posterior_fn,\n    pushforward_fns=[\n        lin_setup,\n        lin_pred_mean,\n        lin_pred_std,\n    ],\n)\n\n# Set a batch of calibration data\nclbr_batch = {\"input\": X_valid, \"target\": y_valid}\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#select-calibration-objective","title":"Select calibration objective","text":"<pre><code># Create dropdown for library selection.\nclbr_obj_dropdown = widgets.Dropdown(\n    options=[\"nll\", \"chi_squared\", \"marginal log-likelihood\"],\n    value=\"nll\",\n    description=\"Objective:\",\n)\ndisplay(clbr_obj_dropdown)\n</code></pre> <pre><code>Dropdown(description='Objective:', options=('nll', 'chi_squared', 'marginal log-likelihood'), value='nll')\n</code></pre> <pre><code>@jax.jit\ndef nll_objective(prior_arguments, batch):\n    return evaluate_for_given_prior_arguments(\n        prior_arguments=prior_arguments,\n        data=batch,\n        set_prob_predictive=set_prob_predictive,\n        metric=nll_gaussian,\n    )\n\n\n@jax.jit\ndef chi_squared_objective(prior_arguments, batch):\n    return evaluate_for_given_prior_arguments(\n        prior_arguments=prior_arguments,\n        data=batch,\n        set_prob_predictive=set_prob_predictive,\n        metric=chi_squared_zero,\n        # This is chi_squared tansformed to have its optimal value at zero.\n    )\n\n\n@jax.jit\ndef marginal_log_likelihood_objective(prior_arguments, batch):\n    return -marginal_log_likelihood(\n        curv_estimate,\n        prior_arguments=prior_arguments,\n        data=batch,\n        model_fn=model_fn,\n        params=params,\n        loss_fn=\"mse\",\n        curv_type=curv_type,\n    )\n\n\n# Select objective based on dropdown menu\nobjective = {\n    \"nll\": nll_objective,\n    \"chi_squared\": chi_squared_objective,\n    \"marginal log-likelihood\": marginal_log_likelihood_objective,\n}[clbr_obj_dropdown.value]\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#grid-search","title":"Grid search","text":"<pre><code>prior_prec = optimize_prior_prec(\n    objective=partial(objective, batch=clbr_batch),\n    log_prior_prec_min=-3.0,\n    log_prior_prec_max=3.0,\n    grid_size=50,\n)\n\nprint(\"Calibrated prior precision: \", prior_prec)\n</code></pre> <pre><code>\u001b[32m2025-11-18 16:19:40.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mCaught nan, setting result to inf.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.1630 seconds, prior prec: 0.0010, result: inf\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mCaught nan, setting result to inf.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0037 seconds, prior prec: 0.0013, result: inf\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mCaught nan, setting result to inf.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0021 seconds, prior prec: 0.0018, result: inf\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mCaught nan, setting result to inf.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 0.0023, result: inf\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mCaught nan, setting result to inf.\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 0.0031, result: inf\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 0.0041, result: 0.589194\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0018 seconds, prior prec: 0.0054, result: 0.692883\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 0.0072, result: 0.692233\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0020 seconds, prior prec: 0.0095, result: 0.695813\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0019 seconds, prior prec: 0.0126, result: 0.698771\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0019 seconds, prior prec: 0.0168, result: 0.719765\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0012 seconds, prior prec: 0.0222, result: 0.725796\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 0.0295, result: 0.748759\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 0.0391, result: 0.761235\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0017 seconds, prior prec: 0.0518, result: 0.777536\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0018 seconds, prior prec: 0.0687, result: 0.793358\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0017 seconds, prior prec: 0.0910, result: 0.814793\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0018 seconds, prior prec: 0.1207, result: 0.825115\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0009 seconds, prior prec: 0.1600, result: 0.851190\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 0.2121, result: 0.873288\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0009 seconds, prior prec: 0.2812, result: 0.899353\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0015 seconds, prior prec: 0.3728, result: 0.925958\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 0.4942, result: 0.956281\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0013 seconds, prior prec: 0.6551, result: 0.985040\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0009 seconds, prior prec: 0.8685, result: 1.018489\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0015 seconds, prior prec: 1.1514, result: 1.050153\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 1.5264, result: 1.084453\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0012 seconds, prior prec: 2.0236, result: 1.120389\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0015 seconds, prior prec: 2.6827, result: 1.159454\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0013 seconds, prior prec: 3.5565, result: 1.201323\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 4.7149, result: 1.247882\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 6.2506, result: 1.299314\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 8.2864, result: 1.355583\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 10.9854, result: 1.417005\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0013 seconds, prior prec: 14.5635, result: 1.483379\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0010 seconds, prior prec: 19.3070, result: 1.554855\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0019 seconds, prior prec: 25.5955, result: 1.631271\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0014 seconds, prior prec: 33.9322, result: 1.713325\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0011 seconds, prior prec: 44.9843, result: 1.801812\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0012 seconds, prior prec: 59.6362, result: 1.897861\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0013 seconds, prior prec: 79.0604, result: 2.003031\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0015 seconds, prior prec: 104.8113, result: 2.118735\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0016 seconds, prior prec: 138.9495, result: 2.246382\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0011 seconds, prior prec: 184.2069, result: 2.387300\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0012 seconds, prior prec: 244.2053, result: 2.543013\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0011 seconds, prior prec: 323.7458, result: 2.715356\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0011 seconds, prior prec: 429.1934, result: 2.906862\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0017 seconds, prior prec: 568.9865, result: 3.121205\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0017 seconds, prior prec: 754.3121, result: 3.363734\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mTook 0.0018 seconds, prior prec: 1000.0000, result: 3.642258\u001b[0m\n\n\n\u001b[32m2025-11-18 16:19:40.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlaplax.eval.calibrate\u001b[0m:\u001b[36mgrid_search\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mChosen prior prec = 0.0041\u001b[0m\n\n\nCalibrated prior precision:  0.0040949145\n</code></pre> <p>We can use a similar pipeline to evaluate an arbitrary set of metrics. A few common \\ regression metrics are natively supported in <code>laplax</code>.</p> <pre><code># Set test batch\ntest_batch = {\"input\": X_test, \"target\": y_test}\n\nprob_predictive = set_prob_predictive(prior_arguments={\"prior_prec\": prior_prec})\nresults = evaluate_metrics_on_dataset(\n    pred_fn=prob_predictive,\n    data=test_batch,\n    metrics=DEFAULT_REGRESSION_METRICS,\n    reduce=jnp.mean,  # How to aggregate metrics over batch.\n)\n\n\n# Print metrics\nprint_results(results, \"Model Evaluation Metrics\")\n\n# Predict for plot\nX_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\npred = jax.vmap(prob_predictive)(X_pred)\n\n_ = plot_regression_with_uncertainty(\n    X_train=train_batch[\"input\"],\n    y_train=train_batch[\"target\"],\n    X_pred=X_pred,\n    y_pred=pred[\"pred_mean\"][:, 0],\n    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n)\n</code></pre> <pre><code>Model Evaluation Metrics\n----------------------------------------\nchi^2 : 3.170391\ncrps  : 0.233743\nnll   : 1.017358\nrmse  : 0.325561\n</code></pre> <p></p>"},{"location":"_examples/0001_laplax_for_regression/#gradient-descent","title":"Gradient descent","text":"<p>A major benefit from the gradient descent objective is that we can straightforwardly extend the calibration to other hyperparameters. So far, we were just able to calibrate the prior precision, which will not account properly for the additional observation noise in our regression task. To change this, we will introduce the so-called <code>sigma_squared</code> term in our objective, which will support us in modeling the model uncertainty as well. The marginal log-likelihood for the mean squared error loss is then given by:</p> \\[ \\log p(\\mathcal{D}\\vert\\mathcal{M}) = \\frac{1}{2\\sigma}\\sum_{n=1}^N (y_n - f(x_n, \\theta_*))^2 + \\tau \\|\\theta_*\\|^2 - \\frac{1}{2} \\log \\vert \\frac{1}{2\\pi} \\mathrm{H}_ {\\theta_*}\\vert \\] <pre><code># Initialize prior arguments\nprior_arguments = {\"prior_prec\": jnp.array(1.0), \"sigma_squared\": jnp.array(0.1)}\n</code></pre> <pre><code># Set parameters\nnum_clbr_epochs = 100\nlr = 1e-3\n\n# Set optimizer\noptimizer = optax.adam(lr)\nopt_state = optimizer.init(prior_arguments)\nvalid_loader = DataLoader(X_valid, y_valid, batch_size=16)\n\n# Transform prior arguments, so we can optimize over all reals\nprior_arguments = jax.tree.map(jnp.log, prior_arguments)\n\n# Optimize prior arguments\nwith tqdm(total=num_clbr_epochs, desc=\"Training\") as pbar:\n    for _ in range(num_clbr_epochs):\n        epoch_vals = []\n        for batch in valid_loader:\n            val, grads = jax.value_and_grad(\n                lambda p: objective(\n                    jax.tree.map(jnp.exp, p),\n                    input_target_split(batch),  # noqa: B023\n                )\n            )(prior_arguments)\n\n            # Update the parameters using the optimizer\n            updates, opt_state = optimizer.update(grads, opt_state)\n            prior_arguments = optax.apply_updates(prior_arguments, updates)\n            epoch_vals.append(val)\n\n        avg_val = sum(epoch_vals) / len(epoch_vals)\n        pbar.set_postfix({\"objective\": f\"{avg_val:.4f}\"})\n        pbar.update(1)\n\n# Transform prior arguments back\nprior_arguments = jax.tree.map(jnp.exp, prior_arguments)\n\nprint(\"Final values:\", dict(prior_arguments))\n</code></pre> <pre><code>Training:   0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\nFinal values: {'prior_prec': Array(1.3951623, dtype=float32), 'sigma_squared': Array(0.13577305, dtype=float32)}\n</code></pre> <pre><code>prob_predictive = set_prob_predictive(prior_arguments=prior_arguments)\nresults = evaluate_metrics_on_dataset(\n    pred_fn=prob_predictive,\n    data=test_batch,\n    metrics=DEFAULT_REGRESSION_METRICS,\n    reduce=jnp.mean,  # How to aggregate metrics over batch.\n)\n\n# Print metrics\nprint_results(results, \"Model Evaluation Metrics\")\n\n# Predict for plot\nX_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\npred = jax.vmap(prob_predictive)(X_pred)\n\n_ = plot_regression_with_uncertainty(\n    X_train=train_batch[\"input\"],\n    y_train=train_batch[\"target\"],\n    X_pred=X_pred,\n    y_pred=pred[\"pred_mean\"][:, 0],\n    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n)\n</code></pre> <pre><code>Model Evaluation Metrics\n----------------------------------------\nchi^2 : 1.203062\ncrps  : 0.224622\nnll   : 0.497671\nrmse  : 0.325561\n</code></pre> <p></p>"},{"location":"_examples/0001_laplax_for_regression/#bonus-registering-skerch","title":"Bonus: Registering <code>skerch</code>","text":"<p>Let us get to some bonus content. One benefit of <code>laplax</code> is its modularity, which should make it easy to extend or bend its use cases. For example, we can easily register our favorite curvature approximation method: <code>skerch</code>; even though it was written for <code>torch</code>. To make it available for creating a posterior function based on its curvature structure, we can either implement (+register) all methods or refer to a default method, which might already exist in <code>laplax</code>.</p> <pre><code>class JAXMV(linops.TorchLinOpWrapper):\n    def __init__(self, matvec, shape):\n        self.shape = shape\n        self.matvec = matvec\n\n    def __matmul__(self, x):\n        x_dtype = x.dtype\n        x = jnp.asarray(x.detach().cpu().numpy())\n        x = self.matvec(x)\n        return torch.tensor(np.asarray(x), dtype=x_dtype)\n\n    def __rmatmul__(self, x):\n        return self.__matmul__(x.T)\n\n\ndef skerch_low_rank(\n    A,\n    *,\n    layout=None,\n    rank: int = 100,\n    return_dtype: DType = jnp.float64,\n    mv_jittable=True,\n    **kwargs,\n):\n    del kwargs\n    # Setup mv product.\n    matvec, size = get_matvec(A, layout=layout, jit=mv_jittable)\n    op = JAXMV(matvec, (size, size))\n\n    res = seigh(\n        op, op_device=\"cpu\", op_dtype=torch.float64, outer_dim=rank, inner_dim=rank\n    )\n\n    low_rank_result = LowRankTerms(\n        U=jnp.asarray((res[0] @ res[1]).detach().cpu()),\n        S=jnp.asarray(res[2].detach().cpu().numpy()),\n        scalar=jnp.asarray(0.0, dtype=return_dtype),\n    )\n    return low_rank_result\n\n\nregister_curvature_method(\n    name=\"skerch\", create_curvature_fn=skerch_low_rank, default=\"lanczos\"\n)\n</code></pre> <pre><code>posterior_fn = create_posterior_fn(\n    curv_type=\"skerch\",\n    mv=ggn_mv,\n    layout=params,\n    key=jax.random.key(20),\n    rank=50,\n    mv_jit=True,\n)\n</code></pre> <pre><code>/tmp/ipykernel_4980/1416079365.py:37: UserWarning: Explicitly requested dtype &lt;class 'jax.numpy.float64'&gt; requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scalar=jnp.asarray(0.0, dtype=return_dtype),\n</code></pre>"},{"location":"_examples/0001_laplax_for_regression/#bonus-posterior-gp-kernel","title":"Bonus: Posterior GP kernel","text":"<p>We can now use the posterior function to create a Laplace (GP) kernel to also model covariances between various inputs. While the default only takes a single input, we can use standard vectorization techniques to apply it to multiple inputs at the same time.</p> <pre><code>gp_kernel, dist_state = set_posterior_gp_kernel(\n    model_fn=model_fn,\n    mean=params,\n    posterior_fn=posterior_fn,\n    prior_arguments=prior_arguments,\n    dense=True,  # If dense = False, then a slower kernel-vector product is returned.\n    output_layout=1,\n)\n\n\ndef vectorized_laplace_kernel(a, b):\n    return jnp.vectorize(gp_kernel, signature=\"(d),(d)-&gt;(j,j)\")(a, b)[..., 0]\n\n\nX_pred = jnp.linspace(0.0, 8.0, 200).reshape(200, 1)\nY_pred = model_fn(X_pred, params)[:, 0]\nY_var = vectorized_laplace_kernel(X_pred, X_pred)\n\n_ = plot_regression_with_uncertainty(\n    X_train=train_batch[\"input\"],\n    y_train=train_batch[\"target\"],\n    X_pred=X_pred[:, 0],\n    y_pred=Y_pred,\n    y_std=jnp.sqrt(Y_var)[:, 0],\n)\n</code></pre> <p></p> <p>OK, in this little example this is not so much of an advantage. However, for more complex models/datasets this can be a huge support for downstream tasks.</p>"},{"location":"_examples/0001_laplax_for_regression/#bonus-model-selection","title":"Bonus: Model selection","text":"<pre><code>class ModelOverfit(nnx.Module):\n    def __init__(self, rngs):\n        self.linear1 = nnx.Linear(1, 25, rngs=rngs)\n        self.linear2 = nnx.Linear(25, 50, rngs=rngs)\n        self.linear3 = nnx.Linear(50, 25, rngs=rngs)\n        self.linear4 = nnx.Linear(25, 1, rngs=rngs)\n\n    def __call__(self, x):\n        x = nnx.tanh(self.linear1(x))\n        x = nnx.tanh(self.linear2(x))\n        x = nnx.tanh(self.linear3(x))\n        x = nnx.tanh(self.linear4(x))\n        return x\n\n\nclass FullOverfit(nnx.Module):\n    def __init__(self, rngs):\n        self.linear1 = nnx.Linear(1, 25, rngs=rngs)\n        self.linear2 = nnx.Linear(25, 50, rngs=rngs)\n        self.linear3 = nnx.Linear(50, 25, rngs=rngs)\n        self.linear4 = nnx.Linear(25, 50, rngs=rngs)\n        self.linear5 = nnx.Linear(50, 25, rngs=rngs)\n        self.linear6 = nnx.Linear(25, 50, rngs=rngs)\n        self.linear7 = nnx.Linear(50, 25, rngs=rngs)\n        self.linear8 = nnx.Linear(25, 1, rngs=rngs)\n\n    def __call__(self, x):\n        x = nnx.tanh(self.linear1(x))\n        x = nnx.tanh(self.linear2(x))\n        x = nnx.tanh(self.linear3(x))\n        x = nnx.tanh(self.linear4(x))\n        x = nnx.tanh(self.linear5(x))\n        x = nnx.tanh(self.linear6(x))\n        x = nnx.tanh(self.linear7(x))\n        x = nnx.tanh(self.linear8(x))\n        return x\n\n\nn_epochs = 1000\nlr = 1e-3\nrngs = nnx.Rngs(0)\nmodels = [\n    (Model(1, 2, 1, rngs=rngs), 1000, 1e-3, \"1-2-1\"),\n    (Model(1, 50, 1, rngs=rngs), 1000, 1e-3, \"1-50-1\"),\n    (Model(1, 600, 1, rngs=rngs), 1000, 1e-3, \"1-600-1\"),\n    (ModelOverfit(rngs=rngs), n_epochs, 1e-3, \"1-25-50-25-1\"),\n    (FullOverfit(rngs=rngs), n_epochs, 1e-3, \"1-25-50-25-50-25-50-25-1\"),\n]\n\ntrained_models = []\nfor model, num_epoch, lr, name in models:\n    print(f\"Model {name}\")\n    model.name = name\n    trained_models.append(train_model(model, num_epoch, lr))\n    print(\"-\" * 40)\n</code></pre> <pre><code>Model 1-2-1\n\n\n[epoch 0]: loss: 5.9635\n\n\n[epoch 100]: loss: 6.2015\n\n\n[epoch 200]: loss: 8.1277\n\n\n[epoch 300]: loss: 5.1091\n\n\n[epoch 400]: loss: 8.0070\n\n\n[epoch 500]: loss: 7.9144\n\n\n[epoch 600]: loss: 5.4761\n\n\n[epoch 700]: loss: 7.9413\n\n\n[epoch 800]: loss: 7.0130\n\n\n[epoch 900]: loss: 3.6976\n\n\nFinal loss: 6.2531\n----------------------------------------\nModel 1-50-1\n\n\n[epoch 0]: loss: 9.9370\n\n\n[epoch 100]: loss: 5.0796\n\n\n[epoch 200]: loss: 1.5793\n\n\n[epoch 300]: loss: 0.9821\n\n\n[epoch 400]: loss: 2.2806\n\n\n[epoch 500]: loss: 0.5455\n\n\n[epoch 600]: loss: 0.8418\n\n\n[epoch 700]: loss: 0.9991\n\n\n[epoch 800]: loss: 0.5869\n\n\n[epoch 900]: loss: 0.7341\n\n\nFinal loss: 0.8031\n----------------------------------------\nModel 1-600-1\n\n\n[epoch 0]: loss: 7.7758\n\n\n[epoch 100]: loss: 1.7799\n\n\n[epoch 200]: loss: 0.8167\n\n\n[epoch 300]: loss: 2.1499\n\n\n[epoch 400]: loss: 0.6092\n\n\n[epoch 500]: loss: 0.6395\n\n\n[epoch 600]: loss: 0.9502\n\n\n[epoch 700]: loss: 1.2497\n\n\n[epoch 800]: loss: 1.5076\n\n\n[epoch 900]: loss: 0.6278\n\n\nFinal loss: 0.4323\n----------------------------------------\nModel 1-25-50-25-1\n\n\n[epoch 0]: loss: 6.5115\n\n\n[epoch 100]: loss: 0.9744\n\n\n[epoch 200]: loss: 0.9676\n\n\n[epoch 300]: loss: 1.0441\n\n\n[epoch 400]: loss: 0.9354\n\n\n[epoch 500]: loss: 0.5048\n\n\n[epoch 600]: loss: 1.3004\n\n\n[epoch 700]: loss: 1.0012\n\n\n[epoch 800]: loss: 0.9981\n\n\n[epoch 900]: loss: 1.0920\n\n\nFinal loss: 1.0979\n----------------------------------------\nModel 1-25-50-25-50-25-50-25-1\n\n\n[epoch 0]: loss: 8.6605\n\n\n[epoch 100]: loss: 1.1209\n\n\n[epoch 200]: loss: 1.6769\n\n\n[epoch 300]: loss: 1.0294\n\n\n[epoch 400]: loss: 0.9494\n\n\n[epoch 500]: loss: 1.0279\n\n\n[epoch 600]: loss: 0.4329\n\n\n[epoch 700]: loss: 1.0882\n\n\n[epoch 800]: loss: 0.5587\n\n\n[epoch 900]: loss: 0.6489\n\n\nFinal loss: 2.0166\n----------------------------------------\n</code></pre> <pre><code>marglik = {}\ntrain_batch = {\"input\": X_train, \"target\": y_train}\nprior_arguments = {\"prior_prec\": 1.0}\ncurv_type = \"lanczos\"\n\nfor model in trained_models:\n    # Prepare model\n    graph_def, params = nnx.split(model)\n\n    def model_fn(input, params):\n        return nnx.call((graph_def, params))(input)[0]  # noqa: B023\n\n    curv_approx = estimate_curvature(\n        curv_type=curv_type,\n        mv=create_ggn_mv(model_fn, params, train_batch, loss_fn=\"mse\"),\n        layout=params,\n        key=jax.random.key(0),  # If necessary\n        rank=50,  # If necessary\n    )\n\n    marglik[model.name] = marginal_log_likelihood(\n        curv_estimate=curv_approx,\n        prior_arguments=prior_arguments,\n        data=train_batch,\n        model_fn=model_fn,\n        params=params,\n        loss_fn=\"mse\",\n        curv_type=curv_type,\n    ).item()\n</code></pre> <pre><code>print_results(marglik, \"Marginal log-likelihood\")\n</code></pre> <pre><code>Marginal log-likelihood\n----------------------------------------\n1-2-1                    : -61.155720\n1-50-1                   : -60.314087\n1-600-1                  : -225.644897\n1-25-50-25-1             : -89.081299\n1-25-50-25-50-25-50-25-1 : -192.727051\n</code></pre> <p>We would choose the model with the highest marginal log-likelihood. If there exist additional (continuous/relaxed) model parameters, we could use again the marginal log-likelihood in a gradient-based optimization to find its optimal values. Such procedures are often discussed under the name of model selection and differentiable Laplace.</p>"},{"location":"_examples/0002_laplax_on_mnist/","title":"Laplace on MNIST","text":""},{"location":"_examples/0002_laplax_on_mnist/#setup-data-and-model-and-train-it","title":"Setup data and model and train it","text":"<p>We start the tutorial by downloading and setting up a dataloader for <code>MNIST</code>. We will use a simple <code>flax.nnx</code> model for the training. The data + model setup and training closely follows the <code>flax.nnx</code> documentation to stress the flexible post-hoc abilities of <code>laplax</code> (and, of course, Laplace Approximations in general).</p> <pre><code>from functools import partial\nfrom itertools import islice\n\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport torch\nfrom flax import nnx\nfrom plotting import create_proportion_diagram, create_reliability_diagram\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom laplax.curv import create_ggn_mv\nfrom laplax.curv.cov import create_posterior_fn\nfrom laplax.curv.ggn import create_ggn_mv_without_data\nfrom laplax.eval import apply_fns, evaluate_metrics_on_generator, transfer_entry\nfrom laplax.eval.metrics import (\n    calculate_bin_metrics,\n    correctness,\n    expected_calibration_error,\n)\nfrom laplax.eval.pushforward import (\n    lin_mc_pred_act,\n    lin_pred_mean,\n    lin_setup,\n    set_lin_pushforward,\n)\nfrom laplax.util.loader import DataLoaderMV, reduce_add\n\n# Set the random seed for reproducibility\ntorch.manual_seed(0)\n\n# Define constants\ntrain_steps = 2200\neval_every = 200\ntrain_batch_size = 32\nval_batch_size = 32\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)),\n])\n\n# Load MNIST datasets\ntrain_dataset = datasets.MNIST(\n    root=\"data\", train=True, download=True, transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root=\"data\", train=False, download=True, transform=transform\n)\n\n\n# Create data loaders\ndef collate_fn(batch):\n    input, target = (\n        torch.stack([s[0] for s in batch]),\n        torch.tensor([s[1] for s in batch]),\n    )\n    return {\"input\": input.permute(0, 2, 3, 1).numpy(), \"target\": target.numpy()}\n\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=train_batch_size,\n    shuffle=True,\n    drop_last=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=torch.cuda.is_available(),\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=val_batch_size,\n    shuffle=False,\n    drop_last=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=torch.cuda.is_available(),\n)\n\n\n# Create training iterator that yields for exactly train_steps\ntrain_iter = islice(train_loader, train_steps)\nnum_training_samples = len(train_dataset)\n</code></pre> <p>0%|          | 0.00/9.91M [00:00&lt;?, ?B/s]</p> <p>1%|          | 98.3k/9.91M [00:00&lt;00:12, 771kB/s]</p> <p>4%|\u258d         | 393k/9.91M [00:00&lt;00:05, 1.70MB/s]</p> <p>17%|\u2588\u258b        | 1.64M/9.91M [00:00&lt;00:01, 5.39MB/s]</p> <p>30%|\u2588\u2588\u2589       | 2.95M/9.91M [00:00&lt;00:00, 7.30MB/s]</p> <p>63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 6.26M/9.91M [00:00&lt;00:00, 14.0MB/s]</p> <p>95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 9.40M/9.91M [00:00&lt;00:00, 17.6MB/s]</p> <p>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00&lt;00:00, 12.9MB/s]</p> <p>0%|          | 0.00/28.9k [00:00&lt;?, ?B/s]</p> <p>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 415kB/s]</p> <p>0%|          | 0.00/1.65M [00:00&lt;?, ?B/s]</p> <p>6%|\u258c         | 98.3k/1.65M [00:00&lt;00:02, 697kB/s]</p> <p>24%|\u2588\u2588\u258d       | 393k/1.65M [00:00&lt;00:00, 1.51MB/s]</p> <p>97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 1.61M/1.65M [00:00&lt;00:00, 4.69MB/s]</p> <p>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00&lt;00:00, 3.85MB/s]</p> <p>0%|          | 0.00/4.54k [00:00&lt;?, ?B/s]</p> <p>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 15.8MB/s]</p> <pre><code>class CNN(nnx.Module):\n    \"\"\"A simple CNN model.\"\"\"\n\n    def __init__(self, *, rngs: nnx.Rngs):\n        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n        self.conv2 = nnx.Conv(32, 32, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n        self.conv3 = nnx.Conv(32, 64, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n        self.avg_pool1 = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n        self.avg_pool2 = partial(nnx.avg_pool, window_shape=(3, 3), strides=(1, 1))\n        self.linear1 = nnx.Linear(64, 10, rngs=rngs)\n\n    def __call__(self, x):\n        x = self.avg_pool1(nnx.relu(self.conv1(x)))\n        x = self.avg_pool1(nnx.relu(self.conv2(x)))\n        x = self.avg_pool2(nnx.relu(self.conv3(x)))\n        x = x.flatten()\n        x = self.linear1(x)\n        return x\n\n\n# Instantiate the model\nmodel = CNN(rngs=nnx.Rngs(0))\n\n\n# Create forward function with vmap\n@nnx.vmap(in_axes=(None, 0), out_axes=0)\ndef forward(model: CNN, x):\n    return model(x)\n\n\n# Visualize it\n# nnx.display(model)\n</code></pre>"},{"location":"_examples/0002_laplax_on_mnist/#setup-optimizer","title":"Setup optimizer","text":"<pre><code>learning_rate = 3e-4\nmomentum = 0.9\n\noptimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\nmetrics = nnx.MultiMetric(\n    accuracy=nnx.metrics.Accuracy(),\n    loss=nnx.metrics.Average(\"loss\"),\n)\n</code></pre>"},{"location":"_examples/0002_laplax_on_mnist/#setup-training-step-functions","title":"Setup training step functions","text":"<pre><code>def loss_fn(model: CNN, batch):\n    logits = forward(model, batch[\"input\"])\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch[\"target\"]\n    ).mean()\n    return loss, logits\n\n\n@nnx.jit\ndef train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n    \"\"\"Train for a single step.\"\"\"\n    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n    (loss, logits), grads = grad_fn(model, batch)\n    metrics.update(loss=loss, logits=logits, labels=batch[\"target\"])  # In-place updates\n    optimizer.update(grads)  # In-place updates\n\n\n@nnx.jit\ndef eval_step(model: CNN, metrics: nnx.MultiMetric, batch):\n    loss, logits = loss_fn(model, batch)\n    metrics.update(loss=loss, logits=logits, labels=batch[\"target\"])  # In-place updates\n</code></pre>"},{"location":"_examples/0002_laplax_on_mnist/#training-the-model","title":"Training the model","text":"<pre><code>metrics_history = {\n    \"train_loss\": [],\n    \"train_accuracy\": [],\n    \"test_loss\": [],\n    \"test_accuracy\": [],\n}\n\nfor step, batch in enumerate(train_iter):\n    # Run the optimization for one step and make a stateful update to the following:\n    # - The train state's model parameters\n    # - The optimizer state\n    # - The training loss and accuracy batch metrics\n    train_step(model, optimizer, metrics, batch)\n\n    if step &gt; 0 and (step % eval_every == 0 or step == train_steps - 1):\n        # One training epoch has passed.\n        # Log the training metrics.\n        for metric, value in metrics.compute().items():  # Compute the metrics.\n            metrics_history[f\"train_{metric}\"].append(value)  # Record the metrics.\n        metrics.reset()  # Reset the metrics for the test set.\n\n        # Compute the metrics on the test set after each training epoch.\n        for test_batch in test_loader:\n            eval_step(model, metrics, test_batch)\n\n        # Log the test metrics.\n        for metric, value in metrics.compute().items():\n            metrics_history[f\"test_{metric}\"].append(value)\n        metrics.reset()  # Reset the metrics for the next training epoch.\n\n        print(\n            f\"[train] step: {step}, \"\n            f\"loss: {metrics_history['train_loss'][-1]}, \"\n            f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n        )\n        print(\n            f\"[test] step: {step}, \"\n            f\"loss: {metrics_history['test_loss'][-1]}, \"\n            f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n        )\n</code></pre> <pre><code>[train] step: 200, loss: 1.5579854249954224, accuracy: 55.876861572265625\n[test] step: 200, loss: 0.7801689505577087, accuracy: 80.2784423828125\n\n\n[train] step: 400, loss: 0.5933835506439209, accuracy: 84.0\n[test] step: 400, loss: 0.4330975413322449, accuracy: 87.80047607421875\n\n\n[train] step: 600, loss: 0.3991275429725647, accuracy: 88.609375\n[test] step: 600, loss: 0.34466788172721863, accuracy: 89.56330108642578\n\n\n[train] step: 800, loss: 0.3232867121696472, accuracy: 90.9375\n[test] step: 800, loss: 0.27149900794029236, accuracy: 92.24759674072266\n\n\n[train] step: 1000, loss: 0.29091233015060425, accuracy: 91.8125\n[test] step: 1000, loss: 0.2618582844734192, accuracy: 92.61819458007812\n\n\n[train] step: 1200, loss: 0.2643532454967499, accuracy: 92.765625\n[test] step: 1200, loss: 0.23421761393547058, accuracy: 93.21914672851562\n\n\n[train] step: 1400, loss: 0.22148150205612183, accuracy: 93.546875\n[test] step: 1400, loss: 0.20629382133483887, accuracy: 94.04046630859375\n\n\n[train] step: 1600, loss: 0.23098216950893402, accuracy: 93.515625\n[test] step: 1600, loss: 0.19762158393859863, accuracy: 94.18069458007812\n\n\n[train] step: 1800, loss: 0.21612083911895752, accuracy: 94.171875\n[test] step: 1800, loss: 0.1793205738067627, accuracy: 94.98197174072266\n</code></pre>"},{"location":"_examples/0002_laplax_on_mnist/#check-model-calibration","title":"Check model calibration","text":"<p>So far, we have followed along the standard MNIST tutorial from <code>flax.nnx</code>. Now, we want to check the calibration of the model, i.e. whether the probabilities it assigns to each class label represents its confidence. A good score for this is the ECE (see e.g. Mucs\u00e1nyi2023).</p> <pre><code>%matplotlib inline\nNUM_BINS = 15\n\n# Collect predictions and targets from test dataset\nall_predictions = []\nall_targets = []\n\n\nfor batch in test_loader:\n    # Get predictions for this batch\n    predictions = jax.nn.softmax(forward(model, batch[\"input\"]), axis=1)\n    all_predictions.append(predictions)\n    all_targets.append(batch[\"target\"])\n\n\n# Concatenate all batches\npredictions = jnp.concatenate(all_predictions, axis=0)\ntargets = jnp.concatenate(all_targets, axis=0)\n\n# Calculate confidence and correctness\nmax_prob = predictions.max(axis=-1)\ncorrectness_float = correctness(pred=predictions, target=targets).astype(jnp.float32)\n\nprint(f\"Accuracy: {correctness_float.mean():.4f}\")\n\n# Calculate bin metrics\nbin_proportions, bin_confidences, bin_accuracies = calculate_bin_metrics(\n    confidence=max_prob, correctness=correctness_float, num_bins=NUM_BINS\n)\n\n# Plot the reliability diagram\ncreate_reliability_diagram(\n    bin_confidences=bin_confidences,\n    bin_accuracies=bin_accuracies,\n    num_bins=NUM_BINS,\n)\n\n# Plot the proportion diagram\ncreate_proportion_diagram(\n    bin_proportions=bin_proportions,\n    num_bins=NUM_BINS,\n)\n</code></pre> <pre><code>Accuracy: 0.9420\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"_examples/0002_laplax_on_mnist/#apply-laplace-approximation","title":"Apply Laplace Approximation","text":"<p>First, we create the GGN matrix-vector product. As the GGN has 679,730,993,764 entries in this case, the naive representation of the dense matrix would take approximately 2.718 TB of VRAM. Therefore, it is crucial to represent this matrix-vector product implicitly, as shown below.</p> <pre><code># Create GGN\ngraph_def, params = nnx.split(model)\n\n\ndef model_fn(input, params):\n    return nnx.call((graph_def, params))(input)[0]\n\n\ntrain_batch = next(iter(train_loader))\nggn_mv = create_ggn_mv(\n    model_fn,\n    params,\n    train_batch,\n    loss_fn=\"cross_entropy\",\n    num_total_samples=num_training_samples,\n)\n</code></pre> <p>Alternatively, we can consider the <code>ggn_mv</code> over a dataloader instead of a single training batch. To do so, the following cell needs to be commented out.</p> <pre><code># Set maximum number of batches\nclass LimitedLoader:\n    \"\"\"DataLoader wrapper that limits the number of batches.\"\"\"\n\n    def __init__(self, loader, max_batches):\n        self.loader = loader\n        self.max_batches = max_batches\n\n    def __iter__(self):\n        batch_iter = iter(self.loader)\n        for _ in range(self.max_batches):\n            yield next(batch_iter)\n\n    def __len__(self):\n        return self.max_batches\n\n\nNUM_OF_BATCHES = 4\ntrain_loader_limited = LimitedLoader(train_loader, NUM_OF_BATCHES)\n\n# Setup batch-wise GGN-matrix-vector product\nggn_mv_wo_data = create_ggn_mv_without_data(\n    model_fn,\n    params,\n    loss_fn=\"cross_entropy\",\n    factor=num_training_samples / (train_batch_size * NUM_OF_BATCHES),\n)\n\n# Setup ggn_mv with DataLoader\nggn_mv = DataLoaderMV(\n    ggn_mv_wo_data,\n    train_loader_limited,\n    transform=lambda x: x,\n    reduce=reduce_add,\n    verbose_logging=True,  # Shows progress bar when iterating through data loader.\n)\n</code></pre> <p>Callables such as <code>to_dense</code>, <code>diagonal</code> and <code>wrap_function</code> are lowered into the sum of the <code>DataLoaderMV</code>. The cell below can be executed if the model above is made sufficiently small such that the GGN fits into memory.</p> <p>Note: For the remainder of the notebook, we assume an un-wrapped mv function.</p> <pre><code># from laplax.util.flatten import create_pytree_flattener, wrap_function\n# from laplax.util.mv import to_dense\n# from laplax.util.tree import get_size\n\n# flatten, unflatten = create_pytree_flattener(params)\n# ggn_mv = wrap_function(ggn_mv, input_fn=unflatten, output_fn=flatten)\n# arr = to_dense(ggn_mv, layout=get_size(params))\n</code></pre> <p>Next, we use the GGN matrix-vector product above to obtain a low-rank approximation of the GGN. Even though the dense GGN cannot be represented in memory, its low-rank approximation for a sufficiently low rank remains tractable to hold in memory. Having access to the low-rank GGN terms, we can then efficiently invert an isotropically dampened version of it which is the weight-space covariance matrix of our Laplace approximation.</p> <pre><code># Create Posterior\nposterior_fn = create_posterior_fn(\n    \"lanczos\",\n    mv=ggn_mv,\n    layout=params,\n    key=jax.random.key(0),\n    maxiter=100,\n    mv_jit=True,  # If the loader has side effects, such as i/o operations, then\n    # this should be set to False.\n)\n</code></pre> <p>Processing batches:   0%|          | 0/4 [00:00&lt;?, ?it/s]</p> <p>Processing batches:  25%|\u2588\u2588\u258c       | 1/4 [00:00&lt;00:00,  7.91it/s]</p> <p>Processing batches:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00&lt;00:00, 11.06it/s]</p> <p>Processing batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 11.27it/s]</p> <p>Finally, we need a way to represent the model's uncertainty in its output space, as decisions (such as abstaining from prediction) are made based on the model output, not on the weight space.</p> <pre><code>prior_arguments = {\"prior_prec\": 10000.0}\npushforward_fns = [\n    lin_setup,\n    lin_pred_mean,\n    lin_mc_pred_act,\n]\n\npushforward_fn = set_lin_pushforward(\n    model_fn=model_fn,\n    mean_params=params,\n    posterior_fn=posterior_fn,\n    prior_arguments=prior_arguments,\n    pushforward_fns=pushforward_fns,\n    key=jax.random.key(0),\n    num_samples=30,\n)\n\n# Set up two versions of the pushforward function - with and without vmap.\npushforward_fn_jit = jax.jit(pushforward_fn)\npushforward_fn_jit_vmap = jax.jit(jax.vmap(pushforward_fn))\n\n\n# Define the metrics function (ideally with batch dimension)\ndef confidences_map(map_, **kwargs):\n    del kwargs\n    return jnp.max(jax.nn.softmax(map_, axis=-1), axis=-1)\n\n\ndef confidences_pred(pred_act, **kwargs):\n    del kwargs\n    return jnp.max(pred_act, axis=-1)\n</code></pre> <pre><code>results = evaluate_metrics_on_generator(\n    pushforward_fn_jit,\n    test_loader,\n    metrics=[\n        transfer_entry([\"pred_mean\", \"map\", \"mc_pred_act\"]),\n        apply_fns(\n            confidences_map,\n            confidences_pred,\n            correctness,\n            names=[\"confidences_map\", \"confidences_pred\", \"correctness_map\"],\n            map_=\"map\",\n            pred_act=\"mc_pred_act\",\n            pred=\"map\",\n            target=\"target\",\n        ),\n        apply_fns(\n            correctness,\n            names=[\"correctness_pred\"],\n            pred=\"mc_pred_act\",\n            target=\"target\",\n        ),\n    ],\n    reduce=jnp.concatenate,\n    vmap_over_data=True,\n)\n\nconfidences_map_val = results[\"confidences_map\"].astype(jnp.float32)\ncorrectness_map_val = results[\"correctness_map\"].astype(jnp.float32)\nconfidences_pred_val = results[\"confidences_pred\"].astype(jnp.float32)\ncorrectness_pred_val = results[\"correctness_pred\"].astype(jnp.float32)\n\nece_map = expected_calibration_error(\n    confidence=confidences_map_val, correctness=correctness_map_val, num_bins=NUM_BINS\n)\nece_pred = expected_calibration_error(\n    confidence=confidences_pred_val, correctness=correctness_pred_val, num_bins=NUM_BINS\n)\n\nprint(f\"MAP ECE: {ece_map:.4f}\")\nprint(f\"MAP acc: {correctness_map_val.mean():.4f}\")\nprint(f\"Laplace ECE: {ece_pred:.4f}\")\nprint(f\"Laplace acc: {correctness_pred_val.mean():.4f}\")\n</code></pre> <pre><code>MAP ECE: 0.0224\nMAP acc: 0.9420\nLaplace ECE: 0.0228\nLaplace acc: 0.9421\n</code></pre> <p>If all functions (pushforward and metrics) support a batch dimension, then we can set <code>has_batch</code> to <code>False</code>. Otherwise, <code>has_batch=True</code> will apply <code>jax.lax.map</code> along the batch dimension.</p> <pre><code>results = evaluate_metrics_on_generator(\n    pushforward_fn_jit_vmap,\n    test_loader,\n    metrics=[\n        transfer_entry([\"pred_mean\", \"map\", \"mc_pred_act\"]),\n        apply_fns(\n            confidences_map,\n            confidences_pred,\n            correctness,\n            # correctness_pred_act,\n            names=[\n                \"confidences_map\",\n                \"confidences_pred\",\n                \"correctness_map\",\n                # \"correctness_pred\"\n            ],\n            map_=\"map\",\n            pred_act=\"mc_pred_act\",\n            pred=\"map\",\n            target=\"target\",\n        ),\n        apply_fns(\n            correctness,\n            names=[\"correctness_pred\"],\n            pred=\"mc_pred_act\",\n            target=\"target\",\n        ),\n    ],\n    reduce=jnp.concatenate,\n    vmap_over_data=False,\n    # If all functions handle batch dimensions properly, then setting false works.\n)\n\nconfidences_map_val = results[\"confidences_map\"].astype(jnp.float32)\ncorrectness_map_val = results[\"correctness_map\"].astype(jnp.float32)\nconfidences_pred_val = results[\"confidences_pred\"].astype(jnp.float32)\ncorrectness_pred_val = results[\"correctness_pred\"].astype(jnp.float32)\n\nece_map = expected_calibration_error(\n    confidence=confidences_map_val, correctness=correctness_map_val, num_bins=NUM_BINS\n)\nece_pred = expected_calibration_error(\n    confidence=confidences_pred_val, correctness=correctness_pred_val, num_bins=NUM_BINS\n)\n\nprint(f\"MAP ECE: {ece_map:.4f}\")\nprint(f\"MAP acc: {correctness_map_val.mean():.4f}\")\nprint(f\"Laplace ECE: {ece_pred:.4f}\")\nprint(f\"Laplace acc: {correctness_pred_val.mean():.4f}\")\n</code></pre> <pre><code>MAP ECE: 0.0224\nMAP acc: 0.9420\nLaplace ECE: 0.0228\nLaplace acc: 0.9421\n</code></pre> <pre><code>\n</code></pre>"},{"location":"reference/curv/","title":"Curvature Module","text":""},{"location":"reference/curv/#curvatures","title":"Curvatures","text":"<p>Currently supported curvature-vector products are:</p> <ul> <li> <p>GGN-mv (Generalized Gauss-Newton):</p> \\[ v \\mapsto \\sum_{n=1}^{N} \\mathcal{J}_\\theta^\\top(f_{\\theta^*}(x_n)) \\nabla^2_{f_{\\theta^*}(x_n),f_{\\theta^*}(x_n)} \\ell(f_\\theta(x_n), y_n) \\mathcal{J}_\\theta(f_{\\theta^*})\\, v \\] </li> <li> <p>Hessian-mv (Hessian):     $$     v \\mapsto \\sum_{n=1}^{N} \\nabla_{\\theta \\theta}^2 \\ell(f_\\theta(x_n), y_n)\\,v     $$</p> </li> </ul>"},{"location":"reference/curv/#curvature-estimatorsapproximations","title":"Curvature estimators/approximations","text":"<p>For both curvature-vector products, the following methods are supported for approximating and transforming them into a weight space covariance matrix-vector product:</p> <ul> <li> <p><code>CurvApprox.FULL</code> denses the curvature-vector product into a full matrix. The posterior function is then given by</p> \\[ (\\tau, \\mathcal{C}) \\mapsto \\left[ v \\mapsto \\left(\\textbf{Curv}(\\mathcal{C}) + \\tau I \\right)^{-1} v \\right]. \\] </li> <li> <p><code>CurvApprox.DIAGONAL</code> approximates the curvature using only its diagonal, obtained by evaluating the curvature-vector product with standard basis vectors from both sides. This leads to:</p> \\[ (\\tau, \\mathcal{C}) \\mapsto \\left[ v \\mapsto \\left(\\text{diag}(\\textbf{Curv}(\\mathcal{C}) + \\tau I \\right)^{-1}v  \\right]. \\] </li> <li> <p>Low-Rank employs either a custom Lanczos routine (<code>CurvApprox.LANCZOS</code>) or a variant of the LOBPCG algorithm (<code>CurvApprox.LOBPCG</code>). These methods approximate the top eigenvectors \\(U\\) and eigenvalues \\(S\\) of the curvature via matrix-vector products. The posterior is then given by a low-rank plus scaled diagonal:</p> \\[ (\\tau, \\mathcal{C}) \\mapsto \\left[ v \\mapsto \\left(\\big[U S U^\\top\\big](\\mathcal{C}) + \\tau I \\right)^{-1} v \\right]. \\] </li> </ul>"},{"location":"reference/curv/#main-computational-scaffold","title":"Main computational scaffold","text":"<p>This pipeline is controlled via the following three functions:</p> <ul> <li> <p><code>laplax.curv.estimate_curvature</code>: Estimates the curvature based on the provided type and curvature-vector-product.</p> </li> <li> <p><code>laplax.curv.set_posterior_fn</code>: Takes an estimated curvature and returns a function that maps <code>prior_arguments</code> to the posterior.</p> </li> <li> <p><code>laplax.curv.create_posterior_fn</code>: Combines the <code>estimate_curvature</code> and <code>set_posterior_fn</code>.</p> </li> </ul>"},{"location":"reference/curv/#laplaxcurvestimate_curvature","title":"laplax.curv.estimate_curvature","text":"<p>Estimate the curvature based on the provided type.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvApprox | str</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>mv</code> <code>CurvatureMV</code> <p>Function representing the curvature-vector product.</p> required <code>layout</code> <code>Layout | None</code> <p>Defines the input layer format of the matrix-vector products. If None or an integer, no flattening/unflattening is used.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional key-word arguments passed to the curvature estimation function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>The estimated curvature.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def estimate_curvature(\n    curv_type: CurvApprox | str,\n    mv: CurvatureMV,\n    layout: Layout | None = None,\n    **kwargs: Kwargs,\n) -&gt; PyTree:\n    \"\"\"Estimate the curvature based on the provided type.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        mv: Function representing the curvature-vector product.\n        layout: Defines the input layer format of the matrix-vector products. If None or\n            an integer, no flattening/unflattening is used.\n        **kwargs: Additional key-word arguments passed to the curvature estimation\n            function.\n\n    Returns:\n        The estimated curvature.\n    \"\"\"\n    curv_estimate = CURVATURE_METHODS[curv_type](mv, layout=layout, **kwargs)\n\n    # Ignore lazy evaluation\n    curv_estimate = jax.tree.map(\n        lambda x: x.block_until_ready() if isinstance(x, jax.Array) else x,\n        curv_estimate,\n    )\n\n    return curv_estimate\n</code></pre>"},{"location":"reference/curv/#laplaxcurvset_posterior_fn","title":"laplax.curv.set_posterior_fn","text":"<p>Set the posterior function based on the curvature estimate.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvatureKeyType</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>curv_estimate</code> <code>PyTree</code> <p>Estimated curvature.</p> required <code>layout</code> <code>Layout</code> <p>Defines the input/output layout of the corresponding curvature-vector products. If <code>None</code> or an integer, no flattening/unflattening is used.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional key-word arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function that computes the posterior state.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When layout is neither an integer, a PyTree, nor None.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def set_posterior_fn(\n    curv_type: CurvatureKeyType,\n    curv_estimate: PyTree,\n    *,\n    layout: Layout,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Set the posterior function based on the curvature estimate.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        curv_estimate: Estimated curvature.\n        layout: Defines the input/output layout of the corresponding curvature-vector\n            products. If `None` or an integer, no flattening/unflattening is used.\n        **kwargs: Additional key-word arguments (unused).\n\n    Returns:\n        A function that computes the posterior state.\n\n    Raises:\n        ValueError: When layout is neither an integer, a PyTree, nor None.\n    \"\"\"\n    del kwargs\n    if layout is not None and not isinstance(layout, int | PyTree):\n        msg = \"Layout must be an integer, PyTree or None.\"\n        raise ValueError(msg)\n\n    # Create functions for flattening and unflattening if required\n    if layout is None or isinstance(layout, int):\n        flatten = unflatten = None\n    else:\n        # Use custom flatten/unflatten functions for complex pytrees\n        flatten, unflatten = create_pytree_flattener(layout)\n\n    def posterior_fn(\n        prior_arguments: PriorArguments,\n        loss_scaling_factor: Float = 1.0,\n    ) -&gt; PosteriorState:\n        \"\"\"Compute the posterior state.\n\n        Args:\n            prior_arguments: Prior arguments for the posterior.\n            loss_scaling_factor: Factor by which the user-provided loss function is\n                scaled. Defaults to 1.0.\n\n        Returns:\n            PosteriorState: Dictionary containing:\n\n                - 'state': Updated state of the posterior.\n                - 'cov_mv': Function to compute covariance matrix-vector product.\n                - 'scale_mv': Function to compute scale matrix-vector product.\n        \"\"\"\n        # Calculate posterior precision.\n        precision = CURVATURE_PRECISION_METHODS[curv_type](\n            curv_estimate=curv_estimate,\n            prior_arguments=prior_arguments,\n            loss_scaling_factor=loss_scaling_factor,\n        )\n\n        # Calculate posterior state\n        state = CURVATURE_TO_POSTERIOR_STATE[curv_type](precision)\n\n        # Extract matrix-vector product\n        scale_mv_from_state = CURVATURE_STATE_TO_SCALE[curv_type]\n        cov_mv_from_state = CURVATURE_STATE_TO_COV[curv_type]\n\n        return Posterior(\n            state=state,\n            cov_mv=wrap_factory(cov_mv_from_state, flatten, unflatten),\n            scale_mv=wrap_factory(scale_mv_from_state, flatten, unflatten),\n        )\n\n    return posterior_fn\n</code></pre>"},{"location":"reference/curv/#laplaxcurvcreate_posterior_fn","title":"laplax.curv.create_posterior_fn","text":"<p>Factory function to create the posterior function given a curvature type.</p> <p>This sets up the posterior function, which can then be initiated using <code>prior_arguments</code> by computing a specified curvature approximation and encoding the sequential computational order of:</p> <pre><code>1. `CURVATURE_PRIOR_METHODS`\n2. `CURVATURE_TO_POSTERIOR_STATE`\n3. `CURVATURE_STATE_TO_SCALE`\n4. `CURVATURE_STATE_TO_COV`\n</code></pre> <p>All methods are selected from the corresponding dictionary by the <code>curv_type</code> argument. New methods can be registered using the :func:<code>laplax.register.register_curvature_method</code> method. See the :mod:<code>laplax.register</code> module for more details.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvApprox | str</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>mv</code> <code>CurvatureMV</code> <p>Function representing the curvature.</p> required <code>layout</code> <code>Layout | None</code> <p>Defines the format of the layout for matrix-vector products. If <code>None</code> or an integer, no flattening/unflattening is used.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments passed to the curvature estimation function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A posterior function that takes the <code>prior_arguments</code> and returns the <code>posterior_state</code>.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def create_posterior_fn(\n    curv_type: CurvApprox | str,\n    mv: CurvatureMV,\n    layout: Layout | None = None,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Factory function to create the posterior function given a curvature type.\n\n    This sets up the posterior function, which can then be initiated using\n    `prior_arguments` by computing a specified curvature approximation and encoding the\n    sequential computational order of:\n\n        1. `CURVATURE_PRIOR_METHODS`\n        2. `CURVATURE_TO_POSTERIOR_STATE`\n        3. `CURVATURE_STATE_TO_SCALE`\n        4. `CURVATURE_STATE_TO_COV`\n\n    All methods are selected from the corresponding dictionary by the `curv_type`\n    argument. New methods can be registered using the\n    :func:`laplax.register.register_curvature_method` method.\n    See the :mod:`laplax.register` module for more details.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        mv: Function representing the curvature.\n        layout: Defines the format of the layout for matrix-vector products. If `None`\n            or an integer, no flattening/unflattening is used.\n        **kwargs: Additional keyword arguments passed to the curvature estimation\n            function.\n\n    Returns:\n        A posterior function that takes the `prior_arguments` and returns the\n            `posterior_state`.\n    \"\"\"\n    # Retrieve the curvature estimator based on the provided type\n    curv_estimate = estimate_curvature(curv_type, mv=mv, layout=layout, **kwargs)\n\n    # Set posterior fn based on curv_estimate\n    posterior_fn = set_posterior_fn(curv_type, curv_estimate, layout=layout)\n\n    return posterior_fn\n</code></pre>"},{"location":"reference/enums/","title":"laplax.enums","text":"<p>Enums for Laplace approximations.</p>"},{"location":"reference/enums/#laplax.enums.CalibrationObjective","title":"CalibrationObjective","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported calibration objectives (minimisation!).</p> Source code in <code>laplax/enums.py</code> <pre><code>class CalibrationObjective(StrEnum):\n    \"\"\"Supported calibration objectives (minimisation!).\"\"\"\n\n    NLL = \"nll\"\n    CHI_SQUARED = \"chi_squared\"\n    MARGINAL_LOG_LIKELIHOOD = \"marginal_log_likelihood\"\n    ECE = \"ece\"\n</code></pre>"},{"location":"reference/enums/#laplax.enums.CalibrationMethod","title":"CalibrationMethod","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported calibration methods.</p> Source code in <code>laplax/enums.py</code> <pre><code>class CalibrationMethod(StrEnum):\n    \"\"\"Supported calibration methods.\"\"\"\n\n    GRID_SEARCH = \"grid_search\"\n</code></pre>"},{"location":"reference/enums/#laplax.enums.Pushforward","title":"Pushforward","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported pushforward types for pushing forward weight space uncertainty.</p> Source code in <code>laplax/enums.py</code> <pre><code>class Pushforward(StrEnum):\n    \"\"\"Supported pushforward types for pushing forward weight space uncertainty.\"\"\"\n\n    LINEAR = \"linear\"\n    NONLINEAR = \"nonlinear\"\n</code></pre>"},{"location":"reference/enums/#laplax.enums.Predictive","title":"Predictive","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported predictive types for crossing softmax transformation.</p> Source code in <code>laplax/enums.py</code> <pre><code>class Predictive(StrEnum):\n    \"\"\"Supported predictive types for crossing softmax transformation.\"\"\"\n\n    MC_BRIDGE = \"mc_bridge\"\n    LAPLACE_BRIDGE = \"laplace_bridge\"\n    MEAN_FIELD_0 = \"mean_field_0\"\n    MEAN_FIELD_1 = \"mean_field_1\"\n    MEAN_FIELD_2 = \"mean_field_2\"\n    NONE = \"none\"  # Intended for regression flag\n</code></pre>"},{"location":"reference/enums/#laplax.enums.DefaultMetrics","title":"DefaultMetrics","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported default metric settings.</p> Source code in <code>laplax/enums.py</code> <pre><code>class DefaultMetrics(StrEnum):\n    \"\"\"Supported default metric settings.\"\"\"\n\n    CLASSIFICATION = \"classification\"\n    REGRESSION = \"regression\"\n</code></pre>"},{"location":"reference/eval/","title":"Evaluation Module","text":"<p>The evaluation module provides functionality for propagating weight-space uncertainty to output-space predictions, calibrating probabilistic predictions on datasets, and evaluating various uncertainty metrics on datasets as well as the marginal log-likelihood.</p>"},{"location":"reference/eval/#pushforward-methods","title":"Pushforward Methods","text":"<p>Currently supported pushforward methods are:</p> <ul> <li> <p>Pushforward.LINEAR:   Propagates the weight-space covariance \\(\\mathbf{H}^{-1}\\) through the network using Jacobian-vector products \\(\\mathcal{J}_{\\theta}(f(x_n; \\theta))\\) to obtain a Gaussian predictive distribution in output space:</p> \\[ \\mathcal{N}\\bigg(     f(x_n; \\theta^*),\\     \\mathcal{J}_\\theta(f(x_n; \\theta^*))\\ \\mathbf{H}^{-1}\\ \\mathcal{J}_\\theta(f(x_n; \\theta^*))^\\top \\bigg) \\] </li> <li> <p>Pushforward.NONLINEAR:   Draws samples from the weight-space posterior and passes them through the neural network to form an ensemble of predictions in output space. Empirical statistics (mean, variance, etc.) are then computed from this ensemble:</p> </li> </ul> <p>$$   f(x_n, \\theta_s), \\quad \\theta_s \\sim \\mathcal{N}(\\theta^*,\\ \\mathbf{H}^{-1})   $$</p>"},{"location":"reference/eval/#predictives","title":"Predictives","text":"<p>For the classification case, the following predictives for pushing the uncertainty from the weight space to the output space are supported:</p> <ul> <li> <p><code>MC_BRIDGE</code>: Draw \\(z_s\\sim\\mathcal{N}\\big(\\mu,\\Sigma\\big)\\), compute \\(p_s=\\operatorname{softmax}(z_s)\\) for \\(s=1\\ldots S\\), and form \\(\\frac1S\\sum_s p_s\\).</p> </li> <li> <p><code>LAPLACE_BRIDGE</code>: Transforms the Gaussian over logits into a Dirichlet by moment\u2011matching (\u201cbridge\u201d), yielding closed\u2011form Dirichlet parameters and thus an analytic predictive mean. The Laplace Bridge predictive approximates the true predictive as follows:     \\begin{equation}         \\mathbf{\\hat p} := \\frac{\\frac{1}{\\mathbf{\\tilde\\sigma}^2}\\left(1-\\frac{2}{C}+\\frac{e^{\\mathbf{\\tilde\\mu}}}{C^2}\\sum_{c=1}^Ce^{-\\tilde\\mu_c}\\right)}{\\sum_{c=1}^C\\frac{1}{\\tilde\\sigma^2_c}\\left(1-\\frac{2}{C}+\\frac{e^{\\mathbf{\\tilde\\mu}}}{C^2}\\sum_{c'=1}^Ce^{-\\tilde\\mu_{c'}}\\right)}     \\end{equation}     where     \\begin{equation}         \\mathbf{\\tilde\\mu}^2 := \\sqrt{\\frac{\\sqrt{C/2}} {\\sum_{c=1}^C\\sigma^2_c}}\\mathbf\\mu,\\; \\mathbf{\\tilde\\sigma^2} := \\frac{\\sqrt{C/2}} {\\sum_{c=1}^C\\sigma^2_c}\\mathbf\\sigma^2.     \\end{equation}</p> </li> <li> <p><code>MEAN_FIELD_0_PREDICTIVE</code>:  A zeroth\u2010order mean\u2010field (probit\u2010style) approximation.</p> \\[ \\mathbb{E}[\\operatorname{softmax}_i(\\mathbf{z})]\\approx\\operatorname{softmax}_i\\left(\\frac{\\mathbf{\\mu}}{\\sqrt{1+\\lambda_0\\,\\operatorname{diag}(\\mathbf{\\Sigma}})}\\right) \\] <p>which rescales each mean logit by its variance.</p> </li> <li> <p><code>MEAN_FIELD_1_PREDICTIVE</code>: A first\u2010order pairwise approximation: for each \\(i\\) we approximate \\(\\Pr(z_i&gt;z_j)\\) under the bivariate Gaussian of \\((z_i,z_j)\\) and then normalize:</p> \\[ \\mathbb{E}[\\operatorname{softmax}_i(\\mathbf{z})]\\approx \\frac{1}{1 + \\sum_{i \\neq k} \\exp \\left( -\\frac{(\\mu_k - \\mu_i)}{\\sqrt{1 + \\lambda_0 (\\Sigma_{kk} + \\Sigma_{ii})}} \\right)}. \\] </li> <li> <p><code>MEAN_FIELD_2_PREDICTIVE</code>:  A second\u2010order correction that incorporates full covariance: uses all bivariate variances and covariances in the exponentiated difference integrals:</p> \\[ \\mathbb{E}[\\operatorname{softmax}_i(\\mathbf{z})]\\approx \\frac{1}{1 + \\sum_{i \\neq k} \\exp \\left( -\\frac{(\\mu_k - \\mu_i)}{\\sqrt{1 + \\lambda_0 (\\Sigma_{kk} + \\Sigma_{ii} - 2\\Sigma_{ik})}} \\right)}. \\] </li> </ul> <p>Each method trades off cost versus fidelity: sampling is asymptotically exact but can be slow; the mean\u2011field approximations incur only \\(O(C^2)\\) or \\(O(C)\\) work; and the Laplace bridge often gives the best calibrated probabilities when variances are large.</p>"},{"location":"reference/eval/#calibration","title":"Calibration","text":"<p>The module <code>laplax.eval.calibrate</code> provides functionality for calibrating the <code>prior_prec</code> parameter of posterior covariance using a grid search.</p>"},{"location":"reference/eval/#metrics","title":"Metrics","text":"<p>The submodule <code>laplax.eval.metrics</code> supports simple metrics for evaluating the uncertainty of the predictions, which support both classification and regression. The main evaluation functions can be simply extended to support other forms of evaluations.</p>"},{"location":"reference/eval/#main-computational-scaffold","title":"Main computational scaffold","text":"<p>The main pipeline functionality operates by mapping a list of functions over the predictions. This is done into rounds. The first round computes uncertainty related summaries such as mean, standard deviation, variance, covariance and/or samples; and the second round apply metrics to these predictions. Both lists of functions are applied iteratively making the results of previous functions available to the next functions, which allows for efficient shortcuts (e.g. computing the variance as the diagonal of the previously computed covariance matrix). To allow a flexible interface for operating on such results we provide the <code>apply_fns</code> and <code>transfer_entry</code> functions. These can be used in combination with <code>evaluate_on_dataset</code> and <code>evaluate_on_generator</code> to evaluate the uncertainty of the predictions on a dataset or generator, such as a DataLoader.</p>"},{"location":"reference/main_api/","title":"Laplax API","text":"<p>GGN </p> <p>Create a GGN matrix-vector product function.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>Neural network forward pass.</p> required <code>params</code> <code>Params</code> <p>Network parameters.</p> required <code>data</code> <code>Data | Iterable</code> <p>Training data.</p> required <code>loss_fn</code> <code>LossFn</code> <p>Loss function to use.</p> required <code>factor</code> <code>float</code> <p>Scaling factor for GGN.</p> <code>1.0</code> <code>vmap_over_data</code> <code>bool</code> <p>Whether model expects batch dimension.</p> <code>True</code> <code>verbose_logging</code> <code>bool</code> <p>Whether to enable verbose logging.</p> <code>True</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to data.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Params], Params]</code> <p>GGN matrix-vector product function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input/output shapes don't match.</p> Source code in <code>laplax/api.py</code> <pre><code>def GGN(\n    model_fn: ModelFn,\n    params: Params,\n    data: Data | Iterable,\n    loss_fn: LossFn,\n    *,\n    factor: float = 1.0,\n    vmap_over_data: bool = True,\n    verbose_logging: bool = True,\n    transform: Callable | None = None,\n) -&gt; Callable[[Params], Params]:\n    \"\"\"Create a GGN matrix-vector product function.\n\n    Args:\n        model_fn: Neural network forward pass.\n        params: Network parameters.\n        data: Training data.\n        loss_fn: Loss function to use.\n        factor: Scaling factor for GGN.\n        vmap_over_data: Whether model expects batch dimension.\n        verbose_logging: Whether to enable verbose logging.\n        transform: Transform to apply to data.\n\n    Returns:\n        GGN matrix-vector product function.\n\n    Raises:\n        ValueError: If input/output shapes don't match.\n    \"\"\"\n    ggn_mv = create_ggn_mv_without_data(  # type: ignore[call-arg]\n        model_fn=model_fn,\n        params=params,\n        loss_fn=loss_fn,\n        factor=factor,\n        vmap_over_data=vmap_over_data,\n    )\n\n    mv_bound = _maybe_wrap_loader_or_batch(\n        ggn_mv,\n        data,\n        transform=transform,\n        loader_kwargs={\n            \"verbose_logging\": verbose_logging,\n        },\n    )\n\n    test = mv_bound(params)\n    if not jax.tree.all(\n        jax.tree.map(lambda x, y: x.shape == y.shape, test, params),\n    ):\n        msg = \"Setup of GGN-MV failed: input and output shapes do not match.\"\n        raise ValueError(msg)\n\n    return mv_bound\n</code></pre> <p>laplace </p> <p>Estimate curvature &amp; obtain a Gaussian weight-space posterior.</p> <p>This function computes a Laplace approximation to the posterior distribution over neural network weights. It estimates the curvature of the loss landscape and constructs a Gaussian approximation centered at the MAP estimate.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The neural network forward pass function that takes input and parameters.</p> required <code>params</code> <code>Params</code> <p>The MAP estimate of the network parameters.</p> required <code>data</code> <code>Data | Iterable</code> <p>Either a single batch (tuple/dict) or a DataLoader-like iterable containing the training data.</p> required <code>loss_fn</code> <code>LossFn</code> <p>The supervised loss function to use (e.g., \"mse\" for regression).</p> required <code>curv_type</code> <code>CurvApprox</code> <p>Type of curvature approximation to use (e.g., \"ggn\", \"diag-ggn\").</p> required <code>num_curv_samples</code> <code>Int</code> <p>Number of Monte Carlo samples used to estimate the GGN, by default 1.</p> <code>1</code> <code>num_total_samples</code> <code>Int</code> <p>Total number of samples in the dataset, by default 1.</p> <code>1</code> <code>vmap_over_data</code> <code>bool</code> <p>Whether the model expects a leading batch axis, by default True.</p> <code>True</code> <code>curv_mv_jit</code> <code>bool</code> <p>Whether to jit the curvature matrix-vector product, by default False.</p> <code>False</code> <code>**curv_kwargs</code> <code>Kwargs</code> <p>Additional arguments forwarded to the curvature estimation function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Callable[[PriorArguments, Float], Posterior], PyTree]</code> <p>A tuple containing:</p> <ul> <li>posterior_fn: Function that generates samples from the posterior given     prior arguments.</li> <li>curv_estimate: The estimated curvature in the chosen representation.</li> </ul> Notes <p>The function supports different curvature approximations:</p> <ul> <li>Full GGN: Computes the full Generalized Gauss-Newton matrix</li> <li>Diagonal GGN: Approximates the GGN with its diagonal</li> <li>Low-rank GGN: Uses Lanczos or LOBPCG for efficient approximation</li> </ul> Source code in <code>laplax/api.py</code> <pre><code>def laplace(\n    model_fn: ModelFn,\n    params: Params,\n    data: Data | Iterable,\n    *,\n    loss_fn: LossFn,\n    curv_type: CurvApprox,\n    num_curv_samples: Int = 1,\n    num_total_samples: Int = 1,\n    vmap_over_data: bool = True,\n    curv_mv_jit: bool = False,\n    **curv_kwargs: Kwargs,\n) -&gt; tuple[Callable[[PriorArguments, Float], Posterior], PyTree]:\n    \"\"\"Estimate curvature &amp; obtain a Gaussian weight-space posterior.\n\n    This function computes a Laplace approximation to the posterior distribution over\n    neural network weights. It estimates the curvature of the loss landscape and\n    constructs a Gaussian approximation centered at the MAP estimate.\n\n    Args:\n        model_fn: The neural network forward pass function that takes input and\n            parameters.\n        params: The MAP estimate of the network parameters.\n        data: Either a single batch (tuple/dict) or a DataLoader-like iterable\n            containing the training data.\n        loss_fn: The supervised loss function to use (e.g., \"mse\" for regression).\n        curv_type: Type of curvature approximation to use (e.g., \"ggn\", \"diag-ggn\").\n        num_curv_samples: Number of Monte Carlo samples used to estimate the GGN, by\n            default 1.\n        num_total_samples: Total number of samples in the dataset, by default 1.\n        vmap_over_data: Whether the model expects a leading batch axis, by default True.\n        curv_mv_jit: Whether to jit the curvature matrix-vector product, by default\n            False.\n        **curv_kwargs: Additional arguments forwarded to the curvature estimation\n            function.\n\n    Returns:\n        A tuple containing:\n\n            - posterior_fn: Function that generates samples from the posterior given\n                prior arguments.\n            - curv_estimate: The estimated curvature in the chosen representation.\n\n    Notes:\n        The function supports different curvature approximations:\n\n        - Full GGN: Computes the full Generalized Gauss-Newton matrix\n        - Diagonal GGN: Approximates the GGN with its diagonal\n        - Low-rank GGN: Uses Lanczos or LOBPCG for efficient approximation\n    \"\"\"\n    # Convert curv_type to enum\n    curv_type_enum = _convert_to_enum(CurvApprox, curv_type)\n\n    # Calculate factor\n    factor = float(num_curv_samples) / float(num_total_samples)\n    logger.debug(\n        \"Creating curvature MV - factor = {}/{} = {}\",\n        num_curv_samples,\n        num_total_samples,\n        factor,\n    )\n\n    # Set GGN MV\n    ggn_mv = GGN(\n        model_fn,\n        params,\n        data,\n        loss_fn=loss_fn,\n        factor=factor,\n        vmap_over_data=vmap_over_data,\n    )\n    if curv_mv_jit:\n        ggn_mv = jax.jit(ggn_mv)\n\n    # Curvature estimation\n    curv_estimate = estimate_curvature(\n        curv_type=curv_type_enum,\n        mv=ggn_mv,\n        layout=params,\n        **curv_kwargs,\n    )\n    logger.debug(\"Curvature estimated: {}\", curv_type_enum)\n\n    # Posterior (Gaussian)\n    posterior_fn = set_posterior_fn(\n        curv_type=curv_type_enum,\n        curv_estimate=curv_estimate,\n        layout=params,\n        **curv_kwargs,\n    )\n    logger.debug(\"Posterior callable constructed.\")\n\n    return posterior_fn, curv_estimate\n</code></pre> <p>calibration </p> <p>Calibrate hyperparameters of the Laplace approximation.</p> <p>This function tunes the prior precision (or similar hyperparameters) of the Laplace approximation by optimizing a specified objective function. It supports different calibration objectives and methods.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_fn</code> <code>Callable[[PriorArguments, Float], Posterior]</code> <p>Function that generates samples from the posterior.</p> required <code>model_fn</code> <code>ModelFn</code> <p>The neural network forward pass function.</p> required <code>params</code> <code>Params</code> <p>The MAP estimate of the network parameters.</p> required <code>data</code> <code>Data</code> <p>The validation data used for calibration.</p> required <code>loss_fn</code> <code>LossFn</code> <p>The supervised loss function used for training.</p> required <code>curv_estimate</code> <code>PyTree</code> <p>The estimated curvature from the Laplace approximation.</p> required <code>curv_type</code> <code>CurvApprox</code> <p>Type of curvature approximation used.</p> required <code>predictive_type</code> <code>Predictive | str</code> <p>Type of predictive distribution to use, by default Predictive.NONE.</p> <code>NONE</code> <code>pushforward_type</code> <code>Pushforward | str</code> <p>Type of pushforward approximation to use, by default Pushforward.LINEAR.</p> <code>LINEAR</code> <code>pushforward_fns</code> <code>list[Callable] | None</code> <p>Custom pushforward functions to use, by default None.</p> <code>None</code> <code>sample_key</code> <code>KeyType</code> <p>PRNG key.</p> <code>DEFAULT_KEY</code> <code>num_samples</code> <code>int</code> <p>Number of MC samples for the predictive.</p> <code>30</code> <code>calibration_objective</code> <code>CalibrationObjective | str</code> <p>Objective function to optimize during calibration, by default CalibrationObjective.NLL.</p> <code>NLL</code> <code>calibration_method</code> <code>CalibrationMethod | str</code> <p>Method to use for calibration, by default CalibrationMethod.GRID_SEARCH.</p> <code>GRID_SEARCH</code> <code>vmap_over_data</code> <code>bool</code> <p>Whether the model expects a leading batch axis, by default True.</p> <code>True</code> <code>objective_jit</code> <code>bool</code> <p>Whether to jit the calibration objective, by default True.</p> <code>True</code> <code>**calibration_kwargs</code> <code>Kwargs</code> <p>Additional arguments for the calibration method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[PriorArguments, Callable[[InputArray], dict[str, Array]]]</code> <p>A tuple containing:</p> <ul> <li>prior_arguments : PriorArguments     Dictionary of calibrated hyperparameters.</li> <li>set_prob_predictive : Callable     Function that creates a predictive distribution given prior arguments.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>When an unknown calibration method is provided.</p> Notes <p>Supported calibration objectives:</p> <ul> <li>NLL: Negative log-likelihood</li> <li>CHI_SQUARED: Chi-squared statistic</li> <li>MARGINAL_LOG_LIKELIHOOD: Marginal log-likelihood</li> <li>ECE: Expected Calibration Error</li> </ul> <p>Supported calibration methods:</p> <ul> <li>GRID_SEARCH: Grid search over prior precision</li> </ul> Source code in <code>laplax/api.py</code> <pre><code>def calibration(\n    posterior_fn: Callable[[PriorArguments, Float], Posterior],\n    model_fn: ModelFn,\n    params: Params,\n    data: Data,\n    *,\n    loss_fn: LossFn,\n    curv_estimate: PyTree,\n    curv_type: CurvApprox,\n    predictive_type: Predictive | str = Predictive.NONE,\n    pushforward_type: Pushforward | str = Pushforward.LINEAR,\n    pushforward_fns: list[Callable] | None = None,\n    sample_key: KeyType = DEFAULT_KEY,\n    num_samples: int = 30,\n    calibration_objective: CalibrationObjective | str = CalibrationObjective.NLL,\n    calibration_method: CalibrationMethod | str = CalibrationMethod.GRID_SEARCH,\n    vmap_over_data: bool = True,\n    objective_jit: bool = True,\n    **calibration_kwargs: Kwargs,\n) -&gt; tuple[PriorArguments, Callable[[InputArray], dict[str, Array]]]:\n    \"\"\"Calibrate hyperparameters of the Laplace approximation.\n\n    This function tunes the prior precision (or similar hyperparameters) of the Laplace\n    approximation by optimizing a specified objective function. It supports different\n    calibration objectives and methods.\n\n    Args:\n        posterior_fn: Function that generates samples from the posterior.\n        model_fn: The neural network forward pass function.\n        params: The MAP estimate of the network parameters.\n        data: The validation data used for calibration.\n        loss_fn: The supervised loss function used for training.\n        curv_estimate: The estimated curvature from the Laplace approximation.\n        curv_type: Type of curvature approximation used.\n        predictive_type: Type of predictive distribution to use, by default\n            Predictive.NONE.\n        pushforward_type: Type of pushforward approximation to use, by default\n            Pushforward.LINEAR.\n        pushforward_fns: Custom pushforward functions to use, by default None.\n        sample_key: PRNG key.\n        num_samples: Number of MC samples for the predictive.\n        calibration_objective: Objective function to optimize during calibration, by\n            default CalibrationObjective.NLL.\n        calibration_method: Method to use for calibration, by default\n            CalibrationMethod.GRID_SEARCH.\n        vmap_over_data: Whether the model expects a leading batch axis, by default True.\n        objective_jit: Whether to jit the calibration objective, by default True.\n        **calibration_kwargs: Additional arguments for the calibration method.\n\n    Returns:\n        A tuple containing:\n\n            - prior_arguments : PriorArguments\n                Dictionary of calibrated hyperparameters.\n            - set_prob_predictive : Callable\n                Function that creates a predictive distribution given prior arguments.\n\n    Raises:\n        ValueError: When an unknown calibration method is provided.\n\n    Notes:\n        Supported calibration objectives:\n\n        - NLL: Negative log-likelihood\n        - CHI_SQUARED: Chi-squared statistic\n        - MARGINAL_LOG_LIKELIHOOD: Marginal log-likelihood\n        - ECE: Expected Calibration Error\n\n        Supported calibration methods:\n\n        - GRID_SEARCH: Grid search over prior precision\n    \"\"\"\n    # If task is classification, then no NLL objective\n    is_classification = predictive_type != Predictive.NONE\n\n    # Pushforward construction\n    set_pushforward, pushforward_fns = _setup_pushforward(\n        pushforward_type=pushforward_type,\n        predictive_type=predictive_type,\n        pushforward_fns=pushforward_fns,\n    )\n\n    set_prob_predictive = partial(\n        set_pushforward,\n        model_fn=model_fn,\n        mean_params=params,\n        posterior_fn=posterior_fn,\n        pushforward_fns=pushforward_fns,\n        key=sample_key,\n        num_samples=num_samples,\n    )\n\n    # Calibration objective &amp; optimisation\n    objective_fn = _build_calibration_objective(\n        objective_type=calibration_objective,\n        set_prob_predictive=set_prob_predictive,\n        curv_estimate=curv_estimate,\n        model_fn=model_fn,\n        params=params,\n        loss_fn=loss_fn,\n        curv_type=curv_type,\n        vmap_over_data=vmap_over_data,\n        is_classification=is_classification,\n    )\n\n    calibration_method = _convert_to_enum(\n        CalibrationMethod, calibration_method, str_default=True\n    )\n\n    if calibration_method == CalibrationMethod.GRID_SEARCH:\n        # Get default values if not provided\n        log_prior_prec_min = calibration_kwargs.get(\"log_prior_prec_min\", -3.0)\n        log_prior_prec_max = calibration_kwargs.get(\"log_prior_prec_max\", 3.0)\n        grid_size = calibration_kwargs.get(\"grid_size\", 50)\n        patience = calibration_kwargs.get(\"patience\")\n\n        # Transform calibration batch to {\"input\": ..., \"target\": ...}\n        data = _validate_and_get_transform(data)(data)\n\n        logger.debug(\n            \"Starting calibration with objective {} on grid [{}, {}] ({} pts, pat={})\",\n            calibration_objective,\n            log_prior_prec_min,\n            log_prior_prec_max,\n            grid_size,\n            patience,\n        )\n\n        def objective(x):\n            return objective_fn(x, data)\n\n        if objective_jit:\n            objective = jax.jit(objective)\n\n        prior_prec = calibration_options[calibration_method](\n            objective=objective,\n            log_prior_prec_min=log_prior_prec_min,\n            log_prior_prec_max=log_prior_prec_max,\n            grid_size=grid_size,\n            patience=patience,\n        )\n        prior_args = {\"prior_prec\": prior_prec}\n\n    elif calibration_method in calibration_options:\n        data = _validate_and_get_transform(data)(data)\n\n        if objective_jit:\n            objective_fn = jax.jit(objective_fn)\n\n        prior_args = calibration_options[calibration_method](\n            objective=objective_fn,\n            data=data,\n            **calibration_kwargs,\n        )\n    else:\n        msg = f\"Unknown calibration method: {calibration_method}\"\n        raise ValueError(msg)\n    logger.debug(\"Calibrated prior args = {}\", prior_args)\n\n    return prior_args, set_prob_predictive\n</code></pre> <p>evaluation </p> <p>Evaluate the calibrated Laplace approximation.</p> <p>This function assesses the performance of the calibrated Laplace approximation by computing various metrics on the test data. It supports both regression and classification tasks with different predictive distributions.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_fn</code> <code>Callable[[PriorArguments, Float], Posterior]</code> <p>Function that generates samples from the posterior.</p> required <code>model_fn</code> <code>ModelFn</code> <p>The neural network forward pass function.</p> required <code>params</code> <code>Params</code> <p>The MAP estimate of the network parameters.</p> required <code>arguments</code> <code>PriorArguments</code> <p>The calibrated prior arguments.</p> required <code>data</code> <code>Data | Iterator[Data]</code> <p>The test data for evaluation.</p> required <code>metrics</code> <code>DefaultMetrics | list[Callable] | Callable | str</code> <p>Metrics to compute during evaluation, by default DefaultMetrics.REGRESSION.</p> <code>REGRESSION</code> <code>predictive_type</code> <code>Predictive | str</code> <p>Type of predictive distribution to use, by default Predictive.NONE.</p> <code>NONE</code> <code>pushforward_type</code> <code>Pushforward | str</code> <p>Type of pushforward approximation to use, by default Pushforward.LINEAR.</p> <code>LINEAR</code> <code>pushforward_fns</code> <code>list[Callable] | None</code> <p>Custom pushforward functions to use, by default None.</p> <code>None</code> <code>reduce</code> <code>Callable</code> <p>Function to reduce metrics across batches, by default identity.</p> <code>identity</code> <code>sample_key</code> <code>KeyType</code> <p>Random key for sampling, by default jax.random.key(0).</p> <code>DEFAULT_KEY</code> <code>num_samples</code> <code>int</code> <p>Number of samples for Monte Carlo predictions, by default 10.</p> <code>10</code> <code>predictive_jit</code> <code>bool</code> <p>Whether to jit the predictive distribution, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], Callable[[InputArray], dict[str, Array]]]</code> <p>A tuple containing:</p> <ul> <li>results : dict     Dictionary of computed metrics.</li> <li>prob_predictive : Callable     The predictive distribution function.</li> </ul> Notes <p>Supported metrics:</p> <ul> <li>REGRESSION: Default metrics for regression tasks</li> <li>CLASSIFICATION: Default metrics for classification tasks</li> <li>Custom metrics can be provided as a list of callables</li> </ul> <p>The function supports both linearized and Monte Carlo predictions through different pushforward types.</p> Source code in <code>laplax/api.py</code> <pre><code>def evaluation(\n    posterior_fn: Callable[[PriorArguments, Float], Posterior],\n    model_fn: ModelFn,\n    params: Params,\n    arguments: PriorArguments,\n    data: Data | Iterator[Data],\n    *,\n    metrics: DefaultMetrics\n    | list[Callable]\n    | Callable\n    | str = DefaultMetrics.REGRESSION,\n    predictive_type: Predictive | str = Predictive.NONE,\n    pushforward_type: Pushforward | str = Pushforward.LINEAR,\n    pushforward_fns: list[Callable] | None = None,\n    reduce: Callable = identity,\n    sample_key: KeyType = DEFAULT_KEY,\n    num_samples: int = 10,\n    predictive_jit: bool = True,\n) -&gt; tuple[dict[str, Array], Callable[[InputArray], dict[str, Array]]]:\n    \"\"\"Evaluate the calibrated Laplace approximation.\n\n    This function assesses the performance of the calibrated Laplace approximation\n    by computing various metrics on the test data. It supports both regression and\n    classification tasks with different predictive distributions.\n\n    Args:\n        posterior_fn: Function that generates samples from the posterior.\n        model_fn: The neural network forward pass function.\n        params: The MAP estimate of the network parameters.\n        arguments: The calibrated prior arguments.\n        data: The test data for evaluation.\n        metrics: Metrics to compute during evaluation, by default\n            DefaultMetrics.REGRESSION.\n        predictive_type: Type of predictive distribution to use, by default\n            Predictive.NONE.\n        pushforward_type: Type of pushforward approximation to use, by default\n            Pushforward.LINEAR.\n        pushforward_fns: Custom pushforward functions to use, by default None.\n        reduce: Function to reduce metrics across batches, by default identity.\n        sample_key: Random key for sampling, by default jax.random.key(0).\n        num_samples: Number of samples for Monte Carlo predictions, by default 10.\n        predictive_jit: Whether to jit the predictive distribution, by default True.\n\n    Returns:\n        A tuple containing:\n\n            - results : dict\n                Dictionary of computed metrics.\n            - prob_predictive : Callable\n                The predictive distribution function.\n\n    Notes:\n        Supported metrics:\n\n        - REGRESSION: Default metrics for regression tasks\n        - CLASSIFICATION: Default metrics for classification tasks\n        - Custom metrics can be provided as a list of callables\n\n        The function supports both linearized and Monte Carlo predictions through\n        different pushforward types.\n    \"\"\"\n    metrics_list = _resolve_metrics(metrics)\n\n    set_pushforward, pushforward_fns = _setup_pushforward(\n        pushforward_type=pushforward_type,\n        predictive_type=predictive_type,\n        pushforward_fns=pushforward_fns,\n    )\n\n    # Build predictive distribution\n    prob_predictive = set_pushforward(\n        prior_arguments=arguments,\n        model_fn=model_fn,\n        mean_params=params,\n        posterior_fn=posterior_fn,\n        pushforward_fns=pushforward_fns,\n        key=sample_key,\n        num_samples=num_samples,\n    )\n\n    if predictive_jit:\n        prob_predictive = jax.jit(prob_predictive)\n\n    # Evaluate\n    is_data_loader = _is_data_loader(data)\n    transform = _validate_and_get_transform(\n        next(iter(data)) if is_data_loader else data\n    )\n\n    if is_data_loader:\n        results = evaluate_metrics_on_generator(\n            pred_fn=prob_predictive,\n            data_generator=cast(\"Iterator[Data]\", data),\n            metrics=metrics_list,\n            transform=transform,\n            reduce=jnp.concatenate,\n            has_batch=True,\n        )\n    else:\n        results = evaluate_metrics_on_dataset(\n            pred_fn=prob_predictive,\n            data=transform(data),\n            metrics=metrics_list,\n            reduce=reduce,\n        )\n\n    return results, prob_predictive\n</code></pre>"},{"location":"reference/register/","title":"laplax.register","text":""},{"location":"reference/register/#laplax.register.register_calibration_method","title":"register_calibration_method","text":"<pre><code>register_calibration_method(method_name: str, method_fn: Callable) -&gt; None\n</code></pre> <p>Register a new calibration method.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>Name of the calibration method.</p> required <code>method_fn</code> <code>Callable</code> <p>Function implementing the calibration method.</p> required Notes <p>The method function should have signature <code>method_fn(objective: Callable, **kwargs) -&gt; float</code></p> Source code in <code>laplax/register.py</code> <pre><code>def register_calibration_method(\n    method_name: str,\n    method_fn: Callable,\n) -&gt; None:\n    \"\"\"Register a new calibration method.\n\n    Args:\n        method_name: Name of the calibration method.\n        method_fn: Function implementing the calibration method.\n\n    Notes:\n        The method function should have signature\n        `method_fn(objective: Callable, **kwargs) -&gt; float`\n    \"\"\"\n    calibration_options[method_name] = method_fn\n    logger.info(f\"Registered new calibration method: {method_name}\")\n</code></pre>"},{"location":"reference/register/#laplax.register.register_curvature_method","title":"register_curvature_method","text":"<pre><code>register_curvature_method(name: str, *, create_curvature_fn: Callable[[CurvatureMV, Layout, Any], Any] | None = None, curvature_to_precision_fn: Callable | None = None, prec_to_posterior_fn: Callable | None = None, posterior_state_to_scale_fn: Callable[[PosteriorState], Callable[[FlatParams], FlatParams]] | None = None, posterior_state_to_cov_fn: Callable[[PosteriorState], Callable[[FlatParams], FlatParams]] | None = None, marginal_log_likelihood_fn: Callable | None = None, default: CurvApprox | None = None) -&gt; None\n</code></pre> <p>Register a new curvature method with optional custom functions.</p> <p>This function allows adding new curvature methods with their corresponding functions for creating curvature estimates, adding prior information, computing posterior states, and deriving matrix-vector product functions for scale and covariance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new curvature method.</p> required <code>create_curvature_fn</code> <code>Callable[[CurvatureMV, Layout, Any], Any] | None</code> <p>Custom function to create the curvature estimate. Defaults to None.</p> <code>None</code> <code>curvature_to_precision_fn</code> <code>Callable | None</code> <p>Custom function to convert the curvature estimate to a posterior precision matrix. Defaults to None.</p> <code>None</code> <code>prec_to_posterior_fn</code> <code>Callable | None</code> <p>Custom function to convert the posterior precision matrix to a posterior state. Defaults to None.</p> <code>None</code> <code>posterior_state_to_scale_fn</code> <code>Callable[[PosteriorState], Callable[[FlatParams], FlatParams]] | None</code> <p>Custom function to compute scale matrix-vector products. Defaults to None.</p> <code>None</code> <code>posterior_state_to_cov_fn</code> <code>Callable[[PosteriorState], Callable[[FlatParams], FlatParams]] | None</code> <p>Custom function to compute covariance matrix-vector products. Defaults to None.</p> <code>None</code> <code>marginal_log_likelihood_fn</code> <code>Callable | None</code> <p>Custom function to compute the marginal log-likelihood. Defaults to None.</p> <code>None</code> <code>default</code> <code>CurvApprox | None</code> <p>Default method to inherit missing functionality from. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no default is provided and required functions are missing.</p> Source code in <code>laplax/register.py</code> <pre><code>def register_curvature_method(\n    name: str,\n    *,\n    create_curvature_fn: Callable[[CurvatureMV, Layout, Any], Any] | None = None,\n    curvature_to_precision_fn: Callable | None = None,\n    prec_to_posterior_fn: Callable | None = None,\n    posterior_state_to_scale_fn: Callable[\n        [PosteriorState], Callable[[FlatParams], FlatParams]\n    ]\n    | None = None,\n    posterior_state_to_cov_fn: Callable[\n        [PosteriorState], Callable[[FlatParams], FlatParams]\n    ]\n    | None = None,\n    marginal_log_likelihood_fn: Callable | None = None,\n    default: CurvApprox | None = None,\n) -&gt; None:\n    \"\"\"Register a new curvature method with optional custom functions.\n\n    This function allows adding new curvature methods with their corresponding\n    functions for creating curvature estimates, adding prior information,\n    computing posterior states, and deriving matrix-vector product functions\n    for scale and covariance.\n\n    Args:\n        name: Name of the new curvature method.\n        create_curvature_fn: Custom function to create the curvature\n            estimate. Defaults to None.\n        curvature_to_precision_fn: Custom function to convert the curvature\n            estimate to a posterior precision matrix. Defaults to None.\n        prec_to_posterior_fn: Custom function to convert the posterior precision\n            matrix to a posterior state. Defaults to None.\n        posterior_state_to_scale_fn: Custom function to compute scale\n            matrix-vector products. Defaults to None.\n        posterior_state_to_cov_fn: Custom function to compute covariance\n            matrix-vector products. Defaults to None.\n        marginal_log_likelihood_fn: Custom function to compute the marginal\n            log-likelihood. Defaults to None.\n        default: Default method to inherit missing\n            functionality from. Defaults to None.\n\n    Raises:\n        ValueError: If no default is provided and required functions are missing.\n    \"\"\"\n    # Check whether default is given\n    if default is None and not all((\n        create_curvature_fn,\n        curvature_to_precision_fn,\n        prec_to_posterior_fn,\n        posterior_state_to_scale_fn,\n        posterior_state_to_cov_fn,\n        marginal_log_likelihood_fn,\n    )):\n        missing_functions = [\n            fn_name\n            for fn_name, fn in zip(\n                [\n                    \"create_curvature_fn\",\n                    \"curvature_to_precision_fn\",\n                    \"prec_to_posterior_fn\",\n                    \"posterior_state_to_scale_fn\",\n                    \"posterior_state_to_cov_fn\",\n                    \"marginal_log_likelihood_fn\",\n                ],\n                [\n                    create_curvature_fn,\n                    curvature_to_precision_fn,\n                    prec_to_posterior_fn,\n                    posterior_state_to_scale_fn,\n                    posterior_state_to_cov_fn,\n                    marginal_log_likelihood_fn,\n                ],\n                strict=True,\n            )\n            if fn is None\n        ]\n        msg = (\n            \"Either a default method must be provided or the following functions must \"\n            f\"be specified: {', '.join(missing_functions)}.\"\n        )\n        raise ValueError(msg)\n\n    CURVATURE_METHODS[name] = create_curvature_fn or CURVATURE_METHODS[default]\n    CURVATURE_PRECISION_METHODS[name] = (\n        curvature_to_precision_fn or CURVATURE_PRECISION_METHODS[default]\n    )\n    CURVATURE_TO_POSTERIOR_STATE[name] = (\n        prec_to_posterior_fn or CURVATURE_TO_POSTERIOR_STATE[default]\n    )\n    CURVATURE_STATE_TO_SCALE[name] = (\n        posterior_state_to_scale_fn or CURVATURE_STATE_TO_SCALE[default]\n    )\n    CURVATURE_STATE_TO_COV[name] = (\n        posterior_state_to_cov_fn or CURVATURE_STATE_TO_COV[default]\n    )\n    CURVATURE_MARGINAL_LOG_LIKELIHOOD[name] = (\n        marginal_log_likelihood_fn or CURVATURE_MARGINAL_LOG_LIKELIHOOD[default]\n    )\n</code></pre>"},{"location":"reference/util/","title":"Utility Module","text":"<p>The <code>util</code> module provides utility functions for the <code>laplax</code> package, that might be useful for working with JAX-based libraries in general. Here we list its submodules with a short description.</p>"},{"location":"reference/util/#flattenpy","title":"<code>flatten.py</code>","text":"<p>This module provides utilities for working with JAX PyTrees, primarily focused on flattening them into 1D or 2D arrays and unflattening arrays back into PyTrees. It includes functions for creating flattener/unflattener pairs and for wrapping other functions to automatically handle these data structure transformations.</p>"},{"location":"reference/util/#mvpy","title":"<code>mv.py</code>","text":"<p>This module provides matrix-free operations, specifically for handling matrix-vector products (MVPs). It offers functions to compute the diagonal, or densify both an explicit matrix or an abstract linear operator defined by a function.</p>"},{"location":"reference/util/#loaderpy","title":"<code>loader.py</code>","text":"<p>This module offers tools for processing data in batches from data loaders. It includes various reduction functions (like sum, concatenation, and online mean) to aggregate results across batches. The central <code>DataLoaderMV</code> class allows matrix-vector product functions to be applied over entire datasets by iterating through a data loader, making it possible to compute dataset-wide quantities like diagonals or dense matrices from batch-level operations.</p>"},{"location":"reference/util/#opspy","title":"<code>ops.py</code>","text":"<p>This module contains general-purpose adaptive operations. Its main feature is <code>precompute_list</code>, a function that can optionally precompute and cache the results of applying a function to a sequence of items instead of recomputing them each time.</p>"},{"location":"reference/util/#treepy","title":"<code>tree.py</code>","text":"<p>This module provides a comprehensive suite of utility functions for manipulating JAX PyTrees. It includes element-wise arithmetic and statistical operations, functions for creating structured PyTrees (e.g., of zeros, ones, or random numbers), utilities for linear algebra operations like matrix-vector products with PyTrees, and tools for creating basis vectors and identity-like structures within a PyTree format.</p>"},{"location":"reference/curv/cov/","title":"laplax.curv.cov","text":"<p>Posterior covariance functions for various curvature estimates.</p>"},{"location":"reference/curv/cov/#laplax.curv.cov.estimate_curvature","title":"estimate_curvature","text":"<pre><code>estimate_curvature(curv_type: CurvApprox | str, mv: CurvatureMV, layout: Layout | None = None, **kwargs: Kwargs) -&gt; PyTree\n</code></pre> <p>Estimate the curvature based on the provided type.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvApprox | str</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>mv</code> <code>CurvatureMV</code> <p>Function representing the curvature-vector product.</p> required <code>layout</code> <code>Layout | None</code> <p>Defines the input layer format of the matrix-vector products. If None or an integer, no flattening/unflattening is used.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional key-word arguments passed to the curvature estimation function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>The estimated curvature.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def estimate_curvature(\n    curv_type: CurvApprox | str,\n    mv: CurvatureMV,\n    layout: Layout | None = None,\n    **kwargs: Kwargs,\n) -&gt; PyTree:\n    \"\"\"Estimate the curvature based on the provided type.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        mv: Function representing the curvature-vector product.\n        layout: Defines the input layer format of the matrix-vector products. If None or\n            an integer, no flattening/unflattening is used.\n        **kwargs: Additional key-word arguments passed to the curvature estimation\n            function.\n\n    Returns:\n        The estimated curvature.\n    \"\"\"\n    curv_estimate = CURVATURE_METHODS[curv_type](mv, layout=layout, **kwargs)\n\n    # Ignore lazy evaluation\n    curv_estimate = jax.tree.map(\n        lambda x: x.block_until_ready() if isinstance(x, jax.Array) else x,\n        curv_estimate,\n    )\n\n    return curv_estimate\n</code></pre>"},{"location":"reference/curv/cov/#laplax.curv.cov.set_posterior_fn","title":"set_posterior_fn","text":"<pre><code>set_posterior_fn(curv_type: CurvatureKeyType, curv_estimate: PyTree, *, layout: Layout, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Set the posterior function based on the curvature estimate.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvatureKeyType</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>curv_estimate</code> <code>PyTree</code> <p>Estimated curvature.</p> required <code>layout</code> <code>Layout</code> <p>Defines the input/output layout of the corresponding curvature-vector products. If <code>None</code> or an integer, no flattening/unflattening is used.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional key-word arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function that computes the posterior state.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When layout is neither an integer, a PyTree, nor None.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def set_posterior_fn(\n    curv_type: CurvatureKeyType,\n    curv_estimate: PyTree,\n    *,\n    layout: Layout,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Set the posterior function based on the curvature estimate.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        curv_estimate: Estimated curvature.\n        layout: Defines the input/output layout of the corresponding curvature-vector\n            products. If `None` or an integer, no flattening/unflattening is used.\n        **kwargs: Additional key-word arguments (unused).\n\n    Returns:\n        A function that computes the posterior state.\n\n    Raises:\n        ValueError: When layout is neither an integer, a PyTree, nor None.\n    \"\"\"\n    del kwargs\n    if layout is not None and not isinstance(layout, int | PyTree):\n        msg = \"Layout must be an integer, PyTree or None.\"\n        raise ValueError(msg)\n\n    # Create functions for flattening and unflattening if required\n    if layout is None or isinstance(layout, int):\n        flatten = unflatten = None\n    else:\n        # Use custom flatten/unflatten functions for complex pytrees\n        flatten, unflatten = create_pytree_flattener(layout)\n\n    def posterior_fn(\n        prior_arguments: PriorArguments,\n        loss_scaling_factor: Float = 1.0,\n    ) -&gt; PosteriorState:\n        \"\"\"Compute the posterior state.\n\n        Args:\n            prior_arguments: Prior arguments for the posterior.\n            loss_scaling_factor: Factor by which the user-provided loss function is\n                scaled. Defaults to 1.0.\n\n        Returns:\n            PosteriorState: Dictionary containing:\n\n                - 'state': Updated state of the posterior.\n                - 'cov_mv': Function to compute covariance matrix-vector product.\n                - 'scale_mv': Function to compute scale matrix-vector product.\n        \"\"\"\n        # Calculate posterior precision.\n        precision = CURVATURE_PRECISION_METHODS[curv_type](\n            curv_estimate=curv_estimate,\n            prior_arguments=prior_arguments,\n            loss_scaling_factor=loss_scaling_factor,\n        )\n\n        # Calculate posterior state\n        state = CURVATURE_TO_POSTERIOR_STATE[curv_type](precision)\n\n        # Extract matrix-vector product\n        scale_mv_from_state = CURVATURE_STATE_TO_SCALE[curv_type]\n        cov_mv_from_state = CURVATURE_STATE_TO_COV[curv_type]\n\n        return Posterior(\n            state=state,\n            cov_mv=wrap_factory(cov_mv_from_state, flatten, unflatten),\n            scale_mv=wrap_factory(scale_mv_from_state, flatten, unflatten),\n        )\n\n    return posterior_fn\n</code></pre>"},{"location":"reference/curv/cov/#laplax.curv.cov.create_posterior_fn","title":"create_posterior_fn","text":"<pre><code>create_posterior_fn(curv_type: CurvApprox | str, mv: CurvatureMV, layout: Layout | None = None, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Factory function to create the posterior function given a curvature type.</p> <p>This sets up the posterior function, which can then be initiated using <code>prior_arguments</code> by computing a specified curvature approximation and encoding the sequential computational order of:</p> <pre><code>1. `CURVATURE_PRIOR_METHODS`\n2. `CURVATURE_TO_POSTERIOR_STATE`\n3. `CURVATURE_STATE_TO_SCALE`\n4. `CURVATURE_STATE_TO_COV`\n</code></pre> <p>All methods are selected from the corresponding dictionary by the <code>curv_type</code> argument. New methods can be registered using the :func:<code>laplax.register.register_curvature_method</code> method. See the :mod:<code>laplax.register</code> module for more details.</p> <p>Parameters:</p> Name Type Description Default <code>curv_type</code> <code>CurvApprox | str</code> <p>Type of curvature approximation (<code>CurvApprox.FULL</code>, <code>CurvApprox.DIAGONAL</code>, <code>CurvApprox.LANCZOS</code>, <code>CurvApprox.LOBPCG</code>) or corresponding string (<code>'full'</code>, <code>'diagonal'</code>, <code>'lanczos'</code>, <code>'lobpcg'</code>).</p> required <code>mv</code> <code>CurvatureMV</code> <p>Function representing the curvature.</p> required <code>layout</code> <code>Layout | None</code> <p>Defines the format of the layout for matrix-vector products. If <code>None</code> or an integer, no flattening/unflattening is used.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments passed to the curvature estimation function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A posterior function that takes the <code>prior_arguments</code> and returns the <code>posterior_state</code>.</p> Source code in <code>laplax/curv/cov.py</code> <pre><code>def create_posterior_fn(\n    curv_type: CurvApprox | str,\n    mv: CurvatureMV,\n    layout: Layout | None = None,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Factory function to create the posterior function given a curvature type.\n\n    This sets up the posterior function, which can then be initiated using\n    `prior_arguments` by computing a specified curvature approximation and encoding the\n    sequential computational order of:\n\n        1. `CURVATURE_PRIOR_METHODS`\n        2. `CURVATURE_TO_POSTERIOR_STATE`\n        3. `CURVATURE_STATE_TO_SCALE`\n        4. `CURVATURE_STATE_TO_COV`\n\n    All methods are selected from the corresponding dictionary by the `curv_type`\n    argument. New methods can be registered using the\n    :func:`laplax.register.register_curvature_method` method.\n    See the :mod:`laplax.register` module for more details.\n\n    Args:\n        curv_type: Type of curvature approximation (`CurvApprox.FULL`,\n            `CurvApprox.DIAGONAL`, `CurvApprox.LANCZOS`, `CurvApprox.LOBPCG`) or\n            corresponding string (`'full'`, `'diagonal'`, `'lanczos'`, `'lobpcg'`).\n        mv: Function representing the curvature.\n        layout: Defines the format of the layout for matrix-vector products. If `None`\n            or an integer, no flattening/unflattening is used.\n        **kwargs: Additional keyword arguments passed to the curvature estimation\n            function.\n\n    Returns:\n        A posterior function that takes the `prior_arguments` and returns the\n            `posterior_state`.\n    \"\"\"\n    # Retrieve the curvature estimator based on the provided type\n    curv_estimate = estimate_curvature(curv_type, mv=mv, layout=layout, **kwargs)\n\n    # Set posterior fn based on curv_estimate\n    posterior_fn = set_posterior_fn(curv_type, curv_estimate, layout=layout)\n\n    return posterior_fn\n</code></pre>"},{"location":"reference/curv/diagonal/","title":"laplax.curv.diagonal","text":"<p>Diagonal curvature approximation.</p>"},{"location":"reference/curv/diagonal/#laplax.curv.diagonal.create_diagonal_curvature","title":"create_diagonal_curvature","text":"<pre><code>create_diagonal_curvature(mv: CurvatureMV, layout: Layout, **kwargs: Kwargs) -&gt; FlatParams\n</code></pre> <p>Generate a diagonal curvature.</p> <p>The diagonal of the curvature matrix-vector product is computed as an approximation to the full matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>CurvatureMV</code> <p>Matrix-vector product function representing the curvature.</p> required <code>layout</code> <code>Layout</code> <p>Structure defining the parameter layout that is assumed by the matrix-vector product function.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlatParams</code> <p>A 1D array representing the diagonal curvature.</p> Source code in <code>laplax/curv/diagonal.py</code> <pre><code>def create_diagonal_curvature(\n    mv: CurvatureMV,\n    layout: Layout,\n    **kwargs: Kwargs,\n) -&gt; FlatParams:\n    \"\"\"Generate a diagonal curvature.\n\n    The diagonal of the curvature matrix-vector product is computed as an approximation\n    to the full matrix.\n\n    Args:\n        mv: Matrix-vector product function representing the curvature.\n        layout: Structure defining the parameter layout that is assumed by the\n            matrix-vector product function.\n        **kwargs: Additional arguments (unused).\n\n    Returns:\n        A 1D array representing the diagonal curvature.\n    \"\"\"\n    del kwargs\n    curv_diagonal = diagonal(mv, layout=layout)\n    return curv_diagonal\n</code></pre>"},{"location":"reference/curv/diagonal/#laplax.curv.diagonal.diagonal_curvature_to_precision","title":"diagonal_curvature_to_precision","text":"<pre><code>diagonal_curvature_to_precision(curv_estimate: FlatParams, prior_arguments: PriorArguments, loss_scaling_factor: Float = 1.0) -&gt; FlatParams\n</code></pre> <p>Add prior precision to the diagonal curvature estimate.</p> <p>The prior precision (of an isotropic Gaussian prior) is read of the prior_arguments dictionary and added to the diagonal curvature estimate. The curvature (here: diagonal) is scaled by the \\(\\sigma^2\\) parameter.</p> <p>Parameters:</p> Name Type Description Default <code>curv_estimate</code> <code>FlatParams</code> <p>Diagonal curvature estimate.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Dictionary containing prior precision as 'prior_prec'.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>FlatParams</code> <p>Updated diagonal curvature with added prior precision.</p> Source code in <code>laplax/curv/diagonal.py</code> <pre><code>def diagonal_curvature_to_precision(\n    curv_estimate: FlatParams,\n    prior_arguments: PriorArguments,\n    loss_scaling_factor: Float = 1.0,\n) -&gt; FlatParams:\n    r\"\"\"Add prior precision to the diagonal curvature estimate.\n\n    The prior precision (of an isotropic Gaussian prior) is read of the prior_arguments\n    dictionary and added to the diagonal curvature estimate. The curvature (here:\n    diagonal) is scaled by the $\\sigma^2$ parameter.\n\n    Args:\n        curv_estimate: Diagonal curvature estimate.\n        prior_arguments: Dictionary containing prior precision as 'prior_prec'.\n        loss_scaling_factor: Factor by which the user-provided loss function is\n            scaled. Defaults to 1.0.\n\n    Returns:\n        Updated diagonal curvature with added prior precision.\n    \"\"\"\n    prior_prec = prior_arguments[\"prior_prec\"]\n    sigma_squared = prior_arguments.get(\"sigma_squared\", 1.0)\n    return (\n        sigma_squared * curv_estimate\n        + prior_prec * jnp.ones_like(curv_estimate.shape[-1])\n    ) / loss_scaling_factor\n</code></pre>"},{"location":"reference/curv/diagonal/#laplax.curv.diagonal.diagonal_prec_to_posterior_state","title":"diagonal_prec_to_posterior_state","text":"<pre><code>diagonal_prec_to_posterior_state(prec: FlatParams) -&gt; dict[str, FlatParams]\n</code></pre> <p>Convert precision matrix to scale matrix.</p> <p>The provided diagonal precision matrix is converted to the corresponding scale diagonal, which is returned as a <code>PosteriorState</code> dictionary. The scale matrix is the diagonal matrix with the inverse of the diagonal elements.</p> <p>Parameters:</p> Name Type Description Default <code>prec</code> <code>FlatParams</code> <p>Precision matrix to convert.</p> required <p>Returns:</p> Type Description <code>dict[str, FlatParams]</code> <p>Scale matrix L where L @ L.T is the covariance matrix.</p> Source code in <code>laplax/curv/diagonal.py</code> <pre><code>def diagonal_prec_to_posterior_state(\n    prec: FlatParams,\n) -&gt; dict[str, FlatParams]:\n    \"\"\"Convert precision matrix to scale matrix.\n\n    The provided diagonal precision matrix is converted to the corresponding scale\n    diagonal, which is returned as a `PosteriorState` dictionary. The scale matrix is\n    the diagonal matrix with the inverse of the diagonal elements.\n\n    Args:\n        prec: Precision matrix to convert.\n\n    Returns:\n        Scale matrix L where L @ L.T is the covariance matrix.\n    \"\"\"\n    return {\"scale\": jnp.sqrt(jnp.reciprocal(prec))}\n</code></pre>"},{"location":"reference/curv/diagonal/#laplax.curv.diagonal.diagonal_posterior_state_to_scale","title":"diagonal_posterior_state_to_scale","text":"<pre><code>diagonal_posterior_state_to_scale(state: dict[str, FlatParams]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a scale matrix-vector product function.</p> <p>The diagonal scale matrix is read from the state dictionary and is used to create a corresponding matrix-vector product function representing the action of the diagonal scale matrix on a vector.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, FlatParams]</code> <p>Dictionary containing the diagonal scale matrix.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the diagonal scale matrix-vector product.</p> Source code in <code>laplax/curv/diagonal.py</code> <pre><code>def diagonal_posterior_state_to_scale(\n    state: dict[str, FlatParams],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a scale matrix-vector product function.\n\n    The diagonal scale matrix is read from the state dictionary and is used to create\n    a corresponding matrix-vector product function representing the action of the\n    diagonal scale matrix on a vector.\n\n    Args:\n        state: Dictionary containing the diagonal scale matrix.\n\n    Returns:\n        A function that computes the diagonal scale matrix-vector product.\n    \"\"\"\n\n    def diag_mv(vec: FlatParams) -&gt; FlatParams:\n        return state[\"scale\"] * vec\n\n    return diag_mv\n</code></pre>"},{"location":"reference/curv/diagonal/#laplax.curv.diagonal.diagonal_posterior_state_to_cov","title":"diagonal_posterior_state_to_cov","text":"<pre><code>diagonal_posterior_state_to_cov(state: dict[str, FlatParams]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a covariance matrix-vector product function.</p> <p>The diagonal covariance matrix is computed as the product of the diagonal scale matrix with itself.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, FlatParams]</code> <p>Dictionary containing the diagonal scale matrix.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the diagonal covariance matrix-vector product.</p> Source code in <code>laplax/curv/diagonal.py</code> <pre><code>def diagonal_posterior_state_to_cov(\n    state: dict[str, FlatParams],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a covariance matrix-vector product function.\n\n    The diagonal covariance matrix is computed as the product of the diagonal scale\n    matrix with itself.\n\n    Args:\n        state: Dictionary containing the diagonal scale matrix.\n\n    Returns:\n        A function that computes the diagonal covariance matrix-vector product.\n    \"\"\"\n    arr = state[\"scale\"] ** 2\n\n    def diag_mv(vec: FlatParams) -&gt; FlatParams:\n        return arr * vec\n\n    return diag_mv\n</code></pre>"},{"location":"reference/curv/full/","title":"laplax.curv.full","text":"<p>Full curvature approximation.</p>"},{"location":"reference/curv/full/#laplax.curv.full.create_full_curvature","title":"create_full_curvature","text":"<pre><code>create_full_curvature(mv: CurvatureMV, layout: Layout, **kwargs: Kwargs) -&gt; Num[Array, 'P P']\n</code></pre> <p>Generate a full curvature approximation.</p> <p>The curvature is densed and flattened into a 2D array, that corresponds to the flattened parameter layout.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>CurvatureMV</code> <p>Matrix-vector product function representing the curvature.</p> required <code>layout</code> <code>Layout</code> <p>Structure defining the parameter layout that is assumed by the matrix-vector product function. If <code>None</code> or an integer, no flattening/unflattening is used.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Num[Array, 'P P']</code> <p>A dense matrix representing the full curvature approximation.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def create_full_curvature(\n    mv: CurvatureMV,\n    layout: Layout,\n    **kwargs: Kwargs,\n) -&gt; Num[Array, \"P P\"]:\n    \"\"\"Generate a full curvature approximation.\n\n    The curvature is densed and flattened into a 2D array, that corresponds to the\n    flattened parameter layout.\n\n    Args:\n        mv: Matrix-vector product function representing the curvature.\n        layout: Structure defining the parameter layout that is assumed by the\n            matrix-vector product function. If `None` or an integer, no\n            flattening/unflattening is used.\n        **kwargs: Additional arguments (unused).\n\n    Returns:\n        A dense matrix representing the full curvature approximation.\n    \"\"\"\n    del kwargs\n    if isinstance(layout, int):\n        msg = (\n            \"Full curvature assumes parameter dictionary as input, \"\n            f\"got type {type(layout)} instead. Proceeding without wrapper.\"\n        )\n        logger.warning(msg)\n        mv_wrapped = mv\n    else:\n        flatten, unflatten = create_pytree_flattener(layout)\n        mv_wrapped = wrap_function(mv, input_fn=unflatten, output_fn=flatten)\n    curv_estimate = to_dense(mv_wrapped, layout=get_size(layout))\n    return curv_estimate\n</code></pre>"},{"location":"reference/curv/full/#laplax.curv.full.full_curvature_to_precision","title":"full_curvature_to_precision","text":"<pre><code>full_curvature_to_precision(curv_estimate: Num[Array, 'P P'], prior_arguments: PriorArguments, loss_scaling_factor: Float = 1.0) -&gt; Num[Array, 'P P']\n</code></pre> <p>Add prior precision to the curvature estimate.</p> <p>The prior precision (of an isotropic Gaussian prior) is read of the prior_arguments dictionary and added to the curvature estimate. The curvature is scaled by the \\(\\sigma^2\\) parameter.</p> <p>Parameters:</p> Name Type Description Default <code>curv_estimate</code> <code>Num[Array, 'P P']</code> <p>Full curvature estimate matrix.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Dictionary containing prior precision as 'prior_prec'.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Num[Array, 'P P']</code> <p>Updated curvature matrix with added prior precision.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def full_curvature_to_precision(\n    curv_estimate: Num[Array, \"P P\"],\n    prior_arguments: PriorArguments,\n    loss_scaling_factor: Float = 1.0,\n) -&gt; Num[Array, \"P P\"]:\n    r\"\"\"Add prior precision to the curvature estimate.\n\n    The prior precision (of an isotropic Gaussian prior) is read of the prior_arguments\n    dictionary and added to the curvature estimate. The curvature is scaled by the\n    $\\sigma^2$ parameter.\n\n    Args:\n        curv_estimate: Full curvature estimate matrix.\n        prior_arguments: Dictionary containing prior precision as 'prior_prec'.\n        loss_scaling_factor: Factor by which the user-provided loss function is\n            scaled. Defaults to 1.0.\n\n    Returns:\n        Updated curvature matrix with added prior precision.\n    \"\"\"\n    prior_prec = prior_arguments[\"prior_prec\"]\n    sigma_squared = prior_arguments.get(\"sigma_squared\", 1.0)\n\n    return (\n        sigma_squared * curv_estimate + prior_prec * jnp.eye(curv_estimate.shape[-1])\n    ) / loss_scaling_factor\n</code></pre>"},{"location":"reference/curv/full/#laplax.curv.full.full_prec_to_scale","title":"full_prec_to_scale","text":"<pre><code>full_prec_to_scale(prec: Num[Array, 'P P']) -&gt; Num[Array, 'P P']\n</code></pre> <p>Convert precision matrix to scale matrix using Cholesky decomposition.</p> <p>This converts a precision matrix to a scale matrix using a Cholesky decomposition. The scale matrix is the lower triangular matrix L such that L @ L.T is the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>prec</code> <code>Num[Array, 'P P']</code> <p>Precision matrix to convert.</p> required <p>Returns:</p> Type Description <code>Num[Array, 'P P']</code> <p>Scale matrix L where L @ L.T is the covariance matrix.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def full_prec_to_scale(\n    prec: Num[Array, \"P P\"],\n) -&gt; Num[Array, \"P P\"]:\n    \"\"\"Convert precision matrix to scale matrix using Cholesky decomposition.\n\n    This converts a precision matrix to a scale matrix using a Cholesky decomposition.\n    The scale matrix is the lower triangular matrix L such that L @ L.T is the\n    covariance matrix.\n\n    Args:\n        prec: Precision matrix to convert.\n\n    Returns:\n        Scale matrix L where L @ L.T is the covariance matrix.\n    \"\"\"\n    Lf = jnp.linalg.cholesky(jnp.flip(prec, axis=(-2, -1)))\n    L_inv = jnp.transpose(jnp.flip(Lf, axis=(-2, -1)), axes=(-2, -1))\n    Id = jnp.eye(prec.shape[-1], dtype=prec.dtype)\n    L = jax.scipy.linalg.solve_triangular(L_inv, Id, trans=\"T\")\n    return L\n</code></pre>"},{"location":"reference/curv/full/#laplax.curv.full.full_prec_to_posterior_state","title":"full_prec_to_posterior_state","text":"<pre><code>full_prec_to_posterior_state(prec: Num[Array, 'P P']) -&gt; dict[str, Num[Array, 'P P']]\n</code></pre> <p>Convert precision matrix to scale matrix.</p> <p>The provided precision matrix is converted to a scale matrix, which is the lower triangular matrix L such that L @ L.T is the covariance matrix using :func: <code>full_prec_to_scale</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prec</code> <code>Num[Array, 'P P']</code> <p>Precision matrix to convert.</p> required <p>Returns:</p> Type Description <code>dict[str, Num[Array, 'P P']]</code> <p>Scale matrix L where L @ L.T is the covariance matrix.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def full_prec_to_posterior_state(\n    prec: Num[Array, \"P P\"],\n) -&gt; dict[str, Num[Array, \"P P\"]]:\n    \"\"\"Convert precision matrix to scale matrix.\n\n    The provided precision matrix is converted to a scale matrix, which is the lower\n    triangular matrix L such that L @ L.T is the covariance matrix using\n    :func: `full_prec_to_scale`.\n\n    Args:\n        prec: Precision matrix to convert.\n\n    Returns:\n        Scale matrix L where L @ L.T is the covariance matrix.\n    \"\"\"\n    scale = full_prec_to_scale(prec)\n\n    return {\"scale\": scale}\n</code></pre>"},{"location":"reference/curv/full/#laplax.curv.full.full_posterior_state_to_scale","title":"full_posterior_state_to_scale","text":"<pre><code>full_posterior_state_to_scale(state: dict[str, Num[Array, 'P P']]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a scale matrix-vector product function.</p> <p>The scale matrix is read from the state dictionary and is used to create a corresponding matrix-vector product function representing the action of the scale matrix on a vector.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, Num[Array, 'P P']]</code> <p>Dictionary containing the scale matrix.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the scale matrix-vector product.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def full_posterior_state_to_scale(\n    state: dict[str, Num[Array, \"P P\"]],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a scale matrix-vector product function.\n\n    The scale matrix is read from the state dictionary and is used to create a\n    corresponding matrix-vector product function representing the action of the scale\n    matrix on a vector.\n\n    Args:\n        state: Dictionary containing the scale matrix.\n\n    Returns:\n        A function that computes the scale matrix-vector product.\n    \"\"\"\n\n    def scale_mv(vec: FlatParams) -&gt; FlatParams:\n        return state[\"scale\"] @ vec\n\n    return scale_mv\n</code></pre>"},{"location":"reference/curv/full/#laplax.curv.full.full_posterior_state_to_cov","title":"full_posterior_state_to_cov","text":"<pre><code>full_posterior_state_to_cov(state: dict[str, Num[Array, 'P P']]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a covariance matrix-vector product function.</p> <p>The scale matrix is read from the state dictionary and is used to create a corresponding matrix-vector product function representing the action of the cov matrix on a vector. The covariance matrix is computed as the product of the scale matrix and its transpose.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, Num[Array, 'P P']]</code> <p>Dictionary containing the scale matrix.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the covariance matrix-vector product.</p> Source code in <code>laplax/curv/full.py</code> <pre><code>def full_posterior_state_to_cov(\n    state: dict[str, Num[Array, \"P P\"]],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a covariance matrix-vector product function.\n\n    The scale matrix is read from the state dictionary and is used to create a\n    corresponding matrix-vector product function representing the action of the cov\n    matrix on a vector. The covariance matrix is computed as the product of the scale\n    matrix and its transpose.\n\n    Args:\n        state: Dictionary containing the scale matrix.\n\n    Returns:\n        A function that computes the covariance matrix-vector product.\n    \"\"\"\n    cov = state[\"scale\"] @ state[\"scale\"].T\n\n    def cov_mv(vec: FlatParams) -&gt; FlatParams:\n        return cov @ vec\n\n    return cov_mv\n</code></pre>"},{"location":"reference/curv/ggn/","title":"laplax.curv.ggn","text":"<p>Generalized Gauss-Newton matrix-vector product and loss hessian.</p>"},{"location":"reference/curv/ggn/#laplax.curv.ggn.create_loss_hessian_mv","title":"create_loss_hessian_mv","text":"<pre><code>create_loss_hessian_mv(loss_fn: LossFn | str | Callable[[PredArray, TargetArray], Num[Array, ...]] | None, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Create a function to compute the Hessian-vector product for a specified loss fn.</p> <p>For predefined loss functions like cross-entropy and mean squared error, the function computes their corresponding Hessian-vector products using efficient formulations. For custom loss functions, the Hessian-vector product is computed via automatic differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>LossFn | str | Callable[[PredArray, TargetArray], Num[Array, ...]] | None</code> <p>Loss function to compute the Hessian-vector product for. Supported options are:</p> <ul> <li><code>LossFn.BINARY_CROSS_ENTROPY</code> for binary cross-entropy loss.</li> <li><code>LossFn.CROSS_ENTROPY</code> for cross-entropy loss.</li> <li><code>LossFn.MSE</code> for mean squared error loss.</li> <li><code>LossFn.NONE</code> for no loss.</li> <li>A custom callable loss function that takes predictions and targets.</li> </ul> required <code>**kwargs</code> <code>Kwargs</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function that computes the Hessian-vector product for the given loss function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When <code>loss_fn</code> is <code>None</code>.</p> <code>ValueError</code> <p>When an unsupported loss function (not of type: <code>Callable</code>)is provided.</p> Source code in <code>laplax/curv/ggn.py</code> <pre><code>def create_loss_hessian_mv(\n    loss_fn: LossFn\n    | str\n    | Callable[[PredArray, TargetArray], Num[Array, \"...\"]]\n    | None,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    r\"\"\"Create a function to compute the Hessian-vector product for a specified loss fn.\n\n    For predefined loss functions like cross-entropy and mean squared error, the\n    function computes their corresponding Hessian-vector products using efficient\n    formulations. For custom loss functions, the Hessian-vector product is computed via\n    automatic differentiation.\n\n    Args:\n        loss_fn: Loss function to compute the Hessian-vector product for. Supported\n            options are:\n\n            - `LossFn.BINARY_CROSS_ENTROPY` for binary cross-entropy loss.\n            - `LossFn.CROSS_ENTROPY` for cross-entropy loss.\n            - `LossFn.MSE` for mean squared error loss.\n            - `LossFn.NONE` for no loss.\n            - A custom callable loss function that takes predictions and targets.\n\n        **kwargs: Unused keyword arguments.\n\n    Returns:\n        A function that computes the Hessian-vector product for the given loss function.\n\n    Raises:\n        ValueError: When `loss_fn` is `None`.\n        ValueError: When an unsupported loss function (not of type: `Callable`)is\n            provided.\n    \"\"\"\n    del kwargs\n\n    if loss_fn is None:\n        msg = \"loss_fn cannot be None\"\n        raise ValueError(msg)\n\n    if loss_fn == LossFn.BINARY_CROSS_ENTROPY:\n        return _binary_cross_entropy_hessian_mv\n\n    if loss_fn == LossFn.CROSS_ENTROPY:\n        return _cross_entropy_hessian_mv\n\n    if loss_fn == LossFn.MSE:\n        return _mse_hessian_mv\n\n    if loss_fn == LossFn.NONE:\n\n        def _identity(\n            jv: PredArray,\n            pred: PredArray,\n            target: TargetArray,\n            **kwargs,\n        ) -&gt; Num[Array, \"...\"]:\n            del pred, target, kwargs\n            return jv\n\n        return _identity\n\n    if isinstance(loss_fn, Callable):\n\n        def custom_hessian_mv(\n            jv: PredArray,\n            pred: PredArray,\n            target: TargetArray,\n            **kwargs,\n        ) -&gt; Num[Array, \"...\"]:\n            del kwargs\n\n            def loss_fn_local(p):\n                return loss_fn(p, target)\n\n            return hvp(loss_fn_local, pred, jv)\n\n        return custom_hessian_mv\n\n    msg = \"unsupported loss function provided\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"reference/curv/ggn/#laplax.curv.ggn.create_ggn_mv_without_data","title":"create_ggn_mv_without_data","text":"<pre><code>create_ggn_mv_without_data(model_fn: ModelFn, params: Params, loss_fn: LossFn | str | Callable | None, factor: Float, *, vmap_over_data: bool = True, loss_hessian_mv: Callable | None = None) -&gt; Callable[[Params, Data], Params]\n</code></pre> <p>Create Generalized Gauss-Newton (GGN) matrix-vector productwithout fixed data.</p> <p>The GGN matrix is computed using the Jacobian of the model and the Hessian of the loss function. The resulting product is given by:</p> \\[ \\text{factor} \\cdot \\sum_i J_i^\\top \\nabla^2_{f(x_i, \\theta), f(x_i, \\theta)} \\mathcal{L}(f(x_i, \\theta), y_i) J_i \\cdot v \\] <p>where \\(J_i\\) is the Jacobian of the model at data point \\(i\\), \\(H_{L, i}\\) is the Hessian of the loss, and \\(v\\) is the vector. The <code>factor</code> is a scaling factor that is used to scale the GGN matrix.</p> <p>This function computes the above expression efficiently without hardcoding the dataset, making it suitable for distributed or batched computations.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model's forward pass function.</p> required <code>params</code> <code>Params</code> <p>Model parameters.</p> required <code>loss_fn</code> <code>LossFn | str | Callable | None</code> <p>Loss function to use for the GGN computation.</p> required <code>factor</code> <code>Float</code> <p>Scaling factor for the GGN computation.</p> required <code>vmap_over_data</code> <code>bool</code> <p>Whether to vmap over the data. Defaults to True.</p> <code>True</code> <code>loss_hessian_mv</code> <code>Callable | None</code> <p>The loss Hessian matrix-vector product.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Params, Data], Params]</code> <p>A function that takes a vector and a batch of data, and computes the GGN</p> <code>Callable[[Params, Data], Params]</code> <p>matrix-vector product.</p> Note <p>The function assumes as a default that the data has a batch dimension.</p> Source code in <code>laplax/curv/ggn.py</code> <pre><code>def create_ggn_mv_without_data(\n    model_fn: ModelFn,\n    params: Params,\n    loss_fn: LossFn | str | Callable | None,\n    factor: Float,\n    *,\n    vmap_over_data: bool = True,\n    loss_hessian_mv: Callable | None = None,\n) -&gt; Callable[[Params, Data], Params]:\n    r\"\"\"Create Generalized Gauss-Newton (GGN) matrix-vector productwithout fixed data.\n\n    The GGN matrix is computed using the Jacobian of the model and the Hessian of the\n    loss function. The resulting product is given by:\n\n    $$\n    \\text{factor} \\cdot \\sum_i J_i^\\top \\nabla^2_{f(x_i, \\theta), f(x_i, \\theta)}\n    \\mathcal{L}(f(x_i, \\theta), y_i) J_i \\cdot v\n    $$\n\n    where $J_i$ is the Jacobian of the model at data point $i$, $H_{L, i}$ is the\n    Hessian of the loss, and $v$ is the vector. The `factor` is a scaling factor that\n    is used to scale the GGN matrix.\n\n    This function computes the above expression efficiently without hardcoding the\n    dataset, making it suitable for distributed or batched computations.\n\n    Args:\n        model_fn: The model's forward pass function.\n        params: Model parameters.\n        loss_fn: Loss function to use for the GGN computation.\n        factor: Scaling factor for the GGN computation.\n        vmap_over_data: Whether to vmap over the data. Defaults to True.\n        loss_hessian_mv: The loss Hessian matrix-vector product.\n\n    Returns:\n        A function that takes a vector and a batch of data, and computes the GGN\n        matrix-vector product.\n\n    Note:\n        The function assumes as a default that the data has a batch dimension.\n\n    \"\"\"\n    # Create loss Hessian-vector product\n    loss_hessian_mv = loss_hessian_mv or create_loss_hessian_mv(loss_fn)\n\n    if vmap_over_data:\n        loss_hessian_mv = jax.vmap(loss_hessian_mv)\n\n    def ggn_mv(vec, data):\n        # Step 1: Single jvp for entire batch, if vmap_over_data is True\n        def fwd(p):\n            if vmap_over_data:\n                return jax.vmap(lambda x: model_fn(input=x, params=p))(data[\"input\"])\n            return model_fn(input=data[\"input\"], params=p)\n\n        # Step 2: Linearize the forward pass\n        z, jvp = jax.linearize(fwd, params)\n\n        # Step 3: Compute J^T H J v\n        HJv = loss_hessian_mv(jvp(vec), pred=z, target=data[\"target\"])\n\n        # Step 4: Compute the GGN vector\n        arr = jax.linear_transpose(jvp, vec)(HJv)[0]\n\n        return mul(factor, arr)\n\n    return ggn_mv\n</code></pre>"},{"location":"reference/curv/ggn/#laplax.curv.ggn.create_ggn_mv","title":"create_ggn_mv","text":"<pre><code>create_ggn_mv(model_fn: ModelFn, params: Params, data: Data, loss_fn: LossFn | str | Callable | None = None, *, num_curv_samples: Int | None = None, num_total_samples: Int | None = None, vmap_over_data: bool = True, loss_hessian_mv: Callable | None = None) -&gt; Callable[[Params], Params]\n</code></pre> <p>Computes the Generalized Gauss-Newton (GGN) matrix-vector product with data.</p> <p>The GGN matrix is computed using the Jacobian of the model and the Hessian of the loss function. For a given dataset, the GGN matrix-vector product is computed as:</p> \\[ G(\\theta) = \\text{factor} \\sum_{i=1}^N J_i^\\top \\nabla^2_{f(x_i, \\theta), f(x_i, \\theta)} \\mathcal{L}_i(f(x_i, \\theta), y_i) J_i \\cdot v \\] <p>where \\(J_i\\) is the Jacobian of the model for the \\(i\\)-th data point, \\(\\nabla^2_{ f(x, \\theta), f(x, \\theta)}\\mathcal{L}_i(f(x_i, \\theta), y_i)\\) is the Hessian of the loss for the \\(i\\)-th data point, and \\(N\\) is the number of data points. The <code>factor</code> is a scaling factor that is used to scale the GGN matrix.</p> <p>This function hardcodes the dataset, making it ideal for scenarios where the dataset remains fixed.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model's forward pass function.</p> required <code>params</code> <code>Params</code> <p>Model parameters.</p> required <code>data</code> <code>Data</code> <p>A batch of input and target data.</p> required <code>loss_fn</code> <code>LossFn | str | Callable | None</code> <p>Loss function to use for the GGN computation.</p> <code>None</code> <code>num_curv_samples</code> <code>Int | None</code> <p>Number of samples used to calculate the GGN. Defaults to None, in which case it is inferred from <code>data</code> as its batch size. Note that for losses that contain sums even for a single input (e.g., pixel-wise semantic segmentation losses, this number is not the batch size.</p> <code>None</code> <code>num_total_samples</code> <code>Int | None</code> <p>Number of total samples the model was trained on. See the remark in <code>num_ggn_samples</code>'s description. Defaults to None, in which case it is set to equal <code>num_ggn_samples</code>.</p> <code>None</code> <code>vmap_over_data</code> <code>bool</code> <p>Whether to vmap over the data. Defaults to True.</p> <code>True</code> <code>loss_hessian_mv</code> <code>Callable | None</code> <p>The loss Hessian matrix-vector product. If not provided, it is computed using the <code>loss_fn</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Params], Params]</code> <p>A function that takes a vector and computes the GGN matrix-vector product for the given data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>loss_fn</code> and <code>loss_hessian_mv</code> are provided.</p> <code>ValueError</code> <p>If neither <code>loss_fn</code> nor <code>loss_hessian_mv</code> are provided.</p> Note <p>The function assumes as a default that the data has a batch dimension.</p> Source code in <code>laplax/curv/ggn.py</code> <pre><code>def create_ggn_mv(\n    model_fn: ModelFn,\n    params: Params,\n    data: Data,\n    loss_fn: LossFn | str | Callable | None = None,\n    *,\n    num_curv_samples: Int | None = None,\n    num_total_samples: Int | None = None,\n    vmap_over_data: bool = True,\n    loss_hessian_mv: Callable | None = None,\n) -&gt; Callable[[Params], Params]:\n    r\"\"\"Computes the Generalized Gauss-Newton (GGN) matrix-vector product with data.\n\n    The GGN matrix is computed using the Jacobian of the model and the Hessian of the\n    loss function. For a given dataset, the GGN matrix-vector product is computed as:\n\n    $$\n    G(\\theta) = \\text{factor} \\sum_{i=1}^N J_i^\\top \\nabla^2_{f(x_i, \\theta), f(x_i,\n    \\theta)} \\mathcal{L}_i(f(x_i, \\theta), y_i) J_i \\cdot v\n    $$\n\n    where $J_i$ is the Jacobian of the model for the $i$-th data point, $\\nabla^2_{\n    f(x, \\theta), f(x, \\theta)}\\mathcal{L}_i(f(x_i, \\theta), y_i)$ is the Hessian of\n    the loss for the $i$-th data point, and $N$ is the number of data points. The\n    `factor` is a scaling factor that is used to scale the GGN matrix.\n\n    This function hardcodes the dataset, making it ideal for scenarios where the dataset\n    remains fixed.\n\n    Args:\n        model_fn: The model's forward pass function.\n        params: Model parameters.\n        data: A batch of input and target data.\n        loss_fn: Loss function to use for the GGN computation.\n        num_curv_samples: Number of samples used to calculate the GGN. Defaults to None,\n            in which case it is inferred from `data` as its batch size. Note that for\n            losses that contain sums even for a single input (e.g., pixel-wise semantic\n            segmentation losses, this number is _not_ the batch size.\n        num_total_samples: Number of total samples the model was trained on. See the\n            remark in `num_ggn_samples`'s description. Defaults to None, in which case\n            it is set to equal `num_ggn_samples`.\n        vmap_over_data: Whether to vmap over the data. Defaults to True.\n        loss_hessian_mv: The loss Hessian matrix-vector product. If not provided, it is\n            computed using the `loss_fn`.\n\n    Returns:\n        A function that takes a vector and computes the GGN matrix-vector product for\n            the given data.\n\n    Raises:\n        ValueError: If both `loss_fn` and `loss_hessian_mv` are provided.\n        ValueError: If neither `loss_fn` nor `loss_hessian_mv` are provided.\n\n    Note:\n        The function assumes as a default that the data has a batch dimension.\n    \"\"\"\n    # Enforce either loss_fn or loss_hessian_mv must be provided:\n    if loss_fn is None and loss_hessian_mv is None:\n        msg = \"Either loss_fn or loss_hessian_mv must be provided.\"\n        raise ValueError(msg)\n\n    if loss_fn is not None and loss_hessian_mv is not None:\n        msg = \"Only one of loss_fn or loss_hessian_mv must be provided.\"\n        raise ValueError(msg)\n\n    if num_curv_samples is None:\n        num_curv_samples = data[\"input\"].shape[0]\n\n    if num_total_samples is None:\n        num_total_samples = num_curv_samples\n\n    curv_scaling_factor = num_total_samples / num_curv_samples\n\n    ggn_mv = create_ggn_mv_without_data(\n        model_fn=model_fn,\n        params=params,\n        loss_fn=loss_fn,\n        factor=curv_scaling_factor,\n        vmap_over_data=vmap_over_data,\n        loss_hessian_mv=loss_hessian_mv,\n    )\n\n    def wrapped_ggn_mv(vec: Params) -&gt; Params:\n        return ggn_mv(vec, data)\n\n    return wrapped_ggn_mv\n</code></pre>"},{"location":"reference/curv/hessian/","title":"laplax.curv.hessian","text":"<p>Hessian vector product for curvature estimation.</p>"},{"location":"reference/curv/hessian/#laplax.curv.hessian.hvp","title":"hvp","text":"<pre><code>hvp(func: Callable, primals: PyTree, tangents: PyTree) -&gt; PyTree\n</code></pre> <p>Compute the Hessian-vector product (HVP) for a given function.</p> <p>The Hessian-vector product is computed by differentiating the gradient of the function. This avoids explicitly constructing the Hessian matrix, making the computation efficient.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The scalar function for which the HVP is computed.</p> required <code>primals</code> <code>PyTree</code> <p>The point at which the gradient and Hessian are evaluated.</p> required <code>tangents</code> <code>PyTree</code> <p>The vector to multiply with the Hessian.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>The Hessian-vector product.</p> Source code in <code>laplax/curv/hessian.py</code> <pre><code>def hvp(\n    func: Callable,\n    primals: PyTree,\n    tangents: PyTree,\n) -&gt; PyTree:\n    r\"\"\"Compute the Hessian-vector product (HVP) for a given function.\n\n    The Hessian-vector product is computed by differentiating the gradient of the\n    function. This avoids explicitly constructing the Hessian matrix, making the\n    computation efficient.\n\n    Args:\n        func: The scalar function for which the HVP is computed.\n        primals: The point at which the gradient and Hessian are evaluated.\n        tangents: The vector to multiply with the Hessian.\n\n    Returns:\n        The Hessian-vector product.\n    \"\"\"\n    return jax.jvp(jax.grad(func), (primals,), (tangents,))[1]\n</code></pre>"},{"location":"reference/curv/hessian/#laplax.curv.hessian.create_hessian_mv_without_data","title":"create_hessian_mv_without_data","text":"<pre><code>create_hessian_mv_without_data(model_fn: ModelFn, params: Params, loss_fn: LossFn | str | Callable, factor: Float, *, vmap_over_data: bool = True, **kwargs: Kwargs) -&gt; Callable[[Params, Data], Params]\n</code></pre> <p>Computes the Hessian-vector product (HVP) for a model and loss function.</p> <p>This function computes the HVP by combining the model and loss functions into a single callable. It evaluates the Hessian at the provided model parameters, with respect to the model and loss function.</p> <p>Mathematically:</p> \\[ H \\cdot v = \\nabla^2 \\mathcal{L}(f(x, \\theta), y) \\cdot v, \\] <p>where \\(\\mathcal{L}\\) is the combined loss function, \\(f\\) is the model function, \\(x\\) is the input, \\(y\\) is the target, \\(\\theta\\) are the parameters, and \\(v\\) is the input input vector.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate.</p> required <code>params</code> <code>Params</code> <p>The parameters of the model.</p> required <code>loss_fn</code> <code>LossFn | str | Callable</code> <p>The loss function to apply. Supported options are:</p> <ul> <li><code>LossFn.BINARY_CROSS_ENTROPY</code> for binary cross-entropy loss.</li> <li><code>LossFn.CROSSENTROPY</code> for cross-entropy loss.</li> <li><code>LossFn.MSE</code> for mean squared error.</li> <li><code>LossFn.NONE</code> for no loss.</li> <li>A custom callable loss function.</li> </ul> required <code>factor</code> <code>Float</code> <p>Scaling factor for the Hessian computation.</p> required <code>vmap_over_data</code> <code>bool</code> <p>Whether the model function should be vectorized over the data.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Params, Data], Params]</code> <p>A function that computes the HVP for a given vector and batch of data.</p> Source code in <code>laplax/curv/hessian.py</code> <pre><code>def create_hessian_mv_without_data(\n    model_fn: ModelFn,  # type: ignore[reportRedeclaration]\n    params: Params,\n    loss_fn: LossFn | str | Callable,\n    factor: Float,\n    *,\n    vmap_over_data: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Callable[[Params, Data], Params]:\n    r\"\"\"Computes the Hessian-vector product (HVP) for a model and loss function.\n\n    This function computes the HVP by combining the model and loss functions into a\n    single callable. It evaluates the Hessian at the provided model parameters, with\n    respect to the model and loss function.\n\n    Mathematically:\n\n    $$\n    H \\cdot v = \\nabla^2 \\mathcal{L}(f(x, \\theta), y) \\cdot v,\n    $$\n\n    where $\\mathcal{L}$ is the combined loss function, $f$ is the model function, $x$ is\n    the input, $y$ is the target, $\\theta$ are the parameters, and $v$ is the input\n    input vector.\n\n    Args:\n        model_fn: The model function to evaluate.\n        params: The parameters of the model.\n        loss_fn: The loss function to apply. Supported options are:\n\n            - `LossFn.BINARY_CROSS_ENTROPY` for binary cross-entropy loss.\n            - `LossFn.CROSSENTROPY` for cross-entropy loss.\n            - `LossFn.MSE` for mean squared error.\n            - `LossFn.NONE` for no loss.\n            - A custom callable loss function.\n\n        factor: Scaling factor for the Hessian computation.\n        vmap_over_data: Whether the model function should be vectorized over the data.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        A function that computes the HVP for a given vector and batch of data.\n    \"\"\"\n    del kwargs\n\n    new_model_fn: Callable[[InputArray, TargetArray, Params], Num[Array, \"...\"]] = (  # noqa: UP037\n        concatenate_model_and_loss_fn(model_fn, loss_fn, vmap_over_data=vmap_over_data)\n    )\n\n    def _hessian_mv(vec: Params, data: Data) -&gt; Params:\n        return mul(\n            factor,\n            hvp(\n                lambda p: new_model_fn(data[\"input\"], data[\"target\"], p),\n                params,\n                vec,\n            ),\n        )\n\n    return _hessian_mv\n</code></pre>"},{"location":"reference/curv/hessian/#laplax.curv.hessian.create_hessian_mv","title":"create_hessian_mv","text":"<pre><code>create_hessian_mv(model_fn: ModelFn, params: Params, data: Data, loss_fn: LossFn | str | Callable, *, num_curv_samples: Int | None = None, num_total_samples: Int | None = None, vmap_over_data: bool = True, **kwargs: Kwargs) -&gt; Callable[[Params], Params]\n</code></pre> <p>Computes the Hessian-vector product (HVP) for a model and loss fn. with data.</p> <p>This function wraps :func: <code>create_hessian_mv_without_data</code>, fixing the dataset to produce a function that computes the HVP for the specified data.</p> <p>Mathematically:</p> \\[ H \\cdot v = \\nabla^2 \\mathcal{L}(f(x, \\theta), y) \\cdot v, \\] <p>where \\(\\mathcal{L}\\) is the combined loss function, \\(f\\) is the model function, \\(x\\) is the input, \\(y\\) is the target, \\(\\theta\\) are the parameters, and \\(v\\) is the input vector of the HVP.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate.</p> required <code>params</code> <code>Params</code> <p>The parameters of the model.</p> required <code>data</code> <code>Data</code> <p>A batch of input and target data.</p> required <code>loss_fn</code> <code>LossFn | str | Callable</code> <p>The loss function to apply. Supported options are:</p> <ul> <li><code>LossFn.MSE</code> for mean squared error.</li> <li><code>LossFn.BINARY_CROSS_ENTROPY</code> for binary cross-entropy loss.</li> <li><code>LossFn.CROSSENTROPY</code> for cross-entropy loss.</li> <li><code>LossFn.NONE</code> for no loss.</li> <li>A custom callable loss function.</li> </ul> required <code>num_curv_samples</code> <code>Int | None</code> <p>Number of samples used to calculate the Hessian. Defaults to None, in which case it is inferred from <code>data</code> as its batch size. Note that for losses that contain sums even for a single input (e.g., pixel-wise semantic segmentation losses), this number is not the batch size.</p> <code>None</code> <code>num_total_samples</code> <code>Int | None</code> <p>Number of total samples the model was trained on. See the remark in <code>num_ggn_samples</code>'s description. Defaults to None, in which case it is set to equal <code>num_ggn_samples</code>.</p> <code>None</code> <code>vmap_over_data</code> <code>bool</code> <p>Whether to vmap over the data. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Params], Params]</code> <p>A function that computes the HVP for a given vector and the fixed dataset.</p> Note <p>The function assumes as a default that the data has a batch dimension.</p> Source code in <code>laplax/curv/hessian.py</code> <pre><code>def create_hessian_mv(\n    model_fn: ModelFn,  # type: ignore[reportRedeclaration]\n    params: Params,\n    data: Data,\n    loss_fn: LossFn | str | Callable,\n    *,\n    num_curv_samples: Int | None = None,\n    num_total_samples: Int | None = None,\n    vmap_over_data: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Callable[[Params], Params]:\n    r\"\"\"Computes the Hessian-vector product (HVP) for a model and loss fn. with data.\n\n    This function wraps :func: `create_hessian_mv_without_data`, fixing the dataset to\n    produce a function that computes the HVP for the specified data.\n\n    Mathematically:\n\n    $$\n    H \\cdot v = \\nabla^2 \\mathcal{L}(f(x, \\theta), y) \\cdot v,\n    $$\n\n    where $\\mathcal{L}$ is the combined loss function, $f$ is the model function, $x$ is\n    the input, $y$ is the target, $\\theta$ are the parameters, and $v$ is the input\n    vector of the HVP.\n\n    Args:\n        model_fn: The model function to evaluate.\n        params: The parameters of the model.\n        data: A batch of input and target data.\n        loss_fn: The loss function to apply. Supported options are:\n\n\n            - `LossFn.MSE` for mean squared error.\n            - `LossFn.BINARY_CROSS_ENTROPY` for binary cross-entropy loss.\n            - `LossFn.CROSSENTROPY` for cross-entropy loss.\n            - `LossFn.NONE` for no loss.\n            - A custom callable loss function.\n\n        num_curv_samples: Number of samples used to calculate the Hessian. Defaults to\n            None, in which case it is inferred from `data` as its batch size. Note that\n            for losses that contain sums even for a single input (e.g., pixel-wise\n            semantic segmentation losses), this number is _not_ the batch size.\n        num_total_samples: Number of total samples the model was trained on. See the\n            remark in `num_ggn_samples`'s description. Defaults to None, in which case\n            it is set to equal `num_ggn_samples`.\n        vmap_over_data: Whether to vmap over the data. Defaults to True.\n        **kwargs: Additional arguments.\n\n    Returns:\n        A function that computes the HVP for a given vector and the fixed dataset.\n\n    Note:\n        The function assumes as a default that the data has a batch dimension.\n    \"\"\"\n    if num_curv_samples is None:\n        num_curv_samples = data[\"input\"].shape[0]\n\n    if num_total_samples is None:\n        num_total_samples = num_curv_samples\n\n    curv_scaling_factor = num_total_samples / num_curv_samples\n\n    hessian_mv = create_hessian_mv_without_data(\n        model_fn=model_fn,\n        params=params,\n        loss_fn=loss_fn,\n        factor=curv_scaling_factor,\n        vmap_over_data=vmap_over_data,\n        **kwargs,\n    )\n\n    def wrapped_hessian_mv(vec: Params) -&gt; Params:\n        return hessian_mv(vec, data)\n\n    return wrapped_hessian_mv\n</code></pre>"},{"location":"reference/curv/lanczos/","title":"laplax.curv.lanczos","text":""},{"location":"reference/curv/lanczos/#laplax.curv.lanczos.lanczos_iterations","title":"lanczos_iterations","text":"<pre><code>lanczos_iterations(matvec: Callable[[Array], Array], b: Array, *, maxiter: int = 20, tol: Float = 1e-06, full_reorthogonalize: bool = True, dtype: DType = float64, mv_jit: bool = True) -&gt; tuple[Array, Array, Array]\n</code></pre> <p>Runs Lanczos iterations starting from vector <code>b</code>.</p> <p>Parameters:</p> Name Type Description Default <code>matvec</code> <code>Callable[[Array], Array]</code> <p>A callable that computes <code>A @ x</code>.</p> required <code>b</code> <code>Array</code> <p>Starting vector.</p> required <code>maxiter</code> <code>int</code> <p>Number of iterations.</p> <code>20</code> <code>tol</code> <code>Float</code> <p>Tolerance to detect convergence.</p> <code>1e-06</code> <code>full_reorthogonalize</code> <code>bool</code> <p>If True, reorthogonalize at every step.</p> <code>True</code> <code>dtype</code> <code>DType</code> <p>Data type for the Lanczos scalars/vectors.</p> <code>float64</code> <code>mv_jit</code> <code>bool</code> <p>If True, uses <code>jax.lax.scan</code> for iterations; if False, uses a plain Python for loop. Note that <code>jax.lax.scan</code> can cause problems if, under the hood, the matvec generates a large computational graph (which could be the case if, for example, it's defined as a sum over per-datum curvatures using a dataloader.) In such cases <code>mv_jit</code> should be set to False.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>alpha</code> <code>Array</code> <p>1D array of Lanczos scalars (diagonal of T).</p> <code>beta</code> <code>Array</code> <p>1D array of off-diagonals (with beta[-1] not used).</p> <code>V</code> <code>Array</code> <p>2D array (maxiter+1 x input_dim) of Lanczos vectors.</p> Source code in <code>laplax/curv/lanczos.py</code> <pre><code>def lanczos_iterations(\n    matvec: Callable[[Array], Array],\n    b: Array,\n    *,\n    maxiter: int = 20,\n    tol: Float = 1e-6,\n    full_reorthogonalize: bool = True,\n    dtype: DType = jnp.float64,\n    mv_jit: bool = True,\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"Runs Lanczos iterations starting from vector `b`.\n\n    Args:\n        matvec: A callable that computes `A @ x`.\n        b: Starting vector.\n        maxiter: Number of iterations.\n        tol: Tolerance to detect convergence.\n        full_reorthogonalize: If True, reorthogonalize at every step.\n        dtype: Data type for the Lanczos scalars/vectors.\n        mv_jit: If True, uses `jax.lax.scan` for iterations; if False, uses a plain\n            Python for loop. Note that `jax.lax.scan` can cause problems if, under the\n            hood, the matvec generates a large computational graph (which could be the\n            case if, for example, it's defined as a sum over per-datum curvatures using\n            a dataloader.) In such cases `mv_jit` should be set to False.\n\n    Returns:\n        alpha: 1D array of Lanczos scalars (diagonal of T).\n        beta: 1D array of off-diagonals (with beta[-1] not used).\n        V: 2D array (maxiter+1 x input_dim) of Lanczos vectors.\n    \"\"\"\n    b = jnp.asarray(b, dtype=dtype)\n    b_norm = jnp.linalg.norm(b, 2)\n    v0 = b / b_norm\n\n    alpha = jnp.zeros(maxiter, dtype=dtype)\n    beta = jnp.zeros(maxiter, dtype=dtype)\n    V = jnp.zeros((maxiter + 1, b.shape[0]), dtype=dtype)\n    V = V.at[0].set(v0)\n\n    def reorthogonalize(w: Array, V: Array, i: int) -&gt; Array:\n        def body_fn(j: int, w_acc: Array) -&gt; Array:\n            coeff = jnp.dot(V[j], w_acc)\n            return w_acc - coeff * V[j]\n\n        return jax.lax.fori_loop(0, i, body_fn, w)\n\n    # Define a single iteration function to be used in both cases\n    @jax.jit\n    def iteration_step(v, w, alpha, beta, V, i):\n        a = jnp.dot(v, w)\n        w = w - a * v\n        if full_reorthogonalize:\n            w = reorthogonalize(w, V, i)\n            w = reorthogonalize(w, V, i)\n\n        b_val = jnp.linalg.norm(w, 2)\n        b_val = jnp.where(b_val &lt; tol, 0.0, b_val)\n        v_next = jax.lax.cond(\n            b_val &gt; 0,  # type: ignore[operator]\n            lambda _: w / b_val,\n            lambda _: v,  # In degenerate cases, no progress is made.\n            operand=None,\n        )\n        alpha = alpha.at[i].set(a)\n        beta = beta.at[i].set(b_val)\n        V = V.at[i + 1].set(v_next)\n        return v_next, alpha, beta, V\n\n    def _body_fn(carry, i):\n        v, alpha, beta, V = carry\n        w = matvec(v)\n        v_next, alpha, beta, V = iteration_step(v, w, alpha, beta, V, i)\n        return (v_next, alpha, beta, V), None\n\n    if mv_jit:\n        # Use lax.scan implementation (compilable)\n        init_carry = (v0, alpha, beta, V)\n        indices = jnp.arange(maxiter)\n        (_, alpha, beta, V), _ = jax.lax.scan(_body_fn, init_carry, indices)\n    else:\n        # Use Python loop implementation (not compilable)\n        v = v0\n        for i in range(maxiter):\n            w = matvec(v)\n            v, alpha, beta, V = iteration_step(v, w, alpha, beta, V, i)\n\n    return alpha, beta, V\n</code></pre>"},{"location":"reference/curv/lanczos/#laplax.curv.lanczos.construct_tridiagonal","title":"construct_tridiagonal","text":"<pre><code>construct_tridiagonal(alpha: Array, beta: Array) -&gt; Array\n</code></pre> <p>Constructs the symmetric tridiagonal matrix from Lanczos scalars.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Array</code> <p>Diagonal elements.</p> required <code>beta</code> <code>Array</code> <p>Off-diagonal elements (only beta[:k-1] are used).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A \\(k \\times k\\) symmetric tridiagonal matrix \\(T\\).</p> Source code in <code>laplax/curv/lanczos.py</code> <pre><code>def construct_tridiagonal(\n    alpha: Array,\n    beta: Array,\n) -&gt; Array:\n    r\"\"\"Constructs the symmetric tridiagonal matrix from Lanczos scalars.\n\n    Args:\n        alpha: Diagonal elements.\n        beta: Off-diagonal elements (only beta[:k-1] are used).\n\n    Returns:\n        A $k \\times k$ symmetric tridiagonal matrix $T$.\n    \"\"\"\n    k = alpha.shape[0]\n    T = jnp.zeros((k, k), dtype=alpha.dtype)\n    T = T.at[jnp.arange(k), jnp.arange(k)].set(alpha)\n    # Only the first k-1 values of beta are used.\n    T = T.at[jnp.arange(k - 1), jnp.arange(1, k)].set(beta[: k - 1])\n    T = T.at[jnp.arange(1, k), jnp.arange(k - 1)].set(beta[: k - 1])\n    return T\n</code></pre>"},{"location":"reference/curv/lanczos/#laplax.curv.lanczos.compute_eigendecomposition","title":"compute_eigendecomposition","text":"<pre><code>compute_eigendecomposition(alpha: Array, beta: Array, V: Array, *, compute_vectors: bool = False) -&gt; Array | tuple[Array, Array]\n</code></pre> <p>Computes the eigendecomposition of the tridiagonal matrix generated by Lanczos.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Array</code> <p>Diagonal elements.</p> required <code>beta</code> <code>Array</code> <p>Off-diagonal elements.</p> required <code>V</code> <code>Array</code> <p>Lanczos vectors.</p> required <code>compute_vectors</code> <code>bool</code> <p>If True, compute Ritz vectors in the original space.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array | tuple[Array, Array]</code> <p>If compute_vectors is True: (eigvals, ritz_vectors), else: eigvals.</p> Source code in <code>laplax/curv/lanczos.py</code> <pre><code>def compute_eigendecomposition(\n    alpha: Array, beta: Array, V: Array, *, compute_vectors: bool = False\n) -&gt; Array | tuple[Array, Array]:\n    \"\"\"Computes the eigendecomposition of the tridiagonal matrix generated by Lanczos.\n\n    Args:\n        alpha: Diagonal elements.\n        beta: Off-diagonal elements.\n        V: Lanczos vectors.\n        compute_vectors: If True, compute Ritz vectors in the original space.\n\n    Returns:\n        If compute_vectors is True: (eigvals, ritz_vectors),\n            else: eigvals.\n    \"\"\"\n    T = construct_tridiagonal(alpha, beta)\n    if compute_vectors:\n        eigvals, eigvecs = jnp.linalg.eigh(T)\n        # Use the first maxiter Lanczos vectors (exclude the extra one).\n        V_matrix = V[:-1].T  # shape: (input_dim, maxiter)\n        ritz_vectors = jnp.dot(V_matrix, eigvecs)\n        return eigvals, ritz_vectors\n    eigvals = jnp.linalg.eigvalsh(T)\n    return eigvals\n</code></pre>"},{"location":"reference/curv/lanczos/#laplax.curv.lanczos.lanczos_lowrank","title":"lanczos_lowrank","text":"<pre><code>lanczos_lowrank(A: Callable[[Array], Array] | Array, *, key: KeyType | None = None, b: Array | None = None, layout: Layout | None = None, rank: int = 20, tol: float = 1e-06, mv_dtype: DType | None = None, calc_dtype: DType = float64, return_dtype: DType | None = None, mv_jit: bool = True, full_reorthogonalize: bool = True, **kwargs: Kwargs) -&gt; LowRankTerms\n</code></pre> <p>Compute a low-rank approximation using the Lanczos algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Callable[[Array], Array] | Array</code> <p>Matrix or callable representing the matrix-vector product <code>A @ x</code>.</p> required <code>key</code> <code>KeyType | None</code> <p>PRNG key for random initialization. Either <code>key</code> or <code>b</code> must be provided.</p> <code>None</code> <code>b</code> <code>Array | None</code> <p>Starting vector. Either <code>key</code> or <code>b</code> must be provided.</p> <code>None</code> <code>layout</code> <code>Layout | None</code> <p>Dimension of input vector (required if <code>A</code> is callable).</p> <code>None</code> <code>rank</code> <code>int</code> <p>Number of leading eigenpairs to compute. Defaults to \\(R=20\\).</p> <code>20</code> <code>tol</code> <code>float</code> <p>Convergence tolerance for the algorithm.</p> <code>1e-06</code> <code>mv_dtype</code> <code>DType | None</code> <p>Data type for matrix-vector products. Defaults to <code>float64</code> if <code>jax_enable_x64</code> is enabled, otherwise <code>float32</code>.</p> <code>None</code> <code>calc_dtype</code> <code>DType</code> <p>Data type for internal calculations.</p> <code>float64</code> <code>return_dtype</code> <code>DType | None</code> <p>Data type for returned results.</p> <code>None</code> <code>mv_jit</code> <code>bool</code> <p>If True, enables JIT compilation of matrix-vector products. Note that this can cause problems if the matrix-vector product generates a large computational graph.</p> <code>True</code> <code>full_reorthogonalize</code> <code>bool</code> <p>Whether to perform full reorthogonalization.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LowRankTerms</code> <code>LowRankTerms</code> <p>A dataclass containing:</p> <ul> <li>U: Eigenvectors as a matrix of shape \\((P, R)\\)</li> <li>S: Eigenvalues as an array of length \\((R,)\\)</li> <li>scalar: Scalar factor, initialized to 0.0</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither key nor b is provided.</p> Source code in <code>laplax/curv/lanczos.py</code> <pre><code>def lanczos_lowrank(\n    A: Callable[[Array], Array] | Array,\n    *,\n    key: KeyType | None = None,\n    b: Array | None = None,\n    layout: Layout | None = None,\n    rank: int = 20,\n    tol: float = 1e-6,\n    mv_dtype: DType | None = None,\n    calc_dtype: DType = jnp.float64,\n    return_dtype: DType | None = None,\n    mv_jit: bool = True,\n    full_reorthogonalize: bool = True,\n    **kwargs: Kwargs,\n) -&gt; LowRankTerms:\n    \"\"\"Compute a low-rank approximation using the Lanczos algorithm.\n\n    Args:\n        A: Matrix or callable representing the matrix-vector product `A @ x`.\n        key: PRNG key for random initialization. Either `key` or `b` must be provided.\n        b: Starting vector. Either `key` or `b` must be provided.\n        layout: Dimension of input vector (required if `A` is callable).\n        rank: Number of leading eigenpairs to compute. Defaults to $R=20$.\n        tol: Convergence tolerance for the algorithm.\n        mv_dtype: Data type for matrix-vector products. Defaults to `float64` if\n            `jax_enable_x64` is enabled, otherwise `float32`.\n        calc_dtype: Data type for internal calculations.\n        return_dtype: Data type for returned results.\n        mv_jit: If True, enables JIT compilation of matrix-vector products. Note\n            that this can cause problems if the matrix-vector product generates a large\n            computational graph.\n        full_reorthogonalize: Whether to perform full reorthogonalization.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        LowRankTerms: A dataclass containing:\n\n            - U: Eigenvectors as a matrix of shape $(P, R)$\n            - S: Eigenvalues as an array of length $(R,)$\n            - scalar: Scalar factor, initialized to 0.0\n\n    Raises:\n        ValueError: If neither key nor b is provided.\n    \"\"\"\n    del kwargs\n\n    # Initialize handling mixed precision.\n    original_float64_enabled = jax.config.read(\"jax_enable_x64\")\n\n    if mv_dtype is None:\n        mv_dtype = jnp.float64 if original_float64_enabled else jnp.float32\n\n    if return_dtype is None:\n        return_dtype = jnp.float64 if original_float64_enabled else jnp.float32\n\n    jax.config.update(\"jax_enable_x64\", calc_dtype == jnp.float64)\n\n    # Obtain a uniform matrix-vector multiplication function.\n    matvec, size = get_matvec(A, layout=layout, jit=mv_jit)\n\n    # Wrap to_dtype around mv if necessary.\n    if mv_dtype != calc_dtype:\n        matvec = wrap_function(\n            matvec,\n            input_fn=lambda x: jnp.asarray(x, dtype=mv_dtype),\n            output_fn=lambda x: jnp.asarray(x, dtype=calc_dtype),\n        )\n\n    # Initialize starting vector.\n    if b is not None:\n        b = jnp.asarray(b, dtype=calc_dtype)\n    elif key is not None:\n        b = jax.random.normal(key, (size,), dtype=calc_dtype)\n    else:\n        msg = \"Either key or b must be provided\"\n        raise ValueError(msg)\n\n    # Run Lanczos iterations.\n    alpha, beta, V = lanczos_iterations(\n        matvec,\n        b,\n        maxiter=rank,\n        tol=tol,\n        full_reorthogonalize=full_reorthogonalize,\n        dtype=calc_dtype,\n        mv_jit=mv_jit,\n    )\n    eigvals, eigvecs = compute_eigendecomposition(alpha, beta, V, compute_vectors=True)\n\n    # Prepare and convert the results\n    low_rank_result = LowRankTerms(\n        U=jnp.asarray(eigvecs, dtype=return_dtype),\n        S=jnp.asarray(eigvals, dtype=return_dtype),\n        scalar=jnp.asarray(0.0, dtype=return_dtype),\n    )\n\n    # Restore the original configuration dtype\n    jax.config.update(\"jax_enable_x64\", original_float64_enabled)\n    return low_rank_result\n</code></pre>"},{"location":"reference/curv/lobpcg/","title":"laplax.curv.lobpcg","text":"<p>Mixed-Precision Optional-Non-Jittable LOBPCG Wrapper for Sparse Linear Algebra.</p> <p>This module provides an implementation of the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for finding eigenvalues and eigenvectors of large Hermitian matrices.</p>"},{"location":"reference/curv/lobpcg/#laplax.curv.lobpcg--this-is-a-wrapper","title":"This is a Wrapper","text":"<p>The original source code can be found at: https://github.com/jax-ml/jax/blob/main/jax/experimental/sparse/linalg.py</p>"},{"location":"reference/curv/lobpcg/#laplax.curv.lobpcg--what-changes-were-made","title":"What changes were made?","text":"<p>The implementation relies on the JAX experimental sparse linear algebra package  but extends its functionality to support:</p> <ul> <li> <p>Mixed Precision Arithmetic</p> <ul> <li>Computations inside the algorithm (such as orthonormalization, matrix-vector         products, and eigenvalue updates) can be performed using higher precision         (e.g., <code>float64</code>) to maintain numerical stability in critical steps.</li> <li>Matrix-vector products involving the operator <code>A</code> can be computed in lower         precision (e.g., <code>float32</code>) to reduce memory usage and computation time.</li> </ul> </li> <li> <p>Non-Jittable Operator Support</p> <ul> <li>The implementation supports <code>A</code> as a non-jittable callable, enabling the use of         external libraries such as <code>scipy.sparse.linalg</code> for matrix-vector products.         This is essential for cases where <code>A</code> cannot be expressed using JAX         primitives (e.g., external libraries or precompiled solvers).</li> </ul> </li> </ul>"},{"location":"reference/curv/lobpcg/#laplax.curv.lobpcg--why-this-wrapper","title":"Why this Wrapper?","text":"<p>The primary motivation for this implementation is to work around limitations in the JAX <code>lax.while_loop</code> and sparse linear algebra primitives, which require <code>A</code> to be jittable. By decoupling <code>A</code> from the main loop, we can support a broader range of operators while still leveraging the performance advantages of JAX-accelerated numerical routines where possible.</p>"},{"location":"reference/curv/lobpcg/#laplax.curv.lobpcg.lobpcg_standard","title":"lobpcg_standard","text":"<pre><code>lobpcg_standard(A: Callable[[Array], Array], X: Array, m: int = 100, tol: Float | None = None, calc_dtype: DType = float64, a_dtype: DType = float32, *, A_jit: bool = True) -&gt; tuple[Array, Array, int]\n</code></pre> <p>Compute top-k eigenvalues using LOBPCG with mixed precision.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Callable[[Array], Array]</code> <p>callable representing the Hermitian matrix operation <code>A @ x</code>.</p> required <code>X</code> <code>Array</code> <p>initial guess \\((P, R)\\) array.</p> required <code>m</code> <code>int</code> <p>max iterations</p> <code>100</code> <code>tol</code> <code>Float | None</code> <p>tolerance for convergence</p> <code>None</code> <code>calc_dtype</code> <code>DType</code> <p>dtype for internal calculations (<code>float32</code> or <code>float64</code>)</p> <code>float64</code> <code>a_dtype</code> <code>DType</code> <p>dtype for A calls (e.g., <code>float64</code> for stable matrix-vector products)</p> <code>float32</code> <code>A_jit</code> <code>bool</code> <p>If True, then pass the computation to     <code>jax.experimental.sparse.linalg.lobpcg_standard</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Array, Array, int]</code> <p>Tuple containing:</p> <ul> <li>Eigenvalues: Array of shape \\((R,)\\)</li> <li>Eigenvectors: Array of shape \\((P, R)\\)</li> <li>Iterations: Number of iterations performed</li> </ul> Source code in <code>laplax/curv/lobpcg.py</code> <pre><code>def lobpcg_standard(\n    A: Callable[[Array], Array],\n    X: Array,\n    m: int = 100,\n    tol: Float | None = None,\n    calc_dtype: DType = jnp.float64,\n    a_dtype: DType = jnp.float32,\n    *,\n    A_jit: bool = True,\n) -&gt; tuple[Array, Array, int]:\n    \"\"\"Compute top-k eigenvalues using LOBPCG with mixed precision.\n\n    Args:\n      A: callable representing the Hermitian matrix operation `A @ x`.\n      X: initial guess $(P, R)$ array.\n      m: max iterations\n      tol: tolerance for convergence\n      calc_dtype: dtype for internal calculations (`float32` or `float64`)\n      a_dtype: dtype for A calls (e.g., `float64` for stable matrix-vector products)\n      A_jit: If True, then pass the computation to\n            `jax.experimental.sparse.linalg.lobpcg_standard`.\n\n    Returns:\n        Tuple containing:\n\n            - Eigenvalues: Array of shape $(R,)$\n            - Eigenvectors: Array of shape $(P, R)$\n            - Iterations: Number of iterations performed\n    \"\"\"\n    if A_jit:\n        return linalg.lobpcg_standard(A, X, m, tol)\n\n    n, k = X.shape\n    _check_inputs(A, X)\n\n    if tol is None:\n        tol = jnp.finfo(calc_dtype).eps\n\n    # Convert initial vectors to computation dtype\n    X = X.astype(calc_dtype)\n\n    X = _orthonormalize(X, calc_dtype=calc_dtype)\n    P = _extend_basis(X, X.shape[1], calc_dtype=calc_dtype)\n\n    # Precompute initial AX outside of jit\n    # Cast to a_dtype before A and back to calc_dtype after\n    AX = A(X.astype(a_dtype)).astype(calc_dtype)\n    theta = jnp.sum(X * AX, axis=0, keepdims=True)\n    R = AX - theta * X\n\n    # JIT-ted iteration step that takes AX, AXPR, AS, etc. in calc_dtype\n    @jax.jit\n    def _iteration_first_step(X, P, R, AS):\n        # Projected eigensolve\n        XPR = jnp.concatenate((X, P, R), axis=1)\n        theta, Q = _rayleigh_ritz_orth(AS, XPR)\n\n        # Eigenvector X extraction\n        B = Q[:, :k]\n        normB = jnp.linalg.norm(B, ord=2, axis=0, keepdims=True)\n        B /= normB\n        X = _mm(XPR, B)\n        normX = jnp.linalg.norm(X, ord=2, axis=0, keepdims=True)\n        X /= normX\n\n        # Difference terms P extraction\n        q, _ = jnp.linalg.qr(Q[:k, k:].T)\n        diff_rayleigh_ortho = _mm(Q[:, k:], q)\n        P = _mm(XPR, diff_rayleigh_ortho)\n        normP = jnp.linalg.norm(P, ord=2, axis=0, keepdims=True)\n        P /= jnp.where(normP == 0, 1.0, normP)\n\n        return X, P, R, theta\n\n    @jax.jit\n    def _iteration_second_step(X, R, theta, AX, n, tol):\n        # Compute new residuals.\n        # AX = A(X)\n        R = AX - theta[jnp.newaxis, :k] * X\n        resid_norms = jnp.linalg.norm(R, ord=2, axis=0)\n\n        # Compute residual norms\n        reltol = jnp.linalg.norm(AX, ord=2, axis=0) + theta[:k]\n        reltol *= n\n        # Allow some margin for a few element-wise operations.\n        reltol *= 10\n        res_converged = resid_norms &lt; tol * reltol\n        converged = jnp.sum(res_converged)\n\n        return X, R, theta[jnp.newaxis, :k], converged\n\n    @jax.jit\n    def _projection_step(X, P, R):\n        R = _project_out(jnp.concatenate((X, P), axis=1), R)\n        return R, jnp.concatenate((X, P, R), axis=1)\n\n    i = 0\n    converged = 0\n    while i &lt; m and converged &lt; k:\n        # Residual basis selection\n        R, XPR = _projection_step(X, P, R)\n\n        # Compute AS = AXPR = A(XPR) outside JIT at a_dtype\n        AS = A(XPR.astype(a_dtype)).astype(calc_dtype)\n\n        # Call the first iteration step\n        X, P, R, theta = _iteration_first_step(X, P, R, AS)\n\n        # Calculate AX\n        AX = A(X.astype(a_dtype)).astype(calc_dtype)\n\n        # Call the second iteration step\n        X, R, theta, converged = _iteration_second_step(X, R, theta, AX, n, tol)\n\n        i += 1\n\n    return theta[0, :], X, i\n</code></pre>"},{"location":"reference/curv/lobpcg/#laplax.curv.lobpcg.lobpcg_lowrank","title":"lobpcg_lowrank","text":"<pre><code>lobpcg_lowrank(A: Callable[[Array], Array] | Array, *, key: KeyType | None = None, layout: Layout | None = None, rank: int = 20, tol: Float = 1e-06, mv_dtype: DType | None = None, calc_dtype: DType = float64, return_dtype: DType | None = None, mv_jit: bool = True, **kwargs: Kwargs) -&gt; LowRankTerms\n</code></pre> <p>Compute a low-rank approximation using the LOBPCG algorithm.</p> <p>This function computes the leading eigenvalues and eigenvectors of a matrix represented by a matrix-vector product function <code>mv</code>, without explicitly forming the matrix. It uses the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) algorithm to achieve efficient low-rank approximation, with support for mixed-precision arithmetic and optional JIT compilation.</p> <p>Mathematically, the low-rank approximation seeks to find the leading \\(R\\) eigenpairs \\((\\lambda_i, u_i)\\) such that: \\(A u_i = \\lambda_i u_i \\quad \\text{for } i = 1, \\ldots, R\\), where \\(A\\) is the matrix represented by the matrix-vector product <code>mv</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Callable[[Array], Array] | Array</code> <p>A callable that computes the matrix-vector product, representing the matrix <code>A @ x</code>.</p> required <code>key</code> <code>KeyType | None</code> <p>PRNG key for random initialization of the search directions.</p> <code>None</code> <code>layout</code> <code>Layout | None</code> <p>Dimension of the input/output space of the matrix.</p> <code>None</code> <code>rank</code> <code>int</code> <p>Number of leading eigenpairs to compute. Defaults to \\(R=20\\).</p> <code>20</code> <code>tol</code> <code>Float</code> <p>Convergence tolerance for the algorithm. If <code>None</code>, the machine epsilon for <code>calc_dtype</code> is used.</p> <code>1e-06</code> <code>mv_dtype</code> <code>DType | None</code> <p>Data type for the matrix-vector product function.</p> <code>None</code> <code>calc_dtype</code> <code>DType</code> <p>Data type for internal calculations during LOBPCG.</p> <code>float64</code> <code>return_dtype</code> <code>DType | None</code> <p>Data type for the final results.</p> <code>None</code> <code>mv_jit</code> <code>bool</code> <p>If <code>True</code>, enables JIT compilation for the matrix-vector product.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>LowRankTerms</code> <p>A dataclass containing:</p> <ul> <li><code>U</code>: Eigenvectors as a matrix of shape \\((P, R)\\).</li> <li><code>S</code>: Eigenvalues as an array of length \\((R,)\\).</li> <li><code>scalar</code>: Scalar factor, initialized to 0.0.</li> </ul> Note <ul> <li>If <code>mv_jit</code> is <code>True</code>, the function will be vectorized over the data.</li> <li>If the size of the matrix is small relative to <code>rank</code>, the number of iterations is reduced to avoid over-computation.</li> <li>Mixed precision can significantly reduce memory usage, especially for large     matrices. If <code>mv_dtype</code> is <code>None</code>, the data type is automatically determined     based on the <code>jax_enable_x64</code> configuration.</li> </ul> Example <pre><code>low_rank_terms = lobpcg_lowrank(\n    A=jnp.eye(1000),\n    key=jax.random.key(42),\n    rank=10,\n    tol=1e-6,\n)\n</code></pre> Source code in <code>laplax/curv/lobpcg.py</code> <pre><code>def lobpcg_lowrank(\n    A: Callable[[Array], Array] | Array,\n    *,\n    key: KeyType | None = None,\n    layout: Layout | None = None,\n    rank: int = 20,\n    tol: Float = 1e-6,\n    mv_dtype: DType | None = None,\n    calc_dtype: DType = jnp.float64,\n    return_dtype: DType | None = None,\n    mv_jit: bool = True,\n    **kwargs: Kwargs,\n) -&gt; LowRankTerms:\n    r\"\"\"Compute a low-rank approximation using the LOBPCG algorithm.\n\n    This function computes the leading eigenvalues and eigenvectors of a matrix\n    represented by a matrix-vector product function `mv`, without explicitly forming\n    the matrix. It uses the Locally Optimal Block Preconditioned Conjugate Gradient\n    (LOBPCG) algorithm to achieve efficient low-rank approximation, with support\n    for mixed-precision arithmetic and optional JIT compilation.\n\n    Mathematically, the low-rank approximation seeks to find the leading $R$\n    eigenpairs $(\\lambda_i, u_i)$ such that:\n    $A u_i = \\lambda_i u_i \\quad \\text{for } i = 1, \\ldots, R$, where $A$ is the matrix\n    represented by the matrix-vector product `mv`.\n\n    Args:\n        A: A callable that computes the matrix-vector product, representing the matrix\n            `A @ x`.\n        key: PRNG key for random initialization of the search directions.\n        layout: Dimension of the input/output space of the matrix.\n        rank: Number of leading eigenpairs to compute. Defaults to $R=20$.\n        tol: Convergence tolerance for the algorithm. If `None`, the machine epsilon\n            for `calc_dtype` is used.\n        mv_dtype: Data type for the matrix-vector product function.\n        calc_dtype: Data type for internal calculations during LOBPCG.\n        return_dtype: Data type for the final results.\n        mv_jit: If `True`, enables JIT compilation for the matrix-vector product.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        A dataclass containing:\n\n            - `U`: Eigenvectors as a matrix of shape $(P, R)$.\n            - `S`: Eigenvalues as an array of length $(R,)$.\n            - `scalar`: Scalar factor, initialized to 0.0.\n\n    Note:\n        - If `mv_jit` is `True`, the function will be vectorized over the data.\n        - If the size of the matrix is small relative to `rank`, the number of\n        iterations is reduced to avoid over-computation.\n        - Mixed precision can significantly reduce memory usage, especially for large\n            matrices. If `mv_dtype` is `None`, the data type is automatically determined\n            based on the `jax_enable_x64` configuration.\n\n    Example:\n        ```python\n\n        low_rank_terms = lobpcg_lowrank(\n            A=jnp.eye(1000),\n            key=jax.random.key(42),\n            rank=10,\n            tol=1e-6,\n        )\n\n        ```\n    \"\"\"\n    del kwargs\n\n    # Initialize handling mixed precision.\n    original_float64_enabled = jax.config.read(\"jax_enable_x64\")\n\n    if mv_dtype is None:\n        mv_dtype = jnp.float64 if original_float64_enabled else jnp.float32\n\n    if return_dtype is None:\n        return_dtype = jnp.float64 if original_float64_enabled else jnp.float32\n\n    jax.config.update(\"jax_enable_x64\", calc_dtype == jnp.float64)\n\n    # Obtain a matrix-vector multiplication function.\n    matvec, size = get_matvec(A, layout=layout, jit=mv_jit)\n\n    # Obtain a matrix-matrix product function.\n    matmat = jax.vmap(matvec, in_axes=-1, out_axes=-1)\n\n    # Adjust rank if it's too large compared to problem size\n    if size &lt; rank * 5:\n        rank = max(1, size // 5 - 1)\n        msg = f\"reduced rank to {rank} due to insufficient size\"\n        warnings.warn(msg, stacklevel=1)\n\n    # Wrap to_dtype around mv if necessary.\n    if mv_dtype != calc_dtype:\n        matmat = wrap_function(\n            matmat,\n            input_fn=lambda x: jnp.asarray(x, dtype=mv_dtype),\n            output_fn=lambda x: jnp.asarray(x, dtype=calc_dtype),\n        )\n\n    # Initialize random search directions\n    if key is None:\n        key = jax.random.key(0)\n    X = jax.random.normal(key, (size, rank), dtype=calc_dtype)\n\n    # Perform LOBPCG for eigenvalues and eigenvectors using the new wrapper\n    eigenvals, eigenvecs, _ = lobpcg_standard(\n        A=matmat,\n        X=X,\n        m=rank,\n        tol=tol,\n        calc_dtype=calc_dtype,\n        a_dtype=mv_dtype,  # type: ignore  # noqa: PGH003\n        A_jit=mv_jit,\n    )\n\n    # Prepare and convert the results\n    low_rank_result = LowRankTerms(\n        U=jnp.asarray(eigenvecs, dtype=return_dtype),\n        S=jnp.asarray(eigenvals, dtype=return_dtype),\n        scalar=jnp.asarray(0.0, dtype=return_dtype),\n    )\n\n    # Restore the original configuration dtype\n    jax.config.update(\"jax_enable_x64\", original_float64_enabled)\n\n    return low_rank_result\n</code></pre>"},{"location":"reference/curv/low_rank/","title":"laplax.curv.low_rank","text":"<p>Low-rank curvature approximation.</p>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.create_low_rank_curvature","title":"create_low_rank_curvature","text":"<pre><code>create_low_rank_curvature(mv: CurvatureMV, layout: Layout, low_rank_method: LowRankMethod = LANCZOS, **kwargs: Kwargs) -&gt; LowRankTerms\n</code></pre> <p>Generate a low-rank curvature approximation.</p> <p>The low-rank curvature is computed as an approximation to the full curvature matrix using the provided matrix-vector product function and either the Lanczos or LOBPCG algorithm. The low-rank approximation is returned as a <code>LowRankTerms</code> object. The low-rank approximation is computed as:</p> \\[ \\text{\\textbf{Curv}} \\approx U S U^{\\top} \\] <p>where \\(U\\) are the eigenvectors and \\(S\\) are the eigenvalues. The <code>LowRankTerms</code> holds the eigenvectors, eigenvalues, and a scalar factor. The latter can be used to express an isotropic Gaussian prior.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>CurvatureMV</code> <p>Matrix-vector product function representing the curvature.</p> required <code>layout</code> <code>Layout</code> <p>Structure defining the parameter layout that is assumed by the matrix-vector product function.</p> required <code>low_rank_method</code> <code>LowRankMethod</code> <p>Method to use for computing the low-rank approximation. Can be either <code>LowRankMethod.LANCZOS</code> or <code>LowRankMethod.LOBPCG</code>. Defaults to <code>LowRankMethod.LANCZOS</code>.</p> <code>LANCZOS</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to the low-rank method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LowRankTerms</code> <p>A LowRankTerms object representing the low-rank curvature approximation.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def create_low_rank_curvature(\n    mv: CurvatureMV,\n    layout: Layout,\n    low_rank_method: LowRankMethod = LowRankMethod.LANCZOS,\n    **kwargs: Kwargs,\n) -&gt; LowRankTerms:\n    r\"\"\"Generate a low-rank curvature approximation.\n\n    The low-rank curvature is computed as an approximation to the full curvature matrix\n    using the provided matrix-vector product function and either the Lanczos or LOBPCG\n    algorithm. The low-rank approximation is returned as a `LowRankTerms` object.\n    The low-rank approximation is computed as:\n\n    $$\n    \\text{\\textbf{Curv}} \\approx U S U^{\\top}\n    $$\n\n    where $U$ are the eigenvectors and $S$ are the eigenvalues. The `LowRankTerms` holds\n    the eigenvectors, eigenvalues, and a scalar factor. The latter can be used to\n    express an isotropic Gaussian prior.\n\n    Args:\n        mv: Matrix-vector product function representing the curvature.\n        layout: Structure defining the parameter layout that is assumed by the\n            matrix-vector product function.\n        low_rank_method: Method to use for computing the low-rank approximation.\n            Can be either `LowRankMethod.LANCZOS` or `LowRankMethod.LOBPCG`.\n            Defaults to `LowRankMethod.LANCZOS`.\n        **kwargs: Additional arguments passed to the low-rank method.\n\n    Returns:\n        A LowRankTerms object representing the low-rank curvature approximation.\n    \"\"\"\n    # Select and apply the low-rank method.\n    low_rank_terms = {\n        LowRankMethod.LANCZOS: lanczos_lowrank,\n        LowRankMethod.LOBPCG: lobpcg_lowrank,\n    }[low_rank_method](mv, layout=layout, **kwargs)\n\n    return low_rank_terms\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.create_low_rank_mv","title":"create_low_rank_mv","text":"<pre><code>create_low_rank_mv(low_rank_terms: LowRankTerms) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a low-rank matrix-vector product function.</p> <p>The low-rank matrix-vector product is computed as the sum of the scalar multiple of the vector by the scalar and the product of the matrix-vector product of the eigenvectors and the eigenvalues times the eigenvector-vector product:</p> \\[ scalar * \\text{vec} + U @ (S * (U.T @ \\text{vec})) \\] <p>Parameters:</p> Name Type Description Default <code>low_rank_terms</code> <code>LowRankTerms</code> <p>Low-rank curvature approximation.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the low-rank matrix-vector product.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def create_low_rank_mv(\n    low_rank_terms: LowRankTerms,\n) -&gt; Callable[[FlatParams], FlatParams]:\n    r\"\"\"Create a low-rank matrix-vector product function.\n\n    The low-rank matrix-vector product is computed as the sum of the scalar multiple of\n    the vector by the scalar and the product of the matrix-vector product of the\n    eigenvectors and the eigenvalues times the eigenvector-vector product:\n\n    $$\n    scalar * \\text{vec} + U @ (S * (U.T @ \\text{vec}))\n    $$\n\n    Args:\n        low_rank_terms: Low-rank curvature approximation.\n\n    Returns:\n        A function that computes the low-rank matrix-vector product.\n    \"\"\"\n    U, S, scalar = jax.tree_util.tree_leaves(low_rank_terms)\n\n    def low_rank_mv(vec: FlatParams) -&gt; FlatParams:\n        return scalar * vec + U @ (S * (U.T @ vec))\n\n    return low_rank_mv\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.low_rank_square","title":"low_rank_square","text":"<pre><code>low_rank_square(state: LowRankTerms) -&gt; LowRankTerms\n</code></pre> <p>Square the low-rank curvature approximation.</p> <p>This returns the <code>LowRankTerms</code> which correspond to the squared low-rank approximation. The squared low-rank approximation is computed as:</p> \\[ (U S U^{\\top} + scalar I) ** 2 = scalar**2 + U ((S + scalar) ** 2 - scalar**2) U^{\\top} \\] <p>Parameters:</p> Name Type Description Default <code>state</code> <code>LowRankTerms</code> <p>Low-rank curvature approximation.</p> required <p>Returns:</p> Type Description <code>LowRankTerms</code> <p>A <code>LowRankTerms</code> object representing the squared low-rank curvature approximation.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def low_rank_square(\n    state: LowRankTerms,\n) -&gt; LowRankTerms:\n    r\"\"\"Square the low-rank curvature approximation.\n\n    This returns the `LowRankTerms` which correspond to the squared low-rank\n    approximation. The squared low-rank approximation is computed as:\n\n    $$\n    (U S U^{\\top} + scalar I) ** 2\n    = scalar**2 + U ((S + scalar) ** 2 - scalar**2) U^{\\top}\n    $$\n\n    Args:\n        state: Low-rank curvature approximation.\n\n    Returns:\n        A `LowRankTerms` object representing the squared low-rank curvature\n            approximation.\n    \"\"\"\n    U, S, scalar = jax.tree_util.tree_leaves(state)\n    scalar_sq = scalar**2\n    return LowRankTerms(\n        U=U,\n        S=(S + scalar) ** 2 - scalar_sq,\n        scalar=scalar_sq,\n    )\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.low_rank_curvature_to_precision","title":"low_rank_curvature_to_precision","text":"<pre><code>low_rank_curvature_to_precision(curv_estimate: LowRankTerms, prior_arguments: PriorArguments, loss_scaling_factor: Float = 1.0) -&gt; LowRankTerms\n</code></pre> <p>Add prior precision to the low-rank curvature estimate.</p> <p>The prior precision (of an isotropic Gaussian prior) is read from the <code>prior_arguments</code> dictionary and added to the scalar component of the LowRankTerms.</p> <p>Parameters:</p> Name Type Description Default <code>curv_estimate</code> <code>LowRankTerms</code> <p>Low-rank curvature approximation.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Dictionary containing prior precision as 'prior_prec'.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>LowRankTerms</code> <code>LowRankTerms</code> <p>Updated low-rank curvature approximation with added prior precision.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def low_rank_curvature_to_precision(\n    curv_estimate: LowRankTerms,\n    prior_arguments: PriorArguments,\n    loss_scaling_factor: Float = 1.0,\n) -&gt; LowRankTerms:\n    r\"\"\"Add prior precision to the low-rank curvature estimate.\n\n    The prior precision (of an isotropic Gaussian prior) is read from the\n    `prior_arguments` dictionary and added to the scalar component of the\n    LowRankTerms.\n\n    Args:\n        curv_estimate: Low-rank curvature approximation.\n        prior_arguments: Dictionary containing prior precision\n            as 'prior_prec'.\n        loss_scaling_factor: Factor by which the user-provided loss function is\n            scaled. Defaults to 1.0.\n\n    Returns:\n        LowRankTerms: Updated low-rank curvature approximation with added prior\n            precision.\n    \"\"\"\n    prior_prec = prior_arguments[\"prior_prec\"]\n    sigma_squared = prior_arguments.get(\"sigma_squared\", 1.0)\n    U, S, _ = jax.tree.leaves(curv_estimate)\n    return LowRankTerms(\n        U=U,\n        S=(sigma_squared * S),\n        scalar=prior_prec / loss_scaling_factor,\n    )\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.low_rank_prec_to_posterior_state","title":"low_rank_prec_to_posterior_state","text":"<pre><code>low_rank_prec_to_posterior_state(curv_estimate: LowRankTerms) -&gt; dict[str, LowRankTerms]\n</code></pre> <p>Convert the low-rank precision representation to a posterior state.</p> <p>The scalar component and eigenvalues of the low-rank curvature estimate are transformed to represent the posterior scale, creating again a <code>LowRankTerms</code> representation. The scale matrix is the diagonal matrix with the inverse of the square root of the low-rank approximation using the Woodbury identity for analytic inversion.</p> <p>Parameters:</p> Name Type Description Default <code>curv_estimate</code> <code>LowRankTerms</code> <p>Low-rank curvature estimate.</p> required <p>Returns:</p> Type Description <code>dict[str, LowRankTerms]</code> <p>A dictionary with the posterior state represented as <code>LowRankTerms</code>.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def low_rank_prec_to_posterior_state(\n    curv_estimate: LowRankTerms,\n) -&gt; dict[str, LowRankTerms]:\n    \"\"\"Convert the low-rank precision representation to a posterior state.\n\n    The scalar component and eigenvalues of the low-rank curvature estimate are\n    transformed to represent the posterior scale, creating again a `LowRankTerms`\n    representation. The scale matrix is the diagonal matrix with the inverse of the\n    square root of the low-rank approximation using the Woodbury identity for analytic\n    inversion.\n\n    Args:\n        curv_estimate: Low-rank curvature estimate.\n\n    Returns:\n        A dictionary with the posterior state represented as `LowRankTerms`.\n    \"\"\"\n    U, S, scalar = jax.tree_util.tree_leaves(curv_estimate)\n    scalar_sqrt_inv = jnp.reciprocal(jnp.sqrt(scalar))\n    return {\n        \"scale\": LowRankTerms(\n            U=U,\n            S=jnp.reciprocal(jnp.sqrt(S + scalar)) - scalar_sqrt_inv,\n            scalar=scalar_sqrt_inv,\n        )\n    }\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.low_rank_posterior_state_to_scale","title":"low_rank_posterior_state_to_scale","text":"<pre><code>low_rank_posterior_state_to_scale(state: dict[str, LowRankTerms]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a matrix-vector product function for the scale matrix.</p> <p>The state dictionary containing the low-rank representation of the covariance state is used to create a function that computes the matrix-vector product for the scale matrix. The scale matrix is the diagonal matrix with the inverse of the square root of the eigenvalues.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, LowRankTerms]</code> <p>Dictionary containing the low-rank scale.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the scale matrix-vector product.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def low_rank_posterior_state_to_scale(\n    state: dict[str, LowRankTerms],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a matrix-vector product function for the scale matrix.\n\n    The state dictionary containing the low-rank representation of the covariance state\n    is used to create a function that computes the matrix-vector product for the scale\n    matrix. The scale matrix is the diagonal matrix with the inverse of the square root\n    of the eigenvalues.\n\n    Args:\n        state: Dictionary containing the low-rank scale.\n\n    Returns:\n        A function that computes the scale matrix-vector product.\n    \"\"\"\n    return create_low_rank_mv(state[\"scale\"])\n</code></pre>"},{"location":"reference/curv/low_rank/#laplax.curv.low_rank.low_rank_posterior_state_to_cov","title":"low_rank_posterior_state_to_cov","text":"<pre><code>low_rank_posterior_state_to_cov(state: dict[str, LowRankTerms]) -&gt; Callable[[FlatParams], FlatParams]\n</code></pre> <p>Create a matrix-vector product function for the covariance matrix.</p> <p>The state dictionary containing the low-rank representation of the covariance state is used to create a function that computes the matrix-vector product for the covariance matrix. The covariance matrix is the low-rank approximation squared.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, LowRankTerms]</code> <p>Dictionary containing the low-rank scale.</p> required <p>Returns:</p> Type Description <code>Callable[[FlatParams], FlatParams]</code> <p>A function that computes the covariance matrix-vector product.</p> Source code in <code>laplax/curv/low_rank.py</code> <pre><code>def low_rank_posterior_state_to_cov(\n    state: dict[str, LowRankTerms],\n) -&gt; Callable[[FlatParams], FlatParams]:\n    \"\"\"Create a matrix-vector product function for the covariance matrix.\n\n    The state dictionary containing the low-rank representation of the covariance state\n    is used to create a function that computes the matrix-vector product for the\n    covariance matrix. The covariance matrix is the low-rank approximation squared.\n\n    Args:\n        state: Dictionary containing the low-rank scale.\n\n    Returns:\n        A function that computes the covariance matrix-vector product.\n    \"\"\"\n    return create_low_rank_mv(low_rank_square(state[\"scale\"]))\n</code></pre>"},{"location":"reference/curv/utils/","title":"laplax.curv.utils","text":"<p>Utility functions for curvature estimation.</p>"},{"location":"reference/curv/utils/#laplax.curv.utils.LowRankTerms","title":"LowRankTerms  <code>dataclass</code>","text":"<p>Components of the low-rank curvature approximation.</p> <p>This dataclass encapsulates the results of the low-rank approximation, including the eigenvectors, eigenvalues, and a scalar factor which can be used for the prior.</p> <p>Attributes:</p> Name Type Description <code>U</code> <code>Num[Array, 'P R']</code> <p>Matrix of eigenvectors, where each column corresponds to an eigenvector.</p> <code>S</code> <code>Num[Array, ' R']</code> <p>Array of eigenvalues associated with the eigenvectors.</p> <code>scalar</code> <code>Float[Array, '']</code> <p>Scalar factor added to the matrix during the approximation.</p> Source code in <code>laplax/curv/utils.py</code> <pre><code>@dataclass\nclass LowRankTerms:\n    \"\"\"Components of the low-rank curvature approximation.\n\n    This dataclass encapsulates the results of the low-rank approximation, including\n    the eigenvectors, eigenvalues, and a scalar factor which can be used for the prior.\n\n    Attributes:\n        U: Matrix of eigenvectors, where each column corresponds to an eigenvector.\n        S: Array of eigenvalues associated with the eigenvectors.\n        scalar: Scalar factor added to the matrix during the approximation.\n    \"\"\"\n\n    U: Num[Array, \"P R\"]\n    S: Num[Array, \" R\"]\n    scalar: Float[Array, \"\"]\n</code></pre>"},{"location":"reference/curv/utils/#laplax.curv.utils.get_matvec","title":"get_matvec","text":"<pre><code>get_matvec(A: Callable | Array, *, layout: Layout | None = None, jit: bool = True) -&gt; tuple[Callable[[Array], Array], int]\n</code></pre> <p>Returns a function that computes the matrix-vector product.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Callable | Array</code> <p>Either a jnp.ndarray or a callable performing the operation.</p> required <code>layout</code> <code>Layout | None</code> <p>Required if <code>A</code> is callable; ignored if <code>A</code> is an array.</p> <code>None</code> <code>jit</code> <code>bool</code> <p>Whether to jit-compile the operator.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Callable[[Array], Array], int]</code> <p>A tuple (matvec, input_dim) where matvec is the callable operator.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When <code>A</code> is a callable but <code>layout</code> is not provided.</p> Source code in <code>laplax/curv/utils.py</code> <pre><code>def get_matvec(\n    A: Callable | Array,\n    *,\n    layout: Layout | None = None,\n    jit: bool = True,\n) -&gt; tuple[Callable[[Array], Array], int]:\n    \"\"\"Returns a function that computes the matrix-vector product.\n\n    Args:\n        A: Either a jnp.ndarray or a callable performing the operation.\n        layout: Required if `A` is callable; ignored if `A` is an array.\n        jit: Whether to jit-compile the operator.\n\n    Returns:\n        A tuple (matvec, input_dim) where matvec is the callable operator.\n\n    Raises:\n        TypeError: When `A` is a callable but `layout` is not provided.\n    \"\"\"\n    if isinstance(A, jnp.ndarray):\n        size = A.shape[0]\n\n        def matvec(x):\n            return A @ x\n\n    else:\n        matvec = A\n\n        if layout is None:\n            msg = \"For a callable A, please provide `layout` in PyTree or int format.\"\n            raise TypeError(msg)\n        if isinstance(layout, int):\n            size = layout\n        else:\n            try:\n                flatten, unflatten = create_pytree_flattener(layout)\n                matvec = wrap_function(matvec, input_fn=unflatten, output_fn=flatten)\n                size = get_size(layout)\n            except (ValueError, TypeError) as exc:\n                msg = (\n                    \"For a callable A, please provide `layout` in PyTree or int format.\"\n                )\n                raise TypeError(msg) from exc\n\n    if jit:\n        matvec = jax.jit(matvec)\n\n    return matvec, size\n</code></pre>"},{"location":"reference/curv/utils/#laplax.curv.utils.log_sigmoid_cross_entropy","title":"log_sigmoid_cross_entropy","text":"<pre><code>log_sigmoid_cross_entropy(logits: Num[Array, ...], targets: Num[Array, ...]) -&gt; Num[Array, ...]\n</code></pre> <p>Computes log sigmoid cross entropy given logits and targets.</p> <p>This function computes the cross entropy loss between the sigmoid of the logits and the target values. The formula implemented is:</p> \\[ \\mathcal{L}(f(x, \\theta), y) = -y \\cdot \\log \\sigma(f(x, \\theta)) - (1 - y) \\cdot \\log \\sigma(-f(x, \\theta)) \\] <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Num[Array, ...]</code> <p>The predicted logits before sigmoid activation</p> required <code>targets</code> <code>Num[Array, ...]</code> <p>The target values (0 or 1)</p> required <p>Returns:</p> Type Description <code>Num[Array, ...]</code> <p>The computed loss value</p> Source code in <code>laplax/curv/utils.py</code> <pre><code>def log_sigmoid_cross_entropy(\n    logits: Num[Array, \"...\"], targets: Num[Array, \"...\"]\n) -&gt; Num[Array, \"...\"]:\n    r\"\"\"Computes log sigmoid cross entropy given logits and targets.\n\n    This function computes the cross entropy loss between the sigmoid of the logits\n    and the target values. The formula implemented is:\n\n    $$\n    \\mathcal{L}(f(x, \\theta), y) = -y \\cdot \\log \\sigma(f(x, \\theta)) -\n    (1 - y) \\cdot \\log \\sigma(-f(x, \\theta))\n    $$\n\n    Args:\n        logits: The predicted logits before sigmoid activation\n        targets: The target values (0 or 1)\n\n    Returns:\n        The computed loss value\n    \"\"\"\n    return -targets * jax.nn.log_sigmoid(logits) - (1 - targets) * jax.nn.log_sigmoid(\n        -logits\n    )\n</code></pre>"},{"location":"reference/curv/utils/#laplax.curv.utils.concatenate_model_and_loss_fn","title":"concatenate_model_and_loss_fn","text":"<pre><code>concatenate_model_and_loss_fn(model_fn: ModelFn, loss_fn: LossFn | str | Callable, *, vmap_over_data: bool = False) -&gt; Callable[[InputArray, TargetArray, Params], Num[Array, ...]]\n</code></pre> <p>Combine a model function and a loss function into a single callable.</p> <p>This creates a new function that evaluates the model and applies the specified loss function. If <code>vmap_over_data</code> is <code>True</code>, the model function is vectorized over the batch dimension using <code>jax.vmap</code>.</p> <p>Mathematically, the combined function computes:</p> \\[ \\mathcal{L}(x, y, \\theta) = \\text{loss}(f(x, \\theta), y), \\] <p>where \\(f\\) is the model function, \\(\\theta\\) are the model parameters, \\(x\\) is the input, \\(y\\) is the target, and \\(\\mathcal{L}\\) is the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate.</p> required <code>loss_fn</code> <code>LossFn | str | Callable</code> <p>The loss function to apply. Supported options are:</p> <ul> <li><code>LossFn.MSE</code> for mean squared error.</li> <li><code>LossFn.BINARY_CROSS_ENTROPY</code> for binary cross-entropy loss.</li> <li><code>LossFn.CROSSENTROPY</code> for cross-entropy loss.</li> <li><code>LossFn.NONE</code> for no loss.</li> <li>A custom callable loss function.</li> </ul> required <code>vmap_over_data</code> <code>bool</code> <p>Whether the model function should be vectorized over the data.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[InputArray, TargetArray, Params], Num[Array, ...]]</code> <p>A combined function that computes the loss for given inputs, targets, and parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the loss function is unknown.</p> Source code in <code>laplax/curv/utils.py</code> <pre><code>def concatenate_model_and_loss_fn(\n    model_fn: ModelFn,  # type: ignore[reportRedeclaration]\n    loss_fn: LossFn | str | Callable,\n    *,\n    vmap_over_data: bool = False,\n) -&gt; Callable[[InputArray, TargetArray, Params], Num[Array, \"...\"]]:\n    r\"\"\"Combine a model function and a loss function into a single callable.\n\n    This creates a new function that evaluates the model and applies the specified\n    loss function. If `vmap_over_data` is `True`, the model function is vectorized over\n    the batch dimension using `jax.vmap`.\n\n    Mathematically, the combined function computes:\n\n    $$\n    \\mathcal{L}(x, y, \\theta) = \\text{loss}(f(x, \\theta), y),\n    $$\n\n    where $f$ is the model function, $\\theta$ are the model parameters, $x$ is the\n    input, $y$ is the target, and $\\mathcal{L}$ is the loss function.\n\n    Args:\n        model_fn: The model function to evaluate.\n        loss_fn: The loss function to apply. Supported options are:\n\n            - `LossFn.MSE` for mean squared error.\n            - `LossFn.BINARY_CROSS_ENTROPY` for binary cross-entropy loss.\n            - `LossFn.CROSSENTROPY` for cross-entropy loss.\n            - `LossFn.NONE` for no loss.\n            - A custom callable loss function.\n\n        vmap_over_data: Whether the model function should be vectorized over the data.\n\n    Returns:\n        A combined function that computes the loss for given inputs, targets, and\n            parameters.\n\n    Raises:\n        ValueError: When the loss function is unknown.\n    \"\"\"\n    if vmap_over_data:\n        model_fn = jax.vmap(model_fn, in_axes=(0, None))\n\n    if loss_fn == LossFn.MSE:\n\n        def loss_wrapper(\n            input: InputArray, target: TargetArray, params: Params\n        ) -&gt; Num[Array, \"...\"]:\n            return jnp.sum((model_fn(input, params) - target) ** 2)\n\n        return loss_wrapper\n\n    if loss_fn == LossFn.CROSS_ENTROPY:\n\n        def loss_wrapper(\n            input: InputArray, target: TargetArray, params: Params\n        ) -&gt; Num[Array, \"...\"]:\n            return log_sigmoid_cross_entropy(model_fn(input, params), target)\n\n        return loss_wrapper\n\n    if callable(loss_fn):\n\n        def loss_wrapper(\n            input: InputArray, target: TargetArray, params: Params\n        ) -&gt; Num[Array, \"...\"]:\n            return loss_fn(model_fn(input, params), target)\n\n        return loss_wrapper\n\n    msg = f\"unknown loss function: {loss_fn}\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"reference/eval/calibrate/","title":"laplax.eval.calibrate","text":"<p>Calibration utilities for optimizing prior precision in probabilistic models.</p> <p>This script provides utilities for optimizing prior precision in probabilistic models. It includes functions to:</p> <ul> <li>Evaluate metrics for given prior arguments and datasets.</li> <li>Perform grid search to optimize prior precision using objective functions.</li> <li>Optimize prior precision over a logarithmic grid interval.</li> </ul> <p>The script leverages JAX for numerical operations, Loguru for logging, and custom utilities from the <code>laplax</code> package.</p>"},{"location":"reference/eval/calibrate/#laplax.eval.calibrate.evaluate_for_given_prior_arguments","title":"evaluate_for_given_prior_arguments","text":"<pre><code>evaluate_for_given_prior_arguments(*, data: Data, set_prob_predictive: Callable, metric: Callable = chi_squared_zero, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>Evaluate the metric for a given set of prior arguments and data.</p> <p>This function computes predictions for the input data using a probabilistic predictive function generated by <code>set_prob_predictive</code>. It then evaluates a specified metric using these predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Dataset containing inputs and targets.</p> required <code>set_prob_predictive</code> <code>Callable</code> <p>A callable that generates a probabilistic predictive function.</p> required <code>metric</code> <code>Callable</code> <p>A callable metric function to evaluate the predictions (default: <code>calibration_metric</code>).</p> <code>chi_squared_zero</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to <code>set_prob_predictive</code> and <code>jax.lax.map</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The evaluated metric value.</p> Source code in <code>laplax/eval/calibrate.py</code> <pre><code>def evaluate_for_given_prior_arguments(\n    *,\n    data: Data,\n    set_prob_predictive: Callable,\n    metric: Callable = chi_squared_zero,\n    **kwargs: Kwargs,\n) -&gt; Float:\n    \"\"\"Evaluate the metric for a given set of prior arguments and data.\n\n    This function computes predictions for the input data using a probabilistic\n    predictive function generated by `set_prob_predictive`. It then evaluates a\n    specified metric using these predictions.\n\n    Args:\n        data: Dataset containing inputs and targets.\n        set_prob_predictive: A callable that generates a probabilistic predictive\n            function.\n        metric: A callable metric function to evaluate the predictions\n            (default: `calibration_metric`).\n        **kwargs: Additional arguments passed to `set_prob_predictive` and\n            `jax.lax.map`.\n\n    Returns:\n        The evaluated metric value.\n    \"\"\"\n    prob_predictive = set_prob_predictive(**kwargs)\n\n    def evaluate_data(dp: Data) -&gt; dict[str, Array]:\n        return {**prob_predictive(dp[\"input\"]), \"target\": dp[\"target\"]}\n\n    res = metric(\n        **jax.lax.map(\n            evaluate_data,\n            data,\n            batch_size=kwargs.get(\n                \"evaluate_for_given_prior_arguments_batch_size\",\n                kwargs.get(\"data_batch_size\"),\n            ),\n        )\n    )\n    return res\n</code></pre>"},{"location":"reference/eval/calibrate/#laplax.eval.calibrate.grid_search","title":"grid_search","text":"<pre><code>grid_search(prior_prec_interval: Array, objective: Callable[[PriorArguments], float], patience: int | None = None, max_iterations: int | None = None) -&gt; Float\n</code></pre> <p>Perform grid search to optimize prior precision.</p> <p>This function iteratively evaluates an objective function over a range of prior precisions. It tracks the performance and stops early if results increase consecutively for a specified number of iterations (<code>patience</code>). The prior precision which scores the lowest is returned.</p> <p>Parameters:</p> Name Type Description Default <code>prior_prec_interval</code> <code>Array</code> <p>An array of prior precision values to search.</p> required <code>objective</code> <code>Callable[[PriorArguments], float]</code> <p>A callable objective function that takes <code>PriorArguments</code> as input and returns a float result.</p> required <code>patience</code> <code>int | None</code> <p>The number of consecutive iterations with increasing results to tolerate before stopping (default: 5).</p> <code>None</code> <code>max_iterations</code> <code>int | None</code> <p>The maximum number of iterations to perform (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>Float</code> <p>The prior precision value that minimizes the objective function.</p> Source code in <code>laplax/eval/calibrate.py</code> <pre><code>def grid_search(\n    prior_prec_interval: Array,\n    objective: Callable[[PriorArguments], float],\n    patience: int | None = None,\n    max_iterations: int | None = None,\n) -&gt; Float:\n    \"\"\"Perform grid search to optimize prior precision.\n\n    This function iteratively evaluates an objective function over a range of\n    prior precisions. It tracks the performance and stops early if results\n    increase consecutively for a specified number of iterations (`patience`).\n    The prior precision which scores the lowest is returned.\n\n    Args:\n        prior_prec_interval: An array of prior precision values to search.\n        objective: A callable objective function that takes `PriorArguments` as input\n            and returns a float result.\n        patience: The number of consecutive iterations with increasing results to\n            tolerate before stopping (default: 5).\n        max_iterations: The maximum number of iterations to perform (default: None).\n\n    Returns:\n        The prior precision value that minimizes the objective function.\n    \"\"\"\n    results, prior_precs = [], []\n    increasing_count = 0\n    previous_result = None\n\n    for iteration, prior_prec in enumerate(prior_prec_interval):\n        start_time = time.perf_counter()\n        try:\n            result = objective({\"prior_prec\": prior_prec})\n        except ValueError as error:\n            logger.warning(f\"Caught an exception in validate {error}\")\n            result = float(\"inf\")\n\n        if jnp.isnan(result):\n            logger.info(\"Caught nan, setting result to inf.\")\n            result = float(\"inf\")\n\n        # Logging for performance and tracking\n        logger.info(\n            f\"Took {time.perf_counter() - start_time:.4f} seconds, \"\n            f\"prior prec: {prior_prec:.4f}, \"\n            f\"result: {result:.6f}\",\n        )\n\n        results.append(result)\n        prior_precs.append(prior_prec)\n\n        # If we have a previous result, check if the result has increased\n        if patience is not None and previous_result is not None:\n            if result &gt; previous_result:\n                increasing_count += 1\n                logger.info(f\"Result increased, increasing_count = {increasing_count}\")\n            else:\n                increasing_count = 0\n\n            # Stop if the results have increased for `patience` consecutive iterations\n            if increasing_count &gt;= patience:\n                break\n\n        previous_result = result\n\n        # Check if maximum iterations reached\n        if max_iterations is not None and iteration &gt;= max_iterations:\n            logger.info(f\"Stopping due to reaching max iterations = {max_iterations}\")\n            break\n\n    best_prior_prec = prior_precs[np.nanargmin(results)]\n    logger.info(f\"Chosen prior prec = {best_prior_prec:.4f}\")\n\n    return best_prior_prec\n</code></pre>"},{"location":"reference/eval/calibrate/#laplax.eval.calibrate.optimize_prior_prec","title":"optimize_prior_prec","text":"<pre><code>optimize_prior_prec(objective: Callable[[PriorArguments], float], log_prior_prec_min: float = -5.0, log_prior_prec_max: float = 6.0, grid_size: int = 300, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>Optimize prior precision using logarithmic grid search.</p> <p>This function creates a logarithmically spaced interval of prior precision values and performs a grid search to find the optimal value that minimizes the specified objective function.</p> <p>Parameters:</p> Name Type Description Default <code>objective</code> <code>Callable[[PriorArguments], float]</code> <p>A callable objective function that takes <code>PriorArguments</code> as input and returns a float result.</p> required <code>log_prior_prec_min</code> <code>float</code> <p>The base-10 logarithm of the minimum prior precision value (default: -5.0).</p> <code>-5.0</code> <code>log_prior_prec_max</code> <code>float</code> <p>The base-10 logarithm of the maximum prior precision value (default: 6.0).</p> <code>6.0</code> <code>grid_size</code> <code>int</code> <p>The number of points in the grid interval (default: 300).</p> <code>300</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to <code>grid_search</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The optimized prior precision value.</p> Source code in <code>laplax/eval/calibrate.py</code> <pre><code>def optimize_prior_prec(\n    objective: Callable[[PriorArguments], float],\n    log_prior_prec_min: float = -5.0,\n    log_prior_prec_max: float = 6.0,\n    grid_size: int = 300,\n    **kwargs: Kwargs,\n) -&gt; Float:\n    \"\"\"Optimize prior precision using logarithmic grid search.\n\n    This function creates a logarithmically spaced interval of prior precision\n    values and performs a grid search to find the optimal value that minimizes\n    the specified objective function.\n\n    Args:\n        objective: A callable objective function that takes `PriorArguments` as input\n            and returns a float result.\n        log_prior_prec_min: The base-10 logarithm of the minimum prior precision\n            value (default: -5.0).\n        log_prior_prec_max: The base-10 logarithm of the maximum prior precision\n            value (default: 6.0).\n        grid_size: The number of points in the grid interval (default: 300).\n        **kwargs: Additional arguments passed to `grid_search`.\n\n    Returns:\n        The optimized prior precision value.\n    \"\"\"\n    prior_prec_interval = jnp.logspace(\n        start=log_prior_prec_min,\n        stop=log_prior_prec_max,\n        num=grid_size,\n    )\n    prior_prec = grid_search(\n        prior_prec_interval,\n        objective,\n        **kwargs,\n    )\n\n    return prior_prec\n</code></pre>"},{"location":"reference/eval/likelihood/","title":"laplax.eval.likelihood","text":"<p>Compute the marginal log-likelihood for different curvature estimations.</p> <p>Implemented according to: Smith, J., et al. (2023): Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. Proceedings of the International Conference on Machine Learning, 25(3), 234-245.</p> <p>It includes functions to calculate the marginal log-likelihood based on various curvature approximations, including:</p> <ul> <li>full</li> <li>diagonal</li> <li>low-rank</li> </ul>"},{"location":"reference/eval/likelihood/#laplax.eval.likelihood.joint_log_likelihood","title":"joint_log_likelihood","text":"<pre><code>joint_log_likelihood(full_fn: Callable, prior_arguments: PriorArguments, params: Params, data: Data) -&gt; Float\n</code></pre> <p>Computes the joint log-likelihood for a model.</p> <p>This function computes the joint log-likelihood for a model, which is given by:</p> \\[ \\log p(D, \\theta | M) = \\log p(D | \\theta, M) + \\log p(\\theta | M) \\] <p>If we assume a Gaussian prior on the parameters with precision \\(\\tau^{-2}\\), then the log-prior is given by:</p> \\[     \\log p(\\theta \\vert \\tau^{-2}) = -\\frac{1}{2} \\log |\\frac{1}{2\\pi} \\tau^{-2}     \\vert - \\frac{1}{2} \\tau^{-2} \\vert \\theta \\vert^2 \\] <p>Parameters:</p> Name Type Description Default <code>full_fn</code> <code>Callable</code> <p>model loss function that has the parameters and the data as input and output the loss</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>prior arguments</p> required <code>params</code> <code>Params</code> <p>model parameters</p> required <code>data</code> <code>Data</code> <p>training data</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The joint log-likelihood.</p> Source code in <code>laplax/eval/likelihood.py</code> <pre><code>def joint_log_likelihood(\n    full_fn: Callable,\n    prior_arguments: PriorArguments,\n    params: Params,\n    data: Data,\n) -&gt; Float:\n    r\"\"\"Computes the joint log-likelihood for a model.\n\n    This function computes the joint log-likelihood for a model, which is given by:\n\n    $$\n    \\log p(D, \\theta | M) = \\log p(D | \\theta, M) + \\log p(\\theta | M)\n    $$\n\n    If we assume a Gaussian prior on the parameters with precision $\\tau^{-2}$,\n    then the log-prior is given by:\n\n    $$\n        \\log p(\\theta \\vert \\tau^{-2}) = -\\frac{1}{2} \\log |\\frac{1}{2\\pi} \\tau^{-2}\n        \\vert - \\frac{1}{2} \\tau^{-2} \\vert \\theta \\vert^2\n    $$\n\n    Args:\n        full_fn: model loss function that has the parameters and the data as input and\n            output the loss\n        prior_arguments: prior arguments\n        params: model parameters\n        data: training data\n\n    Returns:\n        The joint log-likelihood.\n    \"\"\"\n    # Compute the log-prior\n    params_square_norm = jnp.sum(full_flatten(params) ** 2)\n    prior_prec = prior_arguments[\"prior_prec\"]\n    log_prior_term1 = -0.5 * prior_prec * params_square_norm\n    log_prior_term2 = (\n        -0.5 * get_size(params) * (jnp.log(2 * jnp.pi) - jnp.log(prior_prec))\n    )\n    log_prior = log_prior_term1 + log_prior_term2\n\n    # Compute the log-likelihood\n    sigma = prior_arguments.get(\"sigma\", 1.0)\n    log_likelihood = -(1 / (2 * sigma)) * full_fn(\n        data[\"input\"],\n        data[\"target\"],\n        params,\n    )  # Assumes summed loss\n\n    # Compute the joint log-likelihood\n    return log_likelihood + log_prior\n</code></pre>"},{"location":"reference/eval/likelihood/#laplax.eval.likelihood.full_marginal_log_likelihood","title":"full_marginal_log_likelihood","text":"<pre><code>full_marginal_log_likelihood(posterior_precision: Num[Array, 'P P'], prior_arguments: PriorArguments, full_fn: Callable, params: Params, data: Data) -&gt; Float\n</code></pre> <p>Computes the marginal log likelihood for the full posterior function.</p> <p>The marginal log-likelihood is given by:</p> \\[     \\log p(D | M) = \\log p(D, \\theta_* | M)     - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert \\] <p>Parameters:</p> Name Type Description Default <code>posterior_precision</code> <code>Num[Array, 'P P']</code> <p>posterior precision</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>prior arguments</p> required <code>full_fn</code> <code>Callable</code> <p>model loss function that has the parameters and the data as input and output the loss</p> required <code>params</code> <code>Params</code> <p>model parameters</p> required <code>data</code> <code>Data</code> <p>training data</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The marginal likelihood estimation</p> Source code in <code>laplax/eval/likelihood.py</code> <pre><code>def full_marginal_log_likelihood(\n    posterior_precision: Num[Array, \"P P\"],\n    prior_arguments: PriorArguments,\n    full_fn: Callable,\n    params: Params,\n    data: Data,\n) -&gt; Float:\n    r\"\"\"Computes the marginal log likelihood for the full posterior function.\n\n    The marginal log-likelihood is given by:\n\n    $$\n        \\log p(D | M) = \\log p(D, \\theta_* | M)\n        - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert\n    $$\n\n    Args:\n        posterior_precision: posterior precision\n        prior_arguments: prior arguments\n        full_fn: model loss function that has the parameters and the data as input and\n            output the loss\n        params: model parameters\n        data: training data\n\n    Returns:\n        The marginal likelihood estimation\n    \"\"\"\n    # Compute the log-likelihood\n    log_likelihood = joint_log_likelihood(\n        full_fn=full_fn,\n        prior_arguments=prior_arguments,\n        params=params,\n        data=data,\n    )\n\n    # Log det of posterior precision\n    log_det_H = jnp.linalg.slogdet(posterior_precision)[1]\n    param_count = get_size(params)\n    evidence = -0.5 * param_count * jnp.log(2 * jnp.pi) + 0.5 * log_det_H\n\n    # Compute the marginal log-likelihood\n    lml = log_likelihood - evidence\n\n    return lml\n</code></pre>"},{"location":"reference/eval/likelihood/#laplax.eval.likelihood.diagonal_marginal_log_likelihood","title":"diagonal_marginal_log_likelihood","text":"<pre><code>diagonal_marginal_log_likelihood(posterior_precision: FlatParams, prior_arguments: PriorArguments, full_fn: Callable, params: Params, data: Data) -&gt; Float\n</code></pre> <p>Computes the marginal log likelihood for a diagonal approximation.</p> <p>The marginal log-likelihood is given by:</p> \\[     \\log p(D | M) = \\log p(D, \\theta_* | M)         - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert \\] <p>Here the log-determinant of the posterior precision simplifies to:</p> \\[     \\sum_{i=1}^{P} \\log d_i \\] <p>where \\(d_i\\) is the \\(i\\)-th diagonal element of the posterior precision.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_precision</code> <code>FlatParams</code> <p>posterior precision</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>prior arguments</p> required <code>full_fn</code> <code>Callable</code> <p>model loss function that has the parameters and the data as input and output the loss</p> required <code>params</code> <code>Params</code> <p>model parameters</p> required <code>data</code> <code>Data</code> <p>training data</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The marginal likelihood estimation.</p> Source code in <code>laplax/eval/likelihood.py</code> <pre><code>def diagonal_marginal_log_likelihood(\n    posterior_precision: FlatParams,\n    prior_arguments: PriorArguments,\n    full_fn: Callable,\n    params: Params,\n    data: Data,\n) -&gt; Float:\n    r\"\"\"Computes the marginal log likelihood for a diagonal approximation.\n\n    The marginal log-likelihood is given by:\n\n    $$\n        \\log p(D | M) = \\log p(D, \\theta_* | M)\n            - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert\n    $$\n\n    Here the log-determinant of the posterior precision simplifies to:\n\n    $$\n        \\sum_{i=1}^{P} \\log d_i\n    $$\n\n    where $d_i$ is the $i$-th diagonal element of the posterior precision.\n\n    Args:\n        posterior_precision: posterior precision\n        prior_arguments: prior arguments\n        full_fn: model loss function that has the parameters and the data as input and\n            output the loss\n        params: model parameters\n        data: training data\n\n    Returns:\n        The marginal likelihood estimation.\n    \"\"\"\n    # Compute the log-likelihood\n    log_likelihood = joint_log_likelihood(\n        full_fn=full_fn,\n        prior_arguments=prior_arguments,\n        params=params,\n        data=data,\n    )\n\n    # Log det of posterior precision\n    log_det_H = jnp.sum(jnp.log(posterior_precision))\n    param_count = get_size(params)\n    evidence = -0.5 * param_count * jnp.log(2 * jnp.pi) + 0.5 * log_det_H\n\n    # Compute the marginal log-likelihood\n    lml = log_likelihood - evidence\n\n    return lml\n</code></pre>"},{"location":"reference/eval/likelihood/#laplax.eval.likelihood.low_rank_marginal_log_likelihood","title":"low_rank_marginal_log_likelihood","text":"<pre><code>low_rank_marginal_log_likelihood(posterior_precision: LowRankTerms, prior_arguments: PriorArguments, full_fn: Callable, params: Params, data: Data) -&gt; Float\n</code></pre> <p>Computes the marginal log likelihood for a low-rank approximation.</p> <p>The marginal log-likelihood is given by:</p> \\[ \\log p(D | M) = \\log p(D, \\theta_* | M)     - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert \\] <p>Here the log-determinant of the posterior precision (with \\(U\\Lambda U^T + D\\)) simplifies to:</p> \\[ \\sum_{i=1}^{R} \\log ( 1 + d_i^{-1} \\cdot \\lambda_i) + \\sum_{i=1}^{P} \\log d_i \\] <p>where \\(d_i\\) is the \\(i\\)-th diagonal element of the prior precision and \\(\\lambda_i\\) is the \\(i\\)-th eigenvalue of the low-rank approximation.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_precision</code> <code>LowRankTerms</code> <p>posterior precision</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>prior arguments</p> required <code>full_fn</code> <code>Callable</code> <p>model loss function that has the parameters and the data as input and output the loss</p> required <code>params</code> <code>Params</code> <p>model parameters</p> required <code>data</code> <code>Data</code> <p>training data</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The marginal likelihood estimation.</p> Source code in <code>laplax/eval/likelihood.py</code> <pre><code>def low_rank_marginal_log_likelihood(\n    posterior_precision: LowRankTerms,\n    prior_arguments: PriorArguments,\n    full_fn: Callable,\n    params: Params,\n    data: Data,\n) -&gt; Float:\n    r\"\"\"Computes the marginal log likelihood for a low-rank approximation.\n\n    The marginal log-likelihood is given by:\n\n    $$\n    \\log p(D | M) = \\log p(D, \\theta_* | M)\n        - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert\n    $$\n\n    Here the log-determinant of the posterior precision (with $U\\Lambda U^T + D$)\n    simplifies to:\n\n    $$\n    \\sum_{i=1}^{R} \\log ( 1 + d_i^{-1} \\cdot \\lambda_i) + \\sum_{i=1}^{P} \\log d_i\n    $$\n\n    where $d_i$ is the $i$-th diagonal element of the prior precision and\n    $\\lambda_i$ is the $i$-th eigenvalue of the low-rank approximation.\n\n    Args:\n        posterior_precision: posterior precision\n        prior_arguments: prior arguments\n        full_fn: model loss function that has the parameters and the data as input and\n            output the loss\n        params: model parameters\n        data: training data\n\n    Returns:\n        The marginal likelihood estimation.\n    \"\"\"\n    # Compute the log-likelihood\n    log_likelihood = joint_log_likelihood(\n        full_fn=full_fn,\n        prior_arguments=prior_arguments,\n        params=params,\n        data=data,\n    )\n\n    # Log det of posterior precision\n    rank = posterior_precision.S.shape[0]\n    log_det_H = rank * jnp.log(posterior_precision.scalar) + jnp.sum(\n        jnp.log(1 + posterior_precision.scalar * posterior_precision.S)\n    )\n    param_count = get_size(params)\n    evidence = -0.5 * param_count * jnp.log(2 * jnp.pi) + 0.5 * log_det_H\n\n    # Compute the marginal log-likelihood\n    lml = log_likelihood - evidence\n\n    return lml\n</code></pre>"},{"location":"reference/eval/likelihood/#laplax.eval.likelihood.marginal_log_likelihood","title":"marginal_log_likelihood","text":"<pre><code>marginal_log_likelihood(curv_estimate: PyTree, prior_arguments: PriorArguments, data: Data, model_fn: ModelFn, params: Params, loss_fn: LossFn | str | Callable, curv_type: CurvatureKeyType, *, vmap_over_data: bool = False, loss_scaling_factor: Float = 1.0) -&gt; Float\n</code></pre> <p>Compute the marginal log-likelihood for a given curvature approximation.</p> <p>The marginal log-likelihood is given by:</p> \\[ \\log p(D | M) = \\log p(D, \\theta_* | M)     - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert \\] <p>Here \\(H_{\\theta_*}\\) is the Hessian/GGN of the loss function evaluated at the model parameters. The likelihood function is given by the negative loss function.</p> <p>Parameters:</p> Name Type Description Default <code>curv_estimate</code> <code>PyTree</code> <p>curvature estimate</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>prior arguments</p> required <code>data</code> <code>Data</code> <p>training data</p> required <code>model_fn</code> <code>ModelFn</code> <p>model function</p> required <code>params</code> <code>Params</code> <p>model parameters</p> required <code>loss_fn</code> <code>LossFn | str | Callable</code> <p>loss function</p> required <code>curv_type</code> <code>CurvatureKeyType</code> <p>curvature type</p> required <code>vmap_over_data</code> <code>bool</code> <p>whether the model has a batch dimension</p> <code>False</code> <code>loss_scaling_factor</code> <code>Float</code> <p>loss scaling factor</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Float</code> <p>The marginal log-likelihood.</p> Source code in <code>laplax/eval/likelihood.py</code> <pre><code>def marginal_log_likelihood(\n    curv_estimate: PyTree,\n    prior_arguments: PriorArguments,\n    data: Data,\n    model_fn: ModelFn,\n    params: Params,\n    loss_fn: LossFn | str | Callable,\n    curv_type: CurvatureKeyType,\n    *,\n    vmap_over_data: bool = False,\n    loss_scaling_factor: Float = 1.0,\n) -&gt; Float:\n    r\"\"\"Compute the marginal log-likelihood for a given curvature approximation.\n\n    The marginal log-likelihood is given by:\n\n    $$\n    \\log p(D | M) = \\log p(D, \\theta_* | M)\n        - \\frac{1}{2} \\log |\\frac{1}{2\\pi} H_{\\theta_*}\\vert\n    $$\n\n    Here $H_{\\theta_*}$ is the Hessian/GGN of the loss function evaluated at the\n    model parameters. The likelihood function is given by the negative loss function.\n\n    Args:\n        curv_estimate: curvature estimate\n        prior_arguments: prior arguments\n        data: training data\n        model_fn: model function\n        params: model parameters\n        loss_fn: loss function\n        curv_type: curvature type\n        vmap_over_data: whether the model has a batch dimension\n        loss_scaling_factor: loss scaling factor\n\n    Returns:\n        The marginal log-likelihood.\n    \"\"\"\n    full_fn = concatenate_model_and_loss_fn(\n        model_fn,\n        loss_fn,\n        vmap_over_data=vmap_over_data,\n    )\n\n    posterior_precision = CURVATURE_PRECISION_METHODS[curv_type](\n        curv_estimate,\n        prior_arguments,\n        loss_scaling_factor=loss_scaling_factor,\n    )\n\n    marginal_log_likelihood = CURVATURE_MARGINAL_LOG_LIKELIHOOD[curv_type](\n        posterior_precision,\n        prior_arguments,\n        full_fn,\n        params,\n        data,\n    )\n\n    return marginal_log_likelihood\n</code></pre>"},{"location":"reference/eval/metrics/","title":"laplax.eval.metrics","text":"<p>Regression and Classification Metrics for Uncertainty Quantification.</p> <p>This module provides a comprehensive suite of classification and regression metrics for evaluating probabilistic models.</p>"},{"location":"reference/eval/metrics/#laplax.eval.metrics--key-features","title":"Key Features","text":""},{"location":"reference/eval/metrics/#laplax.eval.metrics--classification-metrics","title":"Classification Metrics","text":"<ul> <li>Accuracy</li> <li>Top-k Accuracy</li> <li>Cross-Entropy</li> <li>Multiclass Brier Score</li> <li>Expected Calibration Error (ECE)</li> <li>Maximum Calibration Error (MCE)</li> </ul>"},{"location":"reference/eval/metrics/#laplax.eval.metrics--regression-metrics","title":"Regression Metrics","text":"<ul> <li>Root Mean Squared Error (RMSE)</li> <li>Chi-squared</li> <li>Negative Log-Likelihood (NLL) for Gaussian distributions</li> </ul>"},{"location":"reference/eval/metrics/#laplax.eval.metrics--bin-metrics","title":"Bin Metrics","text":"<ul> <li>Confidence and Correctness Metrics binned by confidence intervals</li> </ul> <p>The module leverages JAX for efficient numerical computation and supports flexible evaluation for diverse model outputs.</p>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.correctness","title":"correctness","text":"<pre><code>correctness(pred: Array, target: Array, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Determine if each target label matches the top-1 prediction.</p> <p>Computes a binary indicator for whether the predicted class matches the target class. If the target is a 2D array, it is first reduced to its class index using <code>argmax</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Array</code> <p>Array of predictions with shape <code>(batch_size, num_classes)</code>.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels, either 1D (class indices) or 2D (one-hot encoded).</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Boolean array of shape <code>(batch_size,)</code> indicating correctness for each prediction.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def correctness(pred: Array, target: Array, **kwargs: Kwargs) -&gt; Array:\n    \"\"\"Determine if each target label matches the top-1 prediction.\n\n    Computes a binary indicator for whether the predicted class matches the\n    target class. If the target is a 2D array, it is first reduced to its\n    class index using `argmax`.\n\n    Args:\n        pred: Array of predictions with shape `(batch_size, num_classes)`.\n        target: Array of ground truth labels, either 1D (class indices) or\n            2D (one-hot encoded).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Boolean array of shape `(batch_size,)` indicating correctness\n            for each prediction.\n    \"\"\"\n    del kwargs\n\n    pred = jnp.argmax(pred, axis=-1)\n\n    if target.ndim == 2:\n        target = jnp.argmax(target, axis=-1)\n\n    return pred == target\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.accuracy","title":"accuracy","text":"<pre><code>accuracy(pred: Array, target: Array, top_k: tuple[int] = (1,), **kwargs: Kwargs) -&gt; list[Array]\n</code></pre> <p>Compute top-k accuracy for specified values of k.</p> <p>For each k in <code>top_k</code>, this function calculates the fraction of samples where the ground truth label is among the top-k predictions. If the target is a 2D array, it is reduced to its class index using <code>argmax</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Array</code> <p>Array of predictions with shape <code>(batch_size, num_classes)</code>.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels, either 1D (class indices) or 2D (one-hot encoded).</p> required <code>top_k</code> <code>tuple[int]</code> <p>Tuple of integers specifying the values of k for top-k accuracy.</p> <code>(1,)</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Array]</code> <p>A list of accuracies corresponding to each k in <code>top_k</code>, expressed as percentages.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def accuracy(\n    pred: Array, target: Array, top_k: tuple[int] = (1,), **kwargs: Kwargs\n) -&gt; list[Array]:\n    \"\"\"Compute top-k accuracy for specified values of k.\n\n    For each k in `top_k`, this function calculates the fraction of samples\n    where the ground truth label is among the top-k predictions. If the target\n    is a 2D array, it is reduced to its class index using `argmax`.\n\n    Args:\n        pred: Array of predictions with shape `(batch_size, num_classes)`.\n        target: Array of ground truth labels, either 1D (class indices) or\n            2D (one-hot encoded).\n        top_k: Tuple of integers specifying the values of k for top-k accuracy.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        A list of accuracies corresponding to each k in `top_k`,\n            expressed as percentages.\n    \"\"\"\n    del kwargs\n    max_k = min(max(top_k), pred.shape[1])\n    batch_size = target.shape[0]\n\n    _, pred = lax.top_k(pred, max_k)\n    pred = pred.T\n\n    if target.ndim == 2:\n        target = jnp.argmax(target, axis=-1)\n\n    correctness = pred == target.reshape(1, -1)\n\n    return [\n        jnp.sum(correctness[: min(k, max_k)].reshape(-1).astype(jnp.float32))\n        * 100.0\n        / batch_size\n        for k in top_k\n    ]\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(prob_p: Array, prob_q: Array, axis: int = -1, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute cross-entropy between two probability distributions.</p> <p>This function calculates the cross-entropy of <code>prob_p</code> relative to <code>prob_q</code>, summing over the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>prob_p</code> <code>Array</code> <p>Array of true probability distributions.</p> required <code>prob_q</code> <code>Array</code> <p>Array of predicted probability distributions.</p> required <code>axis</code> <code>int</code> <p>Axis along which to compute the cross-entropy (default: -1).</p> <code>-1</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Cross-entropy values for each sample.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def cross_entropy(\n    prob_p: Array, prob_q: Array, axis: int = -1, **kwargs: Kwargs\n) -&gt; Array:\n    \"\"\"Compute cross-entropy between two probability distributions.\n\n    This function calculates the cross-entropy of `prob_p` relative to `prob_q`,\n    summing over the specified axis.\n\n    Args:\n        prob_p: Array of true probability distributions.\n        prob_q: Array of predicted probability distributions.\n        axis: Axis along which to compute the cross-entropy (default: -1).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Cross-entropy values for each sample.\n    \"\"\"\n    del kwargs\n    p_log_q = jax.scipy.special.xlogy(prob_p, prob_q)\n\n    return -p_log_q.sum(axis=axis)\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.multiclass_brier","title":"multiclass_brier","text":"<pre><code>multiclass_brier(prob: Array, target: Array, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute the multiclass Brier score.</p> <p>The Brier score is a measure of the accuracy of probabilistic predictions. For multiclass classification, it calculates the mean squared difference between the predicted probabilities and the true target.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>Array</code> <p>Array of predicted probabilities with shape <code>(batch_size, num_classes)</code>.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels, either 1D (class indices) or 2D (one-hot encoded).</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Mean Brier score across all samples.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def multiclass_brier(prob: Array, target: Array, **kwargs: Kwargs) -&gt; Array:\n    \"\"\"Compute the multiclass Brier score.\n\n    The Brier score is a measure of the accuracy of probabilistic predictions.\n    For multiclass classification, it calculates the mean squared difference\n    between the predicted probabilities and the true target.\n\n    Args:\n        prob: Array of predicted probabilities with shape `(batch_size, num_classes)`.\n        target: Array of ground truth labels, either 1D (class indices) or\n            2D (one-hot encoded).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Mean Brier score across all samples.\n    \"\"\"\n    del kwargs\n    if target.ndim == 1:\n        target = jax.nn.one_hot(target, num_classes=prob.shape[-1])\n\n    preds_squared_sum = jnp.sum(prob**2, axis=-1, keepdims=True)\n    score_components = 1 - 2 * prob + preds_squared_sum\n\n    return -jnp.mean(target * score_components)\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.calculate_bin_metrics","title":"calculate_bin_metrics","text":"<pre><code>calculate_bin_metrics(confidence: Array, correctness: Array, num_bins: int = 15, **kwargs: Kwargs) -&gt; tuple[Array, Array, Array]\n</code></pre> <p>Calculate bin-wise metrics for confidence and correctness.</p> <p>Computes the proportion of samples, average confidence, and average accuracy within each bin, where the bins are defined by evenly spaced confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>confidence</code> <code>Array</code> <p>Array of predicted confidence values with shape <code>(n,)</code>.</p> required <code>correctness</code> <code>Array</code> <p>Array of correctness labels (0 or 1) with shape <code>(n,)</code>.</p> required <code>num_bins</code> <code>int</code> <p>Number of bins for dividing the confidence range (default: 15).</p> <code>15</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Array, Array, Array]</code> <p>Tuple of arrays containing:</p> <ul> <li>Bin proportions: Proportion of samples in each bin.</li> <li>Bin confidences: Average confidence for each bin.</li> <li>Bin accuracies: Average accuracy for each bin.</li> </ul> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def calculate_bin_metrics(\n    confidence: Array, correctness: Array, num_bins: int = 15, **kwargs: Kwargs\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"Calculate bin-wise metrics for confidence and correctness.\n\n    Computes the proportion of samples, average confidence, and average accuracy\n    within each bin, where the bins are defined by evenly spaced confidence\n    intervals.\n\n    Args:\n        confidence: Array of predicted confidence values with shape `(n,)`.\n        correctness: Array of correctness labels (0 or 1) with shape `(n,)`.\n        num_bins: Number of bins for dividing the confidence range (default: 15).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Tuple of arrays containing:\n\n            - Bin proportions: Proportion of samples in each bin.\n            - Bin confidences: Average confidence for each bin.\n            - Bin accuracies: Average accuracy for each bin.\n    \"\"\"\n    del kwargs\n\n    bin_boundaries = jnp.linspace(0, 1, num_bins + 1)\n    indices = jnp.digitize(confidence, bin_boundaries) - 1\n    indices = jnp.clip(indices, min=0, max=num_bins - 1)\n\n    bin_counts = jnp.zeros(num_bins, dtype=confidence.dtype)\n    bin_confidences = jnp.zeros(num_bins, dtype=confidence.dtype)\n    bin_accuracies = jnp.zeros(num_bins, dtype=correctness.dtype)\n\n    bin_counts = bin_counts.at[indices].add(1)\n    bin_confidences = bin_confidences.at[indices].add(confidence)\n    bin_accuracies = bin_accuracies.at[indices].add(correctness)\n\n    bin_proportions = bin_counts / bin_counts.sum()\n    pos_counts = bin_counts &gt; 0\n    bin_confidences = jnp.where(pos_counts, bin_confidences / bin_counts, 0)\n    bin_accuracies = jnp.where(pos_counts, bin_accuracies / bin_counts, 0)\n\n    return bin_proportions, bin_confidences, bin_accuracies\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.calibration_error","title":"calibration_error","text":"<pre><code>calibration_error(confidence: Array, correctness: Array, num_bins: int, norm: CalibrationErrorNorm, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute the expected/maximum calibration error.</p> <p>Parameters:</p> Name Type Description Default <code>confidence</code> <code>Array</code> <p>Float tensor of shape (n,) containing predicted confidences.</p> required <code>correctness</code> <code>Array</code> <p>Float tensor of shape (n,) containing the true correctness labels.</p> required <code>num_bins</code> <code>int</code> <p>Number of equally sized bins.</p> required <code>norm</code> <code>CalibrationErrorNorm</code> <p>Whether to return ECE (L1 norm) or MCE (inf norm).</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The ECE/MCE.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def calibration_error(\n    confidence: jax.Array,\n    correctness: jax.Array,\n    num_bins: int,\n    norm: CalibrationErrorNorm,\n    **kwargs: Kwargs,\n) -&gt; jax.Array:\n    \"\"\"Compute the expected/maximum calibration error.\n\n    Args:\n        confidence: Float tensor of shape (n,) containing predicted confidences.\n        correctness: Float tensor of shape (n,) containing the true correctness\n            labels.\n        num_bins: Number of equally sized bins.\n        norm: Whether to return ECE (L1 norm) or MCE (inf norm).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The ECE/MCE.\n    \"\"\"\n    del kwargs\n    bin_proportions, bin_confidences, bin_accuracies = calculate_bin_metrics(\n        confidence, correctness, num_bins\n    )\n\n    abs_diffs = jnp.abs(bin_accuracies - bin_confidences)\n\n    if norm == CalibrationErrorNorm.L1:\n        score = (bin_proportions * abs_diffs).sum()\n    else:\n        score = abs_diffs.max()\n\n    return score\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.expected_calibration_error","title":"expected_calibration_error","text":"<pre><code>expected_calibration_error(confidence: Array, correctness: Array, num_bins: int, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute the expected calibration error.</p> <p>Parameters:</p> Name Type Description Default <code>confidence</code> <code>Array</code> <p>Float tensor of shape (n,) containing predicted confidences.</p> required <code>correctness</code> <code>Array</code> <p>Float tensor of shape (n,) containing the true correctness labels.</p> required <code>num_bins</code> <code>int</code> <p>Number of equally sized bins.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The ECE/MCE.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def expected_calibration_error(\n    confidence: jax.Array, correctness: jax.Array, num_bins: int, **kwargs: Kwargs\n) -&gt; jax.Array:\n    \"\"\"Compute the expected calibration error.\n\n    Args:\n        confidence: Float tensor of shape (n,) containing predicted confidences.\n        correctness: Float tensor of shape (n,) containing the true correctness\n            labels.\n        num_bins: Number of equally sized bins.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The ECE/MCE.\n\n    \"\"\"\n    del kwargs\n    return calibration_error(\n        confidence=confidence,\n        correctness=correctness,\n        num_bins=num_bins,\n        norm=CalibrationErrorNorm.L1,\n    )\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.maximum_calibration_error","title":"maximum_calibration_error","text":"<pre><code>maximum_calibration_error(confidence: Array, correctness: Array, num_bins: int, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute the maximum calibration error.</p> <p>Parameters:</p> Name Type Description Default <code>confidence</code> <code>Array</code> <p>Float tensor of shape (n,) containing predicted confidences.</p> required <code>correctness</code> <code>Array</code> <p>Float tensor of shape (n,) containing the true correctness labels.</p> required <code>num_bins</code> <code>int</code> <p>Number of equally sized bins.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The ECE/MCE.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def maximum_calibration_error(\n    confidence: jax.Array, correctness: jax.Array, num_bins: int, **kwargs: Kwargs\n) -&gt; jax.Array:\n    \"\"\"Compute the maximum calibration error.\n\n    Args:\n        confidence: Float tensor of shape (n,) containing predicted confidences.\n        correctness: Float tensor of shape (n,) containing the true correctness\n            labels.\n        num_bins: Number of equally sized bins.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The ECE/MCE.\n\n    \"\"\"\n    del kwargs\n\n    return calibration_error(\n        confidence=confidence,\n        correctness=correctness,\n        num_bins=num_bins,\n        norm=CalibrationErrorNorm.INF,\n    )\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.chi_squared","title":"chi_squared","text":"<pre><code>chi_squared(pred_mean: Array, pred_std: Array, target: Array, *, averaged: bool = True, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>Estimate the q-value for predictions.</p> <p>The \\(\\chi^2\\)-value is a measure of the squared error normalized by the predicted variance.</p> <p>Mathematically:</p> \\[ \\chi^2_{\\text{Avg}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}. \\] <p>Parameters:</p> Name Type Description Default <code>pred_mean</code> <code>Array</code> <p>Array of predicted means.</p> required <code>pred_std</code> <code>Array</code> <p>Array of predicted standard deviations.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels.</p> required <code>averaged</code> <code>bool</code> <p>Whether to return the mean or sum of the q-values.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The estimated q-value.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def chi_squared(\n    pred_mean: Array,\n    pred_std: Array,\n    target: Array,\n    *,\n    averaged: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Float:\n    r\"\"\"Estimate the q-value for predictions.\n\n    The $\\chi^2$-value is a measure of the squared error normalized by the predicted\n    variance.\n\n    Mathematically:\n\n    $$\n    \\chi^2_{\\text{Avg}}\n    = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}.\n    $$\n\n    Args:\n        pred_mean: Array of predicted means.\n        pred_std: Array of predicted standard deviations.\n        target: Array of ground truth labels.\n        averaged: Whether to return the mean or sum of the q-values.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The estimated q-value.\n    \"\"\"\n    del kwargs\n    val = jnp.power(pred_mean - target, 2) / jnp.power(pred_std, 2)\n    return jnp.mean(val) if averaged else jnp.sum(val)\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.chi_squared_zero","title":"chi_squared_zero","text":"<pre><code>chi_squared_zero(**predictions: Kwargs) -&gt; Float\n</code></pre> <p>Computes a calibration metric for a given set of predictions.</p> <p>The calculated metric is the ratio between the error of the prediction and the variance of the output uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>**predictions</code> <code>Kwargs</code> <p>Keyword arguments representing the model predictions, typically including mean, variance, and target.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The calibration metric value.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def chi_squared_zero(**predictions: Kwargs) -&gt; Float:\n    r\"\"\"Computes a calibration metric for a given set of predictions.\n\n    The calculated metric is the ratio between the error of the prediction and\n    the variance of the output uncertainty.\n\n    Args:\n        **predictions: Keyword arguments representing the model predictions,\n            typically including mean, variance, and target.\n\n    Returns:\n        The calibration metric value.\n    \"\"\"\n    return jnp.abs(chi_squared(**predictions) - 1)\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.estimate_rmse","title":"estimate_rmse","text":"<pre><code>estimate_rmse(pred_mean: Array, target: Array, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>Estimate the root mean squared error (RMSE) for predictions.</p> <p>Mathematically:</p> \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}. \\] <p>Parameters:</p> Name Type Description Default <code>pred_mean</code> <code>Array</code> <p>Array of predicted means.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The RMSE value.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def estimate_rmse(pred_mean: Array, target: Array, **kwargs: Kwargs) -&gt; Float:\n    r\"\"\"Estimate the root mean squared error (RMSE) for predictions.\n\n    Mathematically:\n\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.\n    $$\n\n    Args:\n        pred_mean: Array of predicted means.\n        target: Array of ground truth labels.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The RMSE value.\n    \"\"\"\n    del kwargs\n    return jnp.sqrt(jnp.mean(jnp.power(pred_mean - target, 2)))\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.crps_gaussian","title":"crps_gaussian","text":"<pre><code>crps_gaussian(pred_mean: Array, pred_std: Array, target: Array, *, scaled: bool = True, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>The negatively oriented continuous ranked probability score for Gaussians.</p> <p>Negatively oriented means a smaller value is more desirable.</p> <p>Parameters:</p> Name Type Description Default <code>pred_mean</code> <code>Array</code> <p>1D array of the predicted means for the held out dataset.</p> required <code>pred_std</code> <code>Array</code> <p>1D array of he predicted standard deviations for the held out dataset.</p> required <code>target</code> <code>Array</code> <p>1D array of the true labels in the held out dataset.</p> required <code>scaled</code> <code>bool</code> <p>Whether to scale the score by size of held out set.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The crps for the heldout set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>pred_mean, pred_std, and target have incompatible shapes.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def crps_gaussian(\n    pred_mean: Array,\n    pred_std: Array,\n    target: Array,\n    *,\n    scaled: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Float:\n    \"\"\"The negatively oriented continuous ranked probability score for Gaussians.\n\n    Negatively oriented means a smaller value is more desirable.\n\n    Args:\n        pred_mean: 1D array of the predicted means for the held out dataset.\n        pred_std: 1D array of he predicted standard deviations for the held out dataset.\n        target: 1D array of the true labels in the held out dataset.\n        scaled: Whether to scale the score by size of held out set.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The crps for the heldout set.\n\n    Raises:\n        ValueError: pred_mean, pred_std, and target have incompatible shapes.\n    \"\"\"\n    del kwargs\n\n    # Ensure input arrays are 1D and of the same shape\n    if not (pred_mean.shape == pred_std.shape == target.shape):\n        msg = \"arrays must have the same shape\"\n        raise ValueError(msg)\n\n    # Compute crps\n    pred_std_flat = pred_std.flatten()\n    pred_norm = (target.flatten() - pred_mean.flatten()) / pred_std_flat\n    term_1 = 1 / jnp.sqrt(jnp.pi)\n    term_2 = 2 * jax.scipy.stats.norm.pdf(pred_norm, loc=0, scale=1)\n    term_3 = pred_norm * (2 * jax.scipy.stats.norm.cdf(pred_norm, loc=0, scale=1) - 1)\n\n    crps_list = -1 * pred_std_flat * (term_1 - term_2 - term_3)\n    crps = jnp.sum(crps_list)\n\n    # Potentially scale so that sum becomes mean\n    if scaled:\n        crps = crps / len(crps_list)\n\n    return crps\n</code></pre>"},{"location":"reference/eval/metrics/#laplax.eval.metrics.nll_gaussian","title":"nll_gaussian","text":"<pre><code>nll_gaussian(pred_mean: Array, pred_std: Array, target: Array, *, scaled: bool = True, **kwargs: Kwargs) -&gt; Float\n</code></pre> <p>Compute the negative log-likelihood (NLL) for a Gaussian distribution.</p> <p>The NLL quantifies how well the predictive distribution fits the data, assuming a Gaussian distribution characterized by <code>pred</code> (mean) and <code>pred_std</code> (standard deviation).</p> <p>Mathematically:</p> \\[ \\text{NLL} = - \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp \\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right) \\right). \\] <p>Parameters:</p> Name Type Description Default <code>pred_mean</code> <code>Array</code> <p>Array of predicted means for the dataset.</p> required <code>pred_std</code> <code>Array</code> <p>Array of predicted standard deviations for the dataset.</p> required <code>target</code> <code>Array</code> <p>Array of ground truth labels for the dataset.</p> required <code>scaled</code> <code>bool</code> <p>Whether to normalize the NLL by the number of samples (default: True).</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float</code> <p>The computed NLL value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>pred_mean, pred_std, and target have incompatible shapes.</p> Source code in <code>laplax/eval/metrics.py</code> <pre><code>def nll_gaussian(\n    pred_mean: Array,\n    pred_std: Array,\n    target: Array,\n    *,\n    scaled: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Float:\n    r\"\"\"Compute the negative log-likelihood (NLL) for a Gaussian distribution.\n\n    The NLL quantifies how well the predictive distribution fits the data,\n    assuming a Gaussian distribution characterized by `pred` (mean) and `pred_std`\n    (standard deviation).\n\n    Mathematically:\n\n    $$\n    \\text{NLL} = - \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}}\n    \\exp \\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right) \\right).\n    $$\n\n    Args:\n        pred_mean: Array of predicted means for the dataset.\n        pred_std: Array of predicted standard deviations for the dataset.\n        target: Array of ground truth labels for the dataset.\n        scaled: Whether to normalize the NLL by the number of samples (default: True).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        The computed NLL value.\n\n    Raises:\n        ValueError: pred_mean, pred_std, and target have incompatible shapes.\n    \"\"\"\n    del kwargs\n\n    # Ensure input arrays are 1D and of the same shape\n    if not (pred_mean.shape == pred_std.shape == target.shape):\n        msg = \"arrays must have the same shape\"\n        raise ValueError(msg)\n\n    # Compute residuals\n    residuals = pred_mean - target\n\n    # Compute negative log likelihood\n    nll_list = jax.scipy.stats.norm.logpdf(residuals, scale=pred_std)\n    nll = -1 * jnp.sum(nll_list)\n\n    # Scale the result by the number of data points if `scaled` is True\n    if scaled:\n        nll /= math.prod(pred_mean.shape)\n\n    return nll\n</code></pre>"},{"location":"reference/eval/predictives/","title":"laplax.eval.predictives","text":""},{"location":"reference/eval/predictives/#laplax.eval.predictives.laplace_bridge","title":"laplace_bridge","text":"<pre><code>laplace_bridge(mean: Array, var: Array, *, use_correction: bool) -&gt; Array\n</code></pre> <p>Laplace bridge approximation.</p> <p>Returns:</p> Type Description <code>Array</code> <p>The predictive.</p> Source code in <code>laplax/eval/predictives.py</code> <pre><code>def laplace_bridge(\n    mean: jax.Array,\n    var: jax.Array,\n    *,\n    use_correction: bool,\n) -&gt; jax.Array:\n    \"\"\"Laplace bridge approximation.\n\n    Returns:\n        The predictive.\n    \"\"\"\n    num_classes = mean.shape[1]\n\n    if use_correction:\n        c = jnp.sum(var, axis=0) / math.sqrt(num_classes / 2)  # [...]\n        c_expanded = jnp.expand_dims(c, axis=0)  # [1, ...]\n        mean = mean / jnp.sqrt(c_expanded)  # [C, ...]\n        var = var / c_expanded  # [C, ...]\n\n    # Laplace bridge\n    sum_exp_neg_mean_p = jnp.sum(jnp.exp(-mean), axis=0)  # [...]\n    sum_exp_neg_mean_p_expanded = jnp.expand_dims(\n        sum_exp_neg_mean_p, axis=0\n    )  # [1, ...]\n    dirichlet_params = (\n        1\n        - 2 / num_classes\n        + jnp.exp(mean) * sum_exp_neg_mean_p_expanded / (num_classes**2)\n    ) / var  # [C, ...]\n\n    return dirichlet_predictive(dirichlet_params)\n</code></pre>"},{"location":"reference/eval/predictives/#laplax.eval.predictives.dirichlet_predictive","title":"dirichlet_predictive","text":"<pre><code>dirichlet_predictive(dirichlet_params: Array) -&gt; Array\n</code></pre> <p>Predictive mean of Dirichlet distributions.</p> <p>Returns:</p> Type Description <code>Array</code> <p>The predictive.</p> Source code in <code>laplax/eval/predictives.py</code> <pre><code>def dirichlet_predictive(dirichlet_params: jax.Array) -&gt; jax.Array:\n    \"\"\"Predictive mean of Dirichlet distributions.\n\n    Returns:\n        The predictive.\n    \"\"\"\n    predictive = dirichlet_params / jnp.sum(dirichlet_params)  # [C, ...]\n\n    return predictive\n</code></pre>"},{"location":"reference/eval/pushforward/","title":"laplax.eval.pushforward","text":"<p>Pushforward Functions for Weight Space Uncertainty.</p> <p>This module provides functions to propagate uncertainty in weight space to output uncertainty. It includes methods for ensemble-based Monte Carlo predictions and linearized approximations for uncertainty estimation, as well as to create the <code>posterior_gp_kernel</code>.</p>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_get_weight_sample","title":"set_get_weight_sample","text":"<pre><code>set_get_weight_sample(key: KeyType | None, mean_params: Params, scale_mv: Callable[[Array], Array], num_samples: int, **kwargs: Kwargs) -&gt; Callable[[int], Params]\n</code></pre> <p>Creates a function to sample weights from a Gaussian distribution.</p> <p>This function generates weight samples from a Gaussian distribution characterized by the mean and the scale matrix-vector product function. It supports precomputation of samples for efficiency and assumes a fixed number of required samples.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyType | None</code> <p>PRNG key for generating random samples.</p> required <code>mean_params</code> <code>Params</code> <p>Mean of the weight-space Gaussian distribution.</p> required <code>scale_mv</code> <code>Callable[[Array], Array]</code> <p>Function for the scale matrix-vector product.</p> required <code>num_samples</code> <code>int</code> <p>Number of weight samples to generate.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>set_get_weight_sample_precompute</code>: Controls whether samples are   precomputed.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[int], Params]</code> <p>A function that generates a specific weight sample by index.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_get_weight_sample(\n    key: KeyType | None,\n    mean_params: Params,\n    scale_mv: Callable[[Array], Array],\n    num_samples: int,\n    **kwargs: Kwargs,\n) -&gt; Callable[[int], Params]:\n    \"\"\"Creates a function to sample weights from a Gaussian distribution.\n\n    This function generates weight samples from a Gaussian distribution\n    characterized by the mean and the scale matrix-vector product function.\n    It supports precomputation of samples for efficiency and assumes a fixed\n    number of required samples.\n\n    Args:\n        key: PRNG key for generating random samples.\n        mean_params: Mean of the weight-space Gaussian distribution.\n        scale_mv: Function for the scale matrix-vector product.\n        num_samples: Number of weight samples to generate.\n        **kwargs: Additional arguments, including:\n\n            - `set_get_weight_sample_precompute`: Controls whether samples are\n              precomputed.\n\n    Returns:\n        A function that generates a specific weight sample by index.\n    \"\"\"\n    if key is None:\n        key = jax.random.key(0)\n\n    keys = jax.random.split(key, num_samples)\n\n    def get_weight_sample(idx):\n        return util.tree.normal_like(keys[idx], mean=mean_params, scale_mv=scale_mv)\n\n    return precompute_list(\n        get_weight_sample,\n        jnp.arange(num_samples),\n        precompute=kwargs.get(\n            \"set_get_weight_sample_precompute\", kwargs.get(\"precompute\")\n        ),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.get_dist_state","title":"get_dist_state","text":"<pre><code>get_dist_state(mean_params: Params, model_fn: ModelFn, posterior_state: PosteriorState, *, linearized: bool = False, num_samples: int = 0, key: KeyType | None = None, **kwargs: Kwargs) -&gt; DistState\n</code></pre> <p>Construct the distribution state for uncertainty propagation.</p> <p>The distribution state contains information needed to propagate uncertainty from the posterior over weights to predictions. It forms the state for both linearized and ensemble-based Monte Carlo approaches.</p> <p>Parameters:</p> Name Type Description Default <code>mean_params</code> <code>Params</code> <p>Mean of the posterior (model parameters).</p> required <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate.</p> required <code>posterior_state</code> <code>PosteriorState</code> <p>The posterior distribution state.</p> required <code>linearized</code> <code>bool</code> <p>Whether to consider a linearized approximation.</p> <code>False</code> <code>num_samples</code> <code>int</code> <p>Number of weight samples for Monte Carlo methods.</p> <code>0</code> <code>key</code> <code>KeyType | None</code> <p>PRNG key for generating random samples.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>set_get_weight_sample_precompute</code>.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>DistState</code> <p>A dictionary containing functions and parameters for uncertainty propagation.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def get_dist_state(\n    mean_params: Params,\n    model_fn: ModelFn,\n    posterior_state: PosteriorState,\n    *,\n    linearized: bool = False,\n    num_samples: int = 0,\n    key: KeyType | None = None,\n    **kwargs: Kwargs,\n) -&gt; DistState:\n    \"\"\"Construct the distribution state for uncertainty propagation.\n\n    The distribution state contains information needed to propagate uncertainty\n    from the posterior over weights to predictions. It forms the state for both\n    linearized and ensemble-based Monte Carlo approaches.\n\n    Args:\n        mean_params: Mean of the posterior (model parameters).\n        model_fn: The model function to evaluate.\n        posterior_state: The posterior distribution state.\n        linearized: Whether to consider a linearized approximation.\n        num_samples: Number of weight samples for Monte Carlo methods.\n        key: PRNG key for generating random samples.\n        **kwargs: Additional arguments, including:\n\n            - `set_get_weight_sample_precompute`.\n\n    Returns:\n        A dictionary containing functions and parameters for uncertainty propagation.\n    \"\"\"\n    dist_state = {\n        \"posterior_state\": posterior_state,\n        \"num_samples\": num_samples,\n    }\n\n    if linearized:\n        # Create pushforward functions\n        def pf_jvp(input: InputArray, vector: Params) -&gt; PredArray:\n            return jax.jvp(\n                lambda p: model_fn(input=input, params=p),\n                (mean_params,),\n                (vector,),\n            )[1]\n\n        def pf_vjp(input: InputArray, vector: PredArray) -&gt; Params:\n            out, vjp_fun = jax.vjp(\n                lambda p: model_fn(input=input, params=p), mean_params\n            )\n            return vjp_fun(vector.reshape(out.shape))\n\n        dist_state[\"vjp\"] = pf_vjp\n        dist_state[\"jvp\"] = pf_jvp\n\n    if num_samples &gt; 0:\n        weight_sample_mean = (\n            util.tree.zeros_like(mean_params) if linearized else mean_params\n        )\n\n        # Create weight sample function\n        get_weight_samples = set_get_weight_sample(\n            key,\n            mean_params=weight_sample_mean,\n            scale_mv=posterior_state.scale_mv(posterior_state.state),\n            num_samples=num_samples,\n            **kwargs,\n        )\n        dist_state[\"get_weight_samples\"] = get_weight_samples\n\n    return dist_state\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_setup","title":"nonlin_setup","text":"<pre><code>nonlin_setup(results: dict[str, Array], aux: dict[str, Any], input: InputArray, dist_state: DistState, **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Prepare ensemble-based Monte Carlo predictions.</p> <p>This function generates predictions for multiple weight samples and stores them in the auxiliary dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data, including the model function.</p> required <code>input</code> <code>InputArray</code> <p>Input data for prediction.</p> required <code>dist_state</code> <code>DistState</code> <p>Distribution state containing weight sampling functions.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>nonlin_setup_batch_size</code>: Controls batch size for computing predictions.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_setup(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    input: InputArray,\n    dist_state: DistState,\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Prepare ensemble-based Monte Carlo predictions.\n\n    This function generates predictions for multiple weight samples and stores\n    them in the auxiliary dictionary.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data, including the model function.\n        input: Input data for prediction.\n        dist_state: Distribution state containing weight sampling functions.\n        **kwargs: Additional arguments, including:\n\n            - `nonlin_setup_batch_size`: Controls batch size for computing predictions.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n\n    def compute_pred_ptw(idx: int) -&gt; PredArray:\n        weight_sample = dist_state[\"get_weight_samples\"](idx)\n        return aux[\"model_fn\"](input=input, params=weight_sample)\n\n    aux[\"pred_ensemble\"] = jax.lax.map(\n        compute_pred_ptw,\n        jnp.arange(dist_state[\"num_samples\"]),\n        batch_size=kwargs.get(\n            \"nonlin_setup_batch_size\", kwargs.get(\"weight_batch_size\")\n        ),\n    )\n\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_pred_mean","title":"nonlin_pred_mean","text":"<pre><code>nonlin_pred_mean(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute the mean of ensemble predictions.</p> <p>This function calculates the mean of prediction ensemble generated from multiple weight samples in an ensemble-based Monte Carlo approach.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the prediction ensemble.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_pred_mean(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute the mean of ensemble predictions.\n\n    This function calculates the mean of prediction ensemble generated from\n    multiple weight samples in an ensemble-based Monte Carlo approach.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the prediction ensemble.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    del kwargs\n\n    pred_ensemble = aux[\"pred_ensemble\"]\n    results[\"pred_mean\"] = util.tree.mean(pred_ensemble, axis=0)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_pred_cov","title":"nonlin_pred_cov","text":"<pre><code>nonlin_pred_cov(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute the covariance of ensemble predictions.</p> <p>This function calculates the empirical covariance of the ensemble of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the prediction ensemble.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_pred_cov(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute the covariance of ensemble predictions.\n\n    This function calculates the empirical covariance of the ensemble of predictions.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the prediction ensemble.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    del kwargs\n\n    pred_ensemble = aux[\"pred_ensemble\"]\n\n    results[\"pred_cov\"] = util.tree.cov(\n        pred_ensemble.reshape(pred_ensemble.shape[0], -1), rowvar=False\n    )\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_pred_var","title":"nonlin_pred_var","text":"<pre><code>nonlin_pred_var(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute the variance of ensemble predictions.</p> <p>This function calculates the empirical variance of the ensemble of predictions. If the covariance is already available, it extracts the diagonal.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the prediction ensemble.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_pred_var(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute the variance of ensemble predictions.\n\n    This function calculates the empirical variance of the ensemble of predictions.\n    If the covariance is already available, it extracts the diagonal.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the prediction ensemble.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    del kwargs\n\n    if \"pred_cov\" in results:\n        pred_cov = results[\"pred_cov\"]\n        pred_var = (\n            jnp.diagonal(pred_cov)\n            if pred_cov.ndim &gt;= 2\n            else jnp.reshape(pred_cov, (-1,))\n        )\n        pred_var = pred_var.reshape(aux[\"pred_ensemble\"].shape[1:])\n        results[\"pred_var\"] = pred_var\n    else:\n        pred_ensemble = aux[\"pred_ensemble\"]\n        results[\"pred_var\"] = util.tree.var(pred_ensemble, axis=0)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_pred_std","title":"nonlin_pred_std","text":"<pre><code>nonlin_pred_std(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute the standard deviation of ensemble predictions.</p> <p>This function calculates the empirical standard deviation of the ensemble of predictions. If the variance is already available, then it takes the square root.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the prediction ensemble.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_pred_std(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute the standard deviation of ensemble predictions.\n\n    This function calculates the empirical standard deviation of the ensemble of\n    predictions. If the variance is already available, then it takes the square root.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the prediction ensemble.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    del kwargs\n\n    if \"pred_var\" in results:\n        results[\"pred_std\"] = jnp.sqrt(results[\"pred_var\"])\n    else:\n        pred_ensemble = aux[\"pred_ensemble\"]\n        results[\"pred_std\"] = util.tree.std(pred_ensemble, axis=0)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_samples","title":"nonlin_samples","text":"<pre><code>nonlin_samples(results: dict[str, Array], aux: dict[str, Any], num_samples: int = 5, **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Select samples from ensemble.</p> <p>This function selects a subset of samples from the ensemble of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the prediction ensemble.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to select.</p> <code>5</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_samples(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    num_samples: int = 5,\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Select samples from ensemble.\n\n    This function selects a subset of samples from the ensemble of predictions.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the prediction ensemble.\n        num_samples: Number of samples to select.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    del kwargs\n\n    pred_ensemble = aux[\"pred_ensemble\"]\n    results[\"samples\"] = util.tree.tree_slice(pred_ensemble, 0, num_samples)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_special_pred_act","title":"nonlin_special_pred_act","text":"<pre><code>nonlin_special_pred_act(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Apply special predictive methods to nonlinear Laplace for classification.</p> <p>This function applies special predictive methods (Laplace Bridge, Mean Field-0, Mean Field-1, or Mean Field-2) to nonlinear Laplace for classification. These methods transform the predictions into probability space using specific formulations rather than Monte Carlo sampling.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing prediction information.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>special_pred_type</code>: Type of special prediction (\"laplace_bridge\",   \"mean_field_0\", \"mean_field_1\", or \"mean_field_2\")</li> <li><code>use_correction</code>: Whether to apply correction term for applicable methods.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_special_pred_act(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Apply special predictive methods to nonlinear Laplace for classification.\n\n    This function applies special predictive methods (Laplace Bridge, Mean Field-0,\n    Mean Field-1, or Mean Field-2) to nonlinear Laplace for classification. These\n    methods transform the predictions into probability space using specific formulations\n    rather than Monte Carlo sampling.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing prediction information.\n        **kwargs: Additional arguments, including:\n\n            - `special_pred_type`: Type of special prediction (\"laplace_bridge\",\n              \"mean_field_0\", \"mean_field_1\", or \"mean_field_2\")\n            - `use_correction`: Whether to apply correction term for applicable methods.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    return special_pred_act(results, aux, linearized=False, **kwargs)\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.nonlin_mc_pred_act","title":"nonlin_mc_pred_act","text":"<pre><code>nonlin_mc_pred_act(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute Monte Carlo predictions for nonlinear Laplace classification.</p> <p>This function generates Monte Carlo predictions for classification by averaging softmax probabilities across different weight samples. If samples are not already available, it generates them first.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing prediction information.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to sample generation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def nonlin_mc_pred_act(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute Monte Carlo predictions for nonlinear Laplace classification.\n\n    This function generates Monte Carlo predictions for classification by averaging\n    softmax probabilities across different weight samples. If samples are not already\n    available, it generates them first.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing prediction information.\n        **kwargs: Additional arguments passed to sample generation.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    if \"samples\" not in results:\n        results, aux = nonlin_samples(results=results, aux=aux, **kwargs)\n\n    results[\"mc_pred_act\"] = jnp.mean(\n        jax.nn.softmax(results[\"samples\"], axis=1), axis=0\n    )\n\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_output_mv","title":"set_output_mv","text":"<pre><code>set_output_mv(posterior_state: Posterior, input: InputArray, jvp: Callable[[InputArray, Params], PredArray], vjp: Callable[[InputArray, PredArray], Params]) -&gt; dict\n</code></pre> <p>Create matrix-vector product functions for output covariance and scale.</p> <p>This function propagates uncertainty from weight space to output space by constructing matrix-vector product functions for the output covariance and scale matrices. These functions utilize the posterior's covariance and scale operators in conjunction with Jacobian-vector products (JVP) and vector-Jacobian products (VJP).</p> <p>Parameters:</p> Name Type Description Default <code>posterior_state</code> <code>Posterior</code> <p>The posterior state containing covariance and scale operators.</p> required <code>input</code> <code>InputArray</code> <p>Input data for the model.</p> required <code>jvp</code> <code>Callable[[InputArray, Params], PredArray]</code> <p>Function for computing Jacobian-vector products.</p> required <code>vjp</code> <code>Callable[[InputArray, PredArray], Params]</code> <p>Function for computing vector-Jacobian products.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with:</p> <ul> <li><code>cov_mv</code>: Function for the output covariance matrix-vector product.</li> <li><code>jac_mv</code>: Function for the JVP with a fixed input.</li> </ul> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_output_mv(\n    posterior_state: Posterior,\n    input: InputArray,\n    jvp: Callable[[InputArray, Params], PredArray],\n    vjp: Callable[[InputArray, PredArray], Params],\n) -&gt; dict:\n    \"\"\"Create matrix-vector product functions for output covariance and scale.\n\n    This function propagates uncertainty from weight space to output space by\n    constructing matrix-vector product functions for the output covariance and\n    scale matrices. These functions utilize the posterior's covariance and scale\n    operators in conjunction with Jacobian-vector products (JVP) and\n    vector-Jacobian products (VJP).\n\n    Args:\n        posterior_state: The posterior state containing covariance and scale operators.\n        input: Input data for the model.\n        jvp: Function for computing Jacobian-vector products.\n        vjp: Function for computing vector-Jacobian products.\n\n    Returns:\n        A dictionary with:\n\n            - `cov_mv`: Function for the output covariance matrix-vector product.\n            - `jac_mv`: Function for the JVP with a fixed input.\n    \"\"\"\n    cov_mv = posterior_state.cov_mv(posterior_state.state)\n\n    def output_cov_mv(vec: PredArray) -&gt; PredArray:\n        return jvp(input, cov_mv(vjp(input, vec)[0]))\n\n    def output_jac_mv(vec: PredArray) -&gt; PredArray:\n        return jvp(input, vec)\n\n    return {\"cov_mv\": output_cov_mv, \"jac_mv\": output_jac_mv}\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_setup","title":"lin_setup","text":"<pre><code>lin_setup(results: dict[str, Array], aux: dict[str, Any], input: InputArray, dist_state: DistState, **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Prepare linearized pushforward functions for uncertainty propagation.</p> <p>This function sets up matrix-vector product functions for the output covariance and scale matrices in a linearized pushforward framework. It verifies the validity of input components (posterior state, JVP, and VJP) and stores the resulting functions in the auxiliary dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data to store matrix-vector product functions.</p> required <code>input</code> <code>InputArray</code> <p>Input data for the model.</p> required <code>dist_state</code> <code>DistState</code> <p>Distribution state containing posterior state, JVP, and VJP functions.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When the posterior_state, vjp, or jvp has an incorrect type.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_setup(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    input: InputArray,\n    dist_state: DistState,\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Prepare linearized pushforward functions for uncertainty propagation.\n\n    This function sets up matrix-vector product functions for the output covariance\n    and scale matrices in a linearized pushforward framework. It verifies the\n    validity of input components (posterior state, JVP, and VJP) and stores the\n    resulting functions in the auxiliary dictionary.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data to store matrix-vector product functions.\n        input: Input data for the model.\n        dist_state: Distribution state containing posterior state, JVP, and VJP\n            functions.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n\n    Raises:\n        TypeError: When the posterior_state, vjp, or jvp has an incorrect type.\n    \"\"\"\n    del kwargs\n\n    jvp = dist_state[\"jvp\"]\n    vjp = dist_state[\"vjp\"]\n    posterior_state = dist_state[\"posterior_state\"]\n\n    # Check types (mainly needed for type checker)\n    if not isinstance(posterior_state, Posterior):\n        msg = \"posterior state is not a Posterior type\"\n        raise TypeError(msg)\n\n    if not isinstance(jvp, Callable):\n        msg = \"JVP is not a JVPType\"\n        raise TypeError(msg)\n\n    if not isinstance(vjp, Callable):\n        msg = \"VJP is not a VJPType\"\n        raise TypeError(msg)\n\n    mv = set_output_mv(posterior_state, input, jvp, vjp)\n    aux[\"cov_mv\"] = mv[\"cov_mv\"]\n    aux[\"jac_mv\"] = mv[\"jac_mv\"]\n\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_pred_mean","title":"lin_pred_mean","text":"<pre><code>lin_pred_mean(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Restore the linearized predictions.</p> <p>This function extracts the prediction from the results dictionary and stores it.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data (ignored).</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Note <p>This function is used for the linearized mean prediction.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_pred_mean(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Restore the linearized predictions.\n\n    This function extracts the prediction from the results dictionary and\n    stores it.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data (ignored).\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n\n    Note:\n        This function is used for the linearized mean prediction.\n    \"\"\"\n    del kwargs\n\n    results[\"pred_mean\"] = results[\"map\"]\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_pred_var","title":"lin_pred_var","text":"<pre><code>lin_pred_var(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute and store the variance of the linearized predictions.</p> <p>This function calculates the variance of predictions by extracting the diagonal of the output covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary containing computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data, including covariance matrix functions.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_pred_var(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute and store the variance of the linearized predictions.\n\n    This function calculates the variance of predictions by extracting the diagonal\n    of the output covariance matrix.\n\n    Args:\n        results: Dictionary containing computed results.\n        aux: Auxiliary data, including covariance matrix functions.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    cov = results.get(\"pred_cov\", aux[\"cov_mv\"])\n\n    if \"pred_mean\" not in results:\n        results, aux = lin_pred_mean(results, aux, **kwargs)\n\n    pred_mean = results[\"pred_mean\"]\n\n    # Compute diagonal as variance and match output shape\n    pred_var = util.mv.diagonal(\n        cov,\n        layout=math.prod(pred_mean.shape),\n        mv_jittable=kwargs.get(\"mv_jittable\", True),\n    )\n    results[\"pred_var\"] = pred_var.reshape(pred_mean.shape)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_pred_std","title":"lin_pred_std","text":"<pre><code>lin_pred_std(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute and store the standard deviation of the linearized predictions.</p> <p>This function calculates the standard deviation by taking the square root of the predicted variance.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary containing computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data (ignored).</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_pred_std(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute and store the standard deviation of the linearized predictions.\n\n    This function calculates the standard deviation by taking the square root\n    of the predicted variance.\n\n    Args:\n        results: Dictionary containing computed results.\n        aux: Auxiliary data (ignored).\n        **kwargs: Additional arguments.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    if \"pred_var\" not in results:  # Fall back to `lin_pred_var`\n        results, aux = lin_pred_var(results, aux, **kwargs)\n\n    var = results[\"pred_var\"]\n    results[\"pred_std\"] = util.tree.sqrt(var)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_pred_cov","title":"lin_pred_cov","text":"<pre><code>lin_pred_cov(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute and store the covariance of the linearized predictions.</p> <p>This function computes the full output covariance matrix in dense form using the covariance matrix-vector product function.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary containing computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing covariance matrix-vector product functions.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments (ignored).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_pred_cov(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute and store the covariance of the linearized predictions.\n\n    This function computes the full output covariance matrix in dense form\n    using the covariance matrix-vector product function.\n\n    Args:\n        results: Dictionary containing computed results.\n        aux: Auxiliary data containing covariance matrix-vector product functions.\n        **kwargs: Additional arguments (ignored).\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    if \"pred_mean\" not in results:\n        results, aux = lin_pred_mean(results, aux, **kwargs)\n\n    pred_mean = results[\"pred_mean\"]\n    cov_mv = aux[\"cov_mv\"]\n\n    results[\"pred_cov\"] = util.mv.to_dense(cov_mv, layout=pred_mean)\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_samples","title":"lin_samples","text":"<pre><code>lin_samples(results: dict[str, Array], aux: dict[str, Any], dist_state: DistState, **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Generate and store samples from the linearized distribution.</p> <p>This function computes samples in the output space by applying the scale matrix to weight samples generated from the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing the scale matrix function.</p> required <code>dist_state</code> <code>DistState</code> <p>Distribution state containing sampling functions and sample count.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>lin_samples_batch_size</code>: Batch size for computing samples.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_samples(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    dist_state: DistState,\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Generate and store samples from the linearized distribution.\n\n    This function computes samples in the output space by applying the scale\n    matrix to weight samples generated from the posterior distribution.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing the scale matrix function.\n        dist_state: Distribution state containing sampling functions and sample count.\n        **kwargs: Additional arguments, including:\n\n            - `lin_samples_batch_size`: Batch size for computing samples.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    if \"pred_mean\" not in results:\n        results, aux = lin_pred_mean(results, aux, **kwargs)\n\n    # Unpack arguments\n    jac_mv = aux[\"jac_mv\"]\n    get_weight_samples = dist_state[\"get_weight_samples\"]\n    num_samples = dist_state[\"num_samples\"]\n\n    # Compute samples\n    results[\"samples\"] = jax.lax.map(\n        lambda i: add(results[\"pred_mean\"], jac_mv(get_weight_samples(i))),\n        jnp.arange(num_samples),\n        batch_size=kwargs.get(\n            \"lin_samples_batch_size\", kwargs.get(\"weight_batch_size\")\n        ),\n    )\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_special_pred_act","title":"lin_special_pred_act","text":"<pre><code>lin_special_pred_act(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Apply special predictive methods to linearized Laplace for classification.</p> <p>This function applies special predictive methods (Laplace Bridge, Mean Field-0, Mean Field-1, or Mean Field-2) to linearized Laplace for classification. These methods transform the predictions into probability space using specific formulations rather than Monte Carlo sampling.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing prediction information.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>special_pred_type</code>: Type of special prediction (\"laplace_bridge\",     \"mean_field_0\", \"mean_field_1\", or \"mean_field_2\")</li> <li><code>use_correction</code>: Whether to apply correction term for applicable methods.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_special_pred_act(\n    results: dict[str, Array],\n    aux: dict[str, Any],\n    **kwargs: Kwargs,\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Apply special predictive methods to linearized Laplace for classification.\n\n    This function applies special predictive methods (Laplace Bridge, Mean Field-0,\n    Mean Field-1, or Mean Field-2) to linearized Laplace for classification. These\n    methods transform the predictions into probability space using specific formulations\n    rather than Monte Carlo sampling.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing prediction information.\n        **kwargs: Additional arguments, including:\n\n            - `special_pred_type`: Type of special prediction (\"laplace_bridge\",\n                \"mean_field_0\", \"mean_field_1\", or \"mean_field_2\")\n            - `use_correction`: Whether to apply correction term for applicable methods.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    return special_pred_act(results, aux, linearized=True, **kwargs)\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.lin_mc_pred_act","title":"lin_mc_pred_act","text":"<pre><code>lin_mc_pred_act(results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs) -&gt; tuple[dict[str, Array], dict[str, Any]]\n</code></pre> <p>Compute Monte Carlo predictions for linear Laplace classification.</p> <p>This function generates Monte Carlo predictions for classification by averaging softmax probabilities across different weight samples. If samples are not already available, it generates them first.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Array]</code> <p>Dictionary to store computed results.</p> required <code>aux</code> <code>dict[str, Any]</code> <p>Auxiliary data containing prediction information.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to sample generation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Array], dict[str, Any]]</code> <p>Updated <code>results</code> and <code>aux</code>.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def lin_mc_pred_act(\n    results: dict[str, Array], aux: dict[str, Any], **kwargs: Kwargs\n) -&gt; tuple[dict[str, Array], dict[str, Any]]:\n    \"\"\"Compute Monte Carlo predictions for linear Laplace classification.\n\n    This function generates Monte Carlo predictions for classification by averaging\n    softmax probabilities across different weight samples. If samples are not already\n    available, it generates them first.\n\n    Args:\n        results: Dictionary to store computed results.\n        aux: Auxiliary data containing prediction information.\n        **kwargs: Additional arguments passed to sample generation.\n\n    Returns:\n        Updated `results` and `aux`.\n    \"\"\"\n    if \"samples\" not in results:\n        results, aux = lin_samples(results=results, aux=aux, **kwargs)\n\n    results[\"mc_pred_act\"] = jnp.mean(\n        jax.nn.softmax(results[\"samples\"], axis=1), axis=0\n    )\n\n    return results, aux\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_prob_predictive","title":"set_prob_predictive","text":"<pre><code>set_prob_predictive(model_fn: ModelFn, mean_params: Params, dist_state: DistState, pushforward_fns: list[Callable], **kwargs: Kwargs) -&gt; Callable[[InputArray], dict[str, Array]]\n</code></pre> <p>Create a probabilistic predictive function.</p> <p>This function generates a predictive callable that computes uncertainty-aware predictions using a set of pushforward functions. The generated function can evaluate mean predictions and propagate uncertainty from the posterior over weights to output space.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate, which takes input and parameters.</p> required <code>mean_params</code> <code>Params</code> <p>The mean of the posterior distribution over model parameters.</p> required <code>dist_state</code> <code>DistState</code> <p>The distribution state for uncertainty propagation, containing functions and parameters related to the posterior.</p> required <code>pushforward_fns</code> <code>list[Callable]</code> <p>A list of pushforward functions, such as mean, variance, and covariance.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to the pushforward functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[InputArray], dict[str, Array]]</code> <p>A function that takes an input array and returns a dictionary of predictions and uncertainty metrics.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_prob_predictive(\n    model_fn: ModelFn,\n    mean_params: Params,\n    dist_state: DistState,\n    pushforward_fns: list[Callable],\n    **kwargs: Kwargs,\n) -&gt; Callable[[InputArray], dict[str, Array]]:\n    \"\"\"Create a probabilistic predictive function.\n\n    This function generates a predictive callable that computes uncertainty-aware\n    predictions using a set of pushforward functions. The generated function can\n    evaluate mean predictions and propagate uncertainty from the posterior over\n    weights to output space.\n\n    Args:\n        model_fn: The model function to evaluate, which takes input and parameters.\n        mean_params: The mean of the posterior distribution over model parameters.\n        dist_state: The distribution state for uncertainty propagation, containing\n            functions and parameters related to the posterior.\n        pushforward_fns: A list of pushforward functions, such as mean, variance, and\n            covariance.\n        **kwargs: Additional arguments passed to the pushforward functions.\n\n    Returns:\n        A function that takes an input array and returns a dictionary\n            of predictions and uncertainty metrics.\n    \"\"\"\n\n    def prob_predictive(input: InputArray) -&gt; dict[str, Array]:\n        # MAP prediction\n        pred_map = model_fn(input=input, params=mean_params)\n        aux = {\"model_fn\": model_fn, \"mean_params\": mean_params}\n        results = {\"map\": pred_map}\n\n        # Compute prediction\n        return finalize_fns(\n            fns=pushforward_fns,\n            results=results,\n            dist_state=dist_state,\n            aux=aux,\n            input=input,\n            **kwargs,\n        )\n\n    return prob_predictive\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_nonlin_pushforward","title":"set_nonlin_pushforward","text":"<pre><code>set_nonlin_pushforward(model_fn: ModelFn, mean_params: Params, posterior_fn: Callable[[PriorArguments, Int], Posterior], prior_arguments: PriorArguments, *, key: KeyType, loss_scaling_factor: Float = 1.0, pushforward_fns: list = DEFAULT_NONLIN_FINALIZE_FNS, num_samples: int = 100, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Construct a Monte Carlo pushforward predictive function.</p> <p>This function creates a probabilistic predictive callable that computes ensemble-based Monte Carlo (MC) predictions and propagates uncertainty from weight space to output space using sampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate, which takes input and parameters.</p> required <code>mean_params</code> <code>Params</code> <p>The mean of the posterior distribution over model parameters.</p> required <code>posterior_fn</code> <code>Callable[[PriorArguments, Int], Posterior]</code> <p>A callable that generates the posterior state from prior arguments.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Arguments for defining the prior distribution.</p> required <code>key</code> <code>KeyType</code> <p>PRNG key for generating random samples.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <code>pushforward_fns</code> <code>list</code> <p>A list of Monte Carlo pushforward functions (default: <code>DEFAULT_MC_FUNCTIONS</code>).</p> <code>DEFAULT_NONLIN_FINALIZE_FNS</code> <code>num_samples</code> <code>int</code> <p>Number of weight samples for Monte Carlo predictions.</p> <code>100</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to the pushforward functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A probabilistic predictive function that computes predictions and uncertainty metrics using Monte Carlo sampling.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_nonlin_pushforward(\n    model_fn: ModelFn,\n    mean_params: Params,\n    posterior_fn: Callable[[PriorArguments, Int], Posterior],\n    prior_arguments: PriorArguments,\n    *,\n    key: KeyType,\n    loss_scaling_factor: Float = 1.0,\n    pushforward_fns: list = DEFAULT_NONLIN_FINALIZE_FNS,\n    num_samples: int = 100,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Construct a Monte Carlo pushforward predictive function.\n\n    This function creates a probabilistic predictive callable that computes\n    ensemble-based Monte Carlo (MC) predictions and propagates uncertainty\n    from weight space to output space using sampling.\n\n    Args:\n        model_fn: The model function to evaluate, which takes input and parameters.\n        mean_params: The mean of the posterior distribution over model parameters.\n        posterior_fn: A callable that generates the posterior state from prior\n            arguments.\n        prior_arguments: Arguments for defining the prior distribution.\n        key: PRNG key for generating random samples.\n        loss_scaling_factor: Factor by which the user-provided loss function is scaled.\n            Defaults to 1.0.\n        pushforward_fns: A list of Monte Carlo pushforward functions\n            (default: `DEFAULT_MC_FUNCTIONS`).\n        num_samples: Number of weight samples for Monte Carlo predictions.\n        **kwargs: Additional arguments passed to the pushforward functions.\n\n    Returns:\n        A probabilistic predictive function that computes predictions\n            and uncertainty metrics using Monte Carlo sampling.\n    \"\"\"\n    # Create weight sample function\n    posterior_state = posterior_fn(prior_arguments, loss_scaling_factor)\n\n    # Posterior state to dist_state\n    dist_state = get_dist_state(\n        mean_params,\n        model_fn,\n        posterior_state,\n        linearized=False,\n        num_samples=num_samples,\n        key=key,\n        **kwargs,\n    )\n\n    # Set prob predictive\n    prob_predictive = set_prob_predictive(\n        model_fn=model_fn,\n        mean_params=mean_params,\n        dist_state=dist_state,\n        pushforward_fns=pushforward_fns,\n        **kwargs,\n    )\n\n    return prob_predictive\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_lin_pushforward","title":"set_lin_pushforward","text":"<pre><code>set_lin_pushforward(model_fn: ModelFn, mean_params: Params, posterior_fn: Callable[[PriorArguments, Int], Posterior], prior_arguments: PriorArguments, loss_scaling_factor: Float = 1.0, pushforward_fns: list = DEFAULT_LIN_FINALIZE_FNS, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Construct a linearized pushforward predictive function.</p> <p>This function generates a probabilistic predictive callable that computes predictions and propagates uncertainty using a linearized approximation of the model function.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate, which takes input and parameters.</p> required <code>mean_params</code> <code>Params</code> <p>The mean of the posterior distribution over model parameters.</p> required <code>posterior_fn</code> <code>Callable[[PriorArguments, Int], Posterior]</code> <p>A callable that generates the posterior state from prior arguments.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Arguments for defining the prior distribution.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <code>pushforward_fns</code> <code>list</code> <p>A list of linearized pushforward functions (default: <code>DEFAULT_LIN_FINALIZE</code>).</p> <code>DEFAULT_LIN_FINALIZE_FNS</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to the pushforward functions, including:</p> <ul> <li><code>n_samples</code>: Number of samples for approximating uncertainty metrics.</li> <li><code>key</code>: PRNG key for generating random samples.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A probabilistic predictive function that computes predictions and uncertainty metrics using a linearized approximation.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_lin_pushforward(\n    model_fn: ModelFn,\n    mean_params: Params,\n    posterior_fn: Callable[[PriorArguments, Int], Posterior],\n    prior_arguments: PriorArguments,\n    loss_scaling_factor: Float = 1.0,\n    pushforward_fns: list = DEFAULT_LIN_FINALIZE_FNS,\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Construct a linearized pushforward predictive function.\n\n    This function generates a probabilistic predictive callable that computes\n    predictions and propagates uncertainty using a linearized approximation of\n    the model function.\n\n    Args:\n        model_fn: The model function to evaluate, which takes input and parameters.\n        mean_params: The mean of the posterior distribution over model parameters.\n        posterior_fn: A callable that generates the posterior state from prior\n            arguments.\n        prior_arguments: Arguments for defining the prior distribution.\n        loss_scaling_factor: Factor by which the user-provided loss function is scaled.\n            Defaults to 1.0.\n        pushforward_fns: A list of linearized pushforward functions\n            (default: `DEFAULT_LIN_FINALIZE`).\n        **kwargs: Additional arguments passed to the pushforward functions, including:\n\n            - `n_samples`: Number of samples for approximating uncertainty metrics.\n            - `key`: PRNG key for generating random samples.\n\n    Returns:\n        A probabilistic predictive function that computes predictions\n            and uncertainty metrics using a linearized approximation.\n    \"\"\"\n    # Create posterior state\n    posterior_state = posterior_fn(prior_arguments, loss_scaling_factor)\n\n    # Posterior state to dist_state\n    dist_state = get_dist_state(\n        mean_params,\n        model_fn,\n        posterior_state,\n        linearized=True,\n        **kwargs,\n    )\n\n    # Set prob predictive\n    prob_predictive = set_prob_predictive(\n        model_fn=model_fn,\n        mean_params=mean_params,\n        dist_state=dist_state,\n        pushforward_fns=pushforward_fns,\n        **kwargs,\n    )\n\n    return prob_predictive\n</code></pre>"},{"location":"reference/eval/pushforward/#laplax.eval.pushforward.set_posterior_gp_kernel","title":"set_posterior_gp_kernel","text":"<pre><code>set_posterior_gp_kernel(model_fn: ModelFn, mean: Params, posterior_fn: Callable[[PriorArguments, Int], Posterior], prior_arguments: PriorArguments, loss_scaling_factor: Float = 1.0, **kwargs: Kwargs) -&gt; tuple[Callable, DistState]\n</code></pre> <p>Construct a kernel matrix-vector product function for a posterior GP.</p> <p>This function generates a callable for the kernel matrix-vector product (MVP) in a posterior GP framework. The kernel MVP is constructed using the posterior state and propagates uncertainty in weight space to output space via linearization. The resulting kernel MVP can optionally return a dense matrix representation.</p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>ModelFn</code> <p>The model function to evaluate, which takes input and parameters.</p> required <code>mean</code> <code>Params</code> <p>The mean of the posterior distribution over model parameters.</p> required <code>posterior_fn</code> <code>Callable[[PriorArguments, Int], Posterior]</code> <p>A callable that generates the posterior state from prior arguments.</p> required <code>prior_arguments</code> <code>PriorArguments</code> <p>Arguments for defining the prior distribution.</p> required <code>loss_scaling_factor</code> <code>Float</code> <p>Factor by which the user-provided loss function is scaled. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>dense</code>: Whether to return a dense kernel matrix instead of the MVP.</li> <li><code>output_layout</code>: The layout of the dense kernel matrix (required if     <code>dense</code> is True).</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Callable, DistState]</code> <p>A kernel MVP callable or a dense kernel matrix function, and the distribution state containing posterior information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>dense</code> is True but <code>output_layout</code> is not specified.</p> Source code in <code>laplax/eval/pushforward.py</code> <pre><code>def set_posterior_gp_kernel(\n    model_fn: ModelFn,\n    mean: Params,\n    posterior_fn: Callable[[PriorArguments, Int], Posterior],\n    prior_arguments: PriorArguments,\n    loss_scaling_factor: Float = 1.0,\n    **kwargs: Kwargs,\n) -&gt; tuple[Callable, DistState]:\n    \"\"\"Construct a kernel matrix-vector product function for a posterior GP.\n\n    This function generates a callable for the kernel matrix-vector product (MVP)\n    in a posterior GP framework. The kernel MVP is constructed using the posterior\n    state and propagates uncertainty in weight space to output space via linearization.\n    The resulting kernel MVP can optionally return a dense matrix representation.\n\n    Args:\n        model_fn: The model function to evaluate, which takes input and parameters.\n        mean: The mean of the posterior distribution over model parameters.\n        posterior_fn: A callable that generates the posterior state from prior\n            arguments.\n        prior_arguments: Arguments for defining the prior distribution.\n        loss_scaling_factor: Factor by which the user-provided loss function is scaled.\n            Defaults to 1.0.\n        **kwargs: Additional arguments, including:\n\n            - `dense`: Whether to return a dense kernel matrix instead of the MVP.\n            - `output_layout`: The layout of the dense kernel matrix (required if\n                `dense` is True).\n\n    Returns:\n        A kernel MVP callable or a dense kernel matrix function, and the\n            distribution state containing posterior information.\n\n    Raises:\n        ValueError: If `dense` is True but `output_layout` is not specified.\n    \"\"\"\n    # Create posterior state\n    posterior_state = posterior_fn(prior_arguments, loss_scaling_factor)\n\n    # Posterior state to dist_state\n    dist_state = get_dist_state(\n        mean,\n        model_fn,\n        posterior_state,\n        linearized=True,\n        num_samples=0,\n        **kwargs,\n    )\n\n    # Kernel mv\n    def kernel_mv(\n        vec: PredArray, x1: InputArray, x2: InputArray, dist_state: dict[str, Any]\n    ) -&gt; PredArray:\n        cov_mv = dist_state[\"posterior_state\"].cov_mv(\n            dist_state[\"posterior_state\"].state\n        )\n        return dist_state[\"jvp\"](x1, cov_mv(dist_state[\"vjp\"](x2, vec)[0]))\n\n    if kwargs.get(\"dense\"):\n        output_layout = kwargs.get(\"output_layout\")\n        if output_layout:\n            return lambda x1, x2: util.mv.to_dense(\n                lambda v: kernel_mv(v, x1, x2, dist_state), layout=output_layout\n            ), dist_state\n        msg = \"function should return a dense matrix, but no output layout is specified\"\n        raise ValueError(msg)\n\n    return kernel_mv, dist_state\n</code></pre>"},{"location":"reference/eval/utils/","title":"laplax.eval.utils","text":"<p>Pushforward utilities for evaluating probabilistic predictions on datasets.</p> <p>This module provides utilities for evaluating probabilistic models on datasets and managing metric computations.</p> <p>Key features include:</p> <ul> <li>Wrapping functions to store outputs in a structured format.</li> <li>Finalizing multiple functions and collecting results in a dictionary.</li> <li>Applying prediction functions across datasets to generate predictions and evaluating   them against their targets.</li> <li>Computing and transforming evaluation metrics for datasets using custom or default   metrics.</li> </ul> <p>These utilities streamline dataset evaluation workflows and ensure flexibility in metric computation and result aggregation.</p>"},{"location":"reference/eval/utils/#laplax.eval.utils.finalize_fns","title":"finalize_fns","text":"<pre><code>finalize_fns(fns: list[Callable], results: dict, aux: dict[str, Any] | None = None, **kwargs: Kwargs) -&gt; dict\n</code></pre> <p>Execute a set of functions and store their results in a dictionary.</p> <p>This function iterates over a list of functions, executes each function with the provided keyword arguments, and updates the <code>results</code> dictionary with their outputs. The functions know what key they should update the results dict with.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[Callable]</code> <p>A list of callables to execute.</p> required <code>results</code> <code>dict</code> <p>A dictionary to store the outputs of the functions.</p> required <code>aux</code> <code>dict[str, Any] | None</code> <p>Auxiliary data passed to the functions.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments passed to each function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated <code>results</code> dictionary containing the outputs of all executed functions.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def finalize_fns(\n    fns: list[Callable],\n    results: dict,  # Typing must allow empty dict for initializations\n    aux: dict[str, Any] | None = None,\n    **kwargs: Kwargs,\n) -&gt; dict:\n    \"\"\"Execute a set of functions and store their results in a dictionary.\n\n    This function iterates over a list of functions, executes each\n    function with the provided keyword arguments, and updates the `results`\n    dictionary with their outputs. The functions know what key they should update the\n    results dict with.\n\n    Args:\n        fns: A list of callables to execute.\n        results: A dictionary to store the outputs of the functions.\n        aux: Auxiliary data passed to the functions.\n        **kwargs: Additional arguments passed to each function.\n\n    Returns:\n        The updated `results` dictionary containing the outputs of all\n            executed functions.\n    \"\"\"\n    for func in fns:\n        results, aux = func(results=results, aux=aux, **kwargs)\n    return results\n</code></pre>"},{"location":"reference/eval/utils/#laplax.eval.utils.evaluate_on_dataset","title":"evaluate_on_dataset","text":"<pre><code>evaluate_on_dataset(pred_fn: Callable[[InputArray], dict[str, Array]], data: Data, **kwargs: Kwargs) -&gt; dict\n</code></pre> <p>Evaluate a prediction function on a dataset.</p> <p>This function applies a probabilistic predictive function (<code>pred_fn</code>) to each data point in the dataset, combining the predictions with the target labels.</p> <p>Parameters:</p> Name Type Description Default <code>pred_fn</code> <code>Callable[[InputArray], dict[str, Array]]</code> <p>A callable that takes an input array and returns predictions as a dictionary.</p> required <code>data</code> <code>Data</code> <p>A dataset, where each data point is a dictionary containing \"input\" and \"target\".</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>evaluate_on_dataset_batch_size</code>: Batch size for processing data   (default: <code>data_batch_size</code>).</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing predictions and target labels for the entire dataset.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def evaluate_on_dataset(\n    pred_fn: Callable[[InputArray], dict[str, Array]],\n    data: Data,\n    **kwargs: Kwargs,\n) -&gt; dict:\n    \"\"\"Evaluate a prediction function on a dataset.\n\n    This function applies a probabilistic predictive function (`pred_fn`) to\n    each data point in the dataset, combining the predictions with the target\n    labels.\n\n    Args:\n        pred_fn: A callable that takes an input array and returns predictions\n            as a dictionary.\n        data: A dataset, where each data point is a dictionary containing\n            \"input\" and \"target\".\n        **kwargs: Additional arguments, including:\n\n            - `evaluate_on_dataset_batch_size`: Batch size for processing data\n              (default: `data_batch_size`).\n\n    Returns:\n        A dictionary containing predictions and target labels for the entire dataset.\n    \"\"\"\n\n    def evaluate_data_point(dp: Data) -&gt; dict[str, Array]:\n        return {**pred_fn(dp[\"input\"]), \"target\": dp[\"target\"]}\n\n    return jax.lax.map(\n        evaluate_data_point,\n        data,\n        batch_size=kwargs.get(\n            \"evaluate_on_dataset_batch_size\", kwargs.get(\"data_batch_size\")\n        ),\n    )\n</code></pre>"},{"location":"reference/eval/utils/#laplax.eval.utils.apply_fns","title":"apply_fns","text":"<pre><code>apply_fns(*funcs: Callable, names: list[str] | None = None, field: str = 'results', **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Apply multiple functions and store their results in a dictionary.</p> <p>This function takes a sequence of functions, applies them to the provided inputs, and stores their results in either the 'results' or 'aux' dictionary under specified names. This function is useful for applying multiple metrics to the results of a pushforward function.</p> <p>Parameters:</p> Name Type Description Default <code>*funcs</code> <code>Callable</code> <p>Variable number of callable functions to be applied.</p> <code>()</code> <code>names</code> <code>list[str] | None</code> <p>Optional list of names for the functions' results. If None, function names will be used.</p> <code>None</code> <code>field</code> <code>str</code> <p>String indicating where to store results, either 'results' or 'aux' (default: 'results').</p> <code>'results'</code> <code>**kwargs</code> <code>Kwargs</code> <p>Mapping of argument names to keys in results/aux dictionaries that will be passed to the functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function that takes 'results' and 'aux' dictionaries along with additional kwargs, applies the functions, and returns the updated dictionaries.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the provided functions is not callable.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def apply_fns(\n    *funcs: Callable,\n    names: list[str] | None = None,\n    field: str = \"results\",\n    **kwargs: Kwargs,\n) -&gt; Callable:\n    \"\"\"Apply multiple functions and store their results in a dictionary.\n\n    This function takes a sequence of functions, applies them to the provided inputs,\n    and stores their results in either the 'results' or 'aux' dictionary under\n    specified names. This function is useful for applying multiple metrics to the\n    results of a pushforward function.\n\n    Args:\n        *funcs: Variable number of callable functions to be applied.\n        names: Optional list of names for the functions' results. If None,\n            function names will be used.\n        field: String indicating where to store results, either 'results' or 'aux'\n            (default: 'results').\n        **kwargs: Mapping of argument names to keys in results/aux dictionaries\n            that will be passed to the functions.\n\n    Returns:\n        A function that takes 'results' and 'aux' dictionaries along with\n            additional kwargs, applies the functions, and returns the updated\n            dictionaries.\n\n    Raises:\n        TypeError: If any of the provided functions is not callable.\n    \"\"\"\n    # Validate all funcs are callable\n    for i, func in enumerate(funcs):\n        if not callable(func):\n            msg = f\"Argument {i} is not callable. Type is {type(func)}\"\n            raise TypeError(msg)\n\n    def apply(results, aux, **local_kwargs):\n        # Create key-value pair for functions\n        key_value_pairs = {}\n        if kwargs:\n            for k, v in kwargs.items():\n                if v in results:\n                    key_value_pairs[k] = results[v]\n                elif v in aux:\n                    key_value_pairs[k] = aux[v]\n                else:\n                    msg = f\"Key {k} not found in results or aux.\"\n                    raise ValueError(msg)\n        else:\n            logger.warning(\"No kwargs provided, using aux dictionary as input\")\n            key_value_pairs = aux\n\n        # Ensure we have names for all functions\n        if names is None:\n            # Store under the function name\n            func_names = [func.__name__ for func in funcs]\n        else:\n            if len(names) != len(funcs):\n                msg = (\n                    f\"Number of names ({len(names)}) does not match number \"\n                    f\"of functions ({len(funcs)})\"\n                )\n                raise ValueError(msg)\n            func_names = names\n\n        # Apply each function and store results\n        for func, name in zip(funcs, func_names, strict=True):\n            res = func(**key_value_pairs, **local_kwargs)\n\n            if field == \"results\":\n                results[name] = res\n            elif field == \"aux\":\n                aux[name] = res\n            else:\n                msg = f\"Field {field} must be either 'results' or 'aux'.\"\n                raise ValueError(msg)\n\n        return results, aux\n\n    return apply\n</code></pre>"},{"location":"reference/eval/utils/#laplax.eval.utils.transfer_entry","title":"transfer_entry","text":"<pre><code>transfer_entry(mapping: dict[str, str] | list[str], field: str = 'results', access_from: str = 'aux') -&gt; Callable\n</code></pre> <p>Transfer entries between results and auxiliary dictionaries.</p> <p>This function creates a callable that copies values between the results and auxiliary dictionaries based on the provided mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict[str, str] | list[str]</code> <p>Either a dictionary mapping destination keys to source keys, or a list of keys to copy with the same names.</p> required <code>field</code> <code>str</code> <p>String indicating where to store entries, either 'results' or 'aux' (default: 'results').</p> <code>'results'</code> <code>access_from</code> <code>str</code> <p>String indicating which dictionary to read from, either 'results' or 'aux' (default: 'aux').</p> <code>'aux'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function that takes 'results' and 'aux' dictionaries, transfers the specified entries, and returns the updated dictionaries.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If field is not 'results' or 'aux'.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def transfer_entry(\n    mapping: dict[str, str] | list[str],\n    field: str = \"results\",\n    access_from: str = \"aux\",\n) -&gt; Callable:\n    \"\"\"Transfer entries between results and auxiliary dictionaries.\n\n    This function creates a callable that copies values between the results and\n    auxiliary dictionaries based on the provided mapping.\n\n    Args:\n        mapping: Either a dictionary mapping destination keys to source keys,\n            or a list of keys to copy with the same names.\n        field: String indicating where to store entries, either 'results' or 'aux'\n            (default: 'results').\n        access_from: String indicating which dictionary to read from, either\n            'results' or 'aux' (default: 'aux').\n\n    Returns:\n        A function that takes 'results' and 'aux' dictionaries,\n            transfers the specified entries, and returns the updated dictionaries.\n\n    Raises:\n        ValueError: If field is not 'results' or 'aux'.\n    \"\"\"\n    # Convert list to dict if needed\n    if isinstance(mapping, list):\n        mapping = {k: k for k in mapping}\n\n    # Check if field and access_from are valid\n    dict_options = (\"results\", \"aux\")\n    if field not in dict_options or access_from not in dict_options:\n        msg = f\"Field {field} must be either 'results' or 'aux'.\"\n        raise ValueError(msg)\n\n    # Transfer the entry\n    def transfer(results, aux, **kwargs):\n        del kwargs\n        options = {\"results\": results, \"aux\": aux}\n        for k, v in mapping.items():\n            options[field][k] = options[access_from][v]\n        return options[\"results\"], options[\"aux\"]\n\n    return transfer\n</code></pre>"},{"location":"reference/eval/utils/#laplax.eval.utils.evaluate_metrics_on_dataset","title":"evaluate_metrics_on_dataset","text":"<pre><code>evaluate_metrics_on_dataset(pred_fn: Callable[[InputArray], dict[str, Array]], data: Data, *, metrics: list | None = None, metrics_dict: dict[str, Callable] | None = None, reduce: Callable = identity, **kwargs: Kwargs) -&gt; dict\n</code></pre> <p>Evaluate a set of metrics on a dataset.</p> <p>This function computes specified metrics for predictions generated by a probabilistic predictive function (<code>pred_fn</code>) over a dataset. The results can optionally be transformed using an <code>apply</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>pred_fn</code> <code>Callable[[InputArray], dict[str, Array]]</code> <p>A callable that takes an input array and returns predictions as a dictionary.</p> required <code>data</code> <code>Data</code> <p>A dataset, where each data point is a dictionary containing \"input\" and \"target\".</p> required <code>metrics</code> <code>list | None</code> <p>A list of metrics to compute, this should use the <code>apply_fns</code> function to apply the metrics and <code>transfer_entry</code> function to transfer entries between results and auxiliary dictionaries.</p> <code>None</code> <code>metrics_dict</code> <code>dict[str, Callable] | None</code> <p>A dictionary of metrics to compute, where keys are metric names and values are callables.</p> <code>None</code> <code>reduce</code> <code>Callable</code> <p>A callable to transform the evaluated metrics (default: identity).</p> <code>identity</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional arguments, including:</p> <ul> <li><code>evaluate_metrics_on_dataset_batch_size</code>: Batch size for processing data   (default: <code>data_batch_size</code>).</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the evaluated metrics for the entire dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When metrics and metrics_dict are both None.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def evaluate_metrics_on_dataset(\n    pred_fn: Callable[[InputArray], dict[str, Array]],\n    data: Data,\n    *,\n    metrics: list | None = None,\n    metrics_dict: dict[str, Callable] | None = None,\n    reduce: Callable = identity,\n    **kwargs: Kwargs,\n) -&gt; dict:\n    \"\"\"Evaluate a set of metrics on a dataset.\n\n    This function computes specified metrics for predictions generated by a\n    probabilistic predictive function (`pred_fn`) over a dataset. The results\n    can optionally be transformed using an `apply` function.\n\n    Args:\n        pred_fn: A callable that takes an input array and returns predictions\n            as a dictionary.\n        data: A dataset, where each data point is a dictionary containing\n            \"input\" and \"target\".\n        metrics: A list of metrics to compute, this should use the `apply_fns`\n            function to apply the metrics and `transfer_entry` function to transfer\n            entries between results and auxiliary dictionaries.\n        metrics_dict: A dictionary of metrics to compute, where keys are metric\n            names and values are callables.\n        reduce: A callable to transform the evaluated metrics (default: identity).\n        **kwargs: Additional arguments, including:\n\n            - `evaluate_metrics_on_dataset_batch_size`: Batch size for processing data\n              (default: `data_batch_size`).\n\n    Returns:\n        A dictionary containing the evaluated metrics for the entire dataset.\n\n    Raises:\n        ValueError: When metrics and metrics_dict are both None.\n    \"\"\"\n    # Initialize metrics list from metric_dict if provided\n    metrics_from_dict = []\n    if metrics_dict is not None:\n        metrics_from_dict = [\n            apply_fns(*metrics_dict.values(), names=list(metrics_dict.keys()))\n        ]\n\n    # Initialize final metrics list\n    if metrics is None and metrics_dict is None:\n        msg = \"Either metrics or metric_dict must be provided.\"\n        raise ValueError(msg)\n    if metrics is None:\n        metrics = metrics_from_dict\n    elif metrics_dict is not None:\n        metrics.extend(metrics_from_dict)\n\n    def evaluate_data_point(dp: Data) -&gt; dict[str, Array]:\n        pred = {**pred_fn(dp[\"input\"]), \"target\": dp[\"target\"]}\n        return finalize_fns(fns=metrics, results={}, aux=pred, **kwargs)\n\n    # Evaluate metrics\n    evaluated_metrics = jax.lax.map(\n        evaluate_data_point,\n        data,\n        batch_size=kwargs.get(\n            \"evaluate_metrics_on_dataset_batch_size\", kwargs.get(\"data_batch_size\")\n        ),\n    )\n    return {metric: reduce(evaluated_metrics[metric]) for metric in evaluated_metrics}\n</code></pre>"},{"location":"reference/eval/utils/#laplax.eval.utils.evaluate_metrics_on_generator","title":"evaluate_metrics_on_generator","text":"<pre><code>evaluate_metrics_on_generator(pred_fn: Callable[[InputArray], dict[str, Array]], data_generator: Iterator[Data], *, metrics: list | None = None, metrics_dict: dict[str, Callable] | None = None, transform: Callable = identity, reduce: Callable = identity, vmap_over_data: bool = True, **kwargs: Kwargs) -&gt; dict\n</code></pre> <p>Evaluate a set of metrics on a data generator.</p> <p>Similar to evaluate_metrics_on_dataset, but works with a generator of data points instead of a dataset array. This is useful for cases where the data doesn't fit in memory or is being streamed.</p> <p>Parameters:</p> Name Type Description Default <code>pred_fn</code> <code>Callable[[InputArray], dict[str, Array]]</code> <p>A callable that takes an input array and returns predictions as a dictionary.</p> required <code>data_generator</code> <code>Iterator[Data]</code> <p>An iterator yielding data points, where each data point is a dictionary containing \"input\" and \"target\".</p> required <code>metrics</code> <code>list | None</code> <p>A list of metrics to compute, this should use the <code>apply_fns</code> function to apply the metrics and <code>transfer_entry</code> function to transfer entries between results and auxiliary dictionaries.</p> <code>None</code> <code>metrics_dict</code> <code>dict[str, Callable] | None</code> <p>A dictionary of metrics to compute, where keys are metric names and values are callables.</p> <code>None</code> <code>transform</code> <code>Callable</code> <p>The transform over individual data points.</p> <code>identity</code> <code>reduce</code> <code>Callable</code> <p>A callable to transform the evaluated metrics (default: identity).</p> <code>identity</code> <code>vmap_over_data</code> <code>bool</code> <p>Data batches from generator have unaccounted batch dimension (default: True).</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments passed to the metrics functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the evaluated metrics for all data points.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither metrics nor metric_dict is provided.</p> Source code in <code>laplax/eval/utils.py</code> <pre><code>def evaluate_metrics_on_generator(\n    pred_fn: Callable[[InputArray], dict[str, Array]],\n    data_generator: Iterator[Data],\n    *,\n    metrics: list | None = None,\n    metrics_dict: dict[str, Callable] | None = None,\n    transform: Callable = identity,\n    reduce: Callable = identity,\n    vmap_over_data: bool = True,\n    **kwargs: Kwargs,\n) -&gt; dict:\n    \"\"\"Evaluate a set of metrics on a data generator.\n\n    Similar to evaluate_metrics_on_dataset, but works with a generator of data points\n    instead of a dataset array. This is useful for cases where the data doesn't fit\n    in memory or is being streamed.\n\n    Args:\n        pred_fn: A callable that takes an input array and returns predictions\n            as a dictionary.\n        data_generator: An iterator yielding data points, where each data point\n            is a dictionary containing \"input\" and \"target\".\n        metrics: A list of metrics to compute, this should use the `apply_fns`\n            function to apply the metrics and `transfer_entry` function to transfer\n            entries between results and auxiliary dictionaries.\n        metrics_dict: A dictionary of metrics to compute, where keys are metric\n            names and values are callables.\n        transform: The transform over individual data points.\n        reduce: A callable to transform the evaluated metrics (default: identity).\n        vmap_over_data: Data batches from generator have unaccounted batch dimension\n            (default: True).\n        **kwargs: Additional keyword arguments passed to the metrics functions.\n\n    Returns:\n        A dictionary containing the evaluated metrics for all data points.\n\n    Raises:\n        ValueError: If neither metrics nor metric_dict is provided.\n    \"\"\"\n    # Initialize metrics list from metric_dict if provided\n    metrics_from_dict = []\n    if metrics_dict is not None:\n        metrics_from_dict = [\n            apply_fns(*metrics_dict.values(), names=list(metrics_dict.keys()))\n        ]\n\n    # Initialize final metrics list\n    if metrics is None and metrics_dict is None:\n        msg = \"Either metrics or metric_dict must be provided.\"\n        raise ValueError(msg)\n    if metrics is None:\n        metrics = metrics_from_dict\n    elif metrics_dict is not None:\n        metrics.extend(metrics_from_dict)\n\n    def evaluate_data(dp: Data) -&gt; dict[str, Array]:\n        pred = {**pred_fn(dp[\"input\"]), \"target\": dp[\"target\"]}\n        return finalize_fns(fns=metrics, results={}, aux=pred, **kwargs)\n\n    # Vmap over batch dimension, if necessary.\n    if vmap_over_data:\n        evaluate_data = jax.vmap(evaluate_data)\n    evaluate_data = jax.jit(evaluate_data)\n\n    # Evaluate metrics by iterating over the generator\n    all_results = [evaluate_data(transform(dp)) for dp in data_generator]\n\n    # Combine and reduce results\n    if not all_results:\n        return {}\n\n    # Get all metric names from the first result\n    metric_names = all_results[0].keys()\n\n    # Collect and reduce metrics\n    return {\n        metric: reduce([result[metric] for result in all_results])\n        for metric in metric_names\n    }\n</code></pre>"},{"location":"reference/util/flatten/","title":"laplax.util.flatten","text":"<p>Operations for flattening PyTrees into arrays.</p>"},{"location":"reference/util/flatten/#laplax.util.flatten.cumsum","title":"cumsum","text":"<pre><code>cumsum(seq: Generator) -&gt; list[int]\n</code></pre> <p>Compute the cumulative sum of a sequence.</p> <p>This function takes a sequence of integers and returns a list of cumulative sums.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Generator</code> <p>A generator or sequence of integers.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list where each element is the cumulative sum up to that point in the input sequence.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>def cumsum(\n    seq: Generator,\n) -&gt; list[int]:\n    \"\"\"Compute the cumulative sum of a sequence.\n\n    This function takes a sequence of integers and returns a list of cumulative\n    sums.\n\n    Args:\n        seq: A generator or sequence of integers.\n\n    Returns:\n        A list where each element is the cumulative sum up to that point\n            in the input sequence.\n    \"\"\"\n    total = 0\n    return [total := total + ele for ele in seq]\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.full_flatten","title":"full_flatten","text":"<pre><code>full_flatten(tree: PyTree) -&gt; Array\n</code></pre> <p>Flatten a PyTree into a single 1D array.</p> <p>This function takes a PyTree and concatenates all its leaves into a single array.</p> <p>Returns:</p> Type Description <code>Array</code> <p>The flattened PyTree.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>def full_flatten(\n    tree: PyTree,\n) -&gt; Array:\n    \"\"\"Flatten a PyTree into a single 1D array.\n\n    This function takes a PyTree and concatenates all its leaves into a single\n    array.\n\n    Returns:\n        The flattened PyTree.\n    \"\"\"\n    return jnp.concatenate([jnp.ravel(leaf) for leaf in jax.tree.flatten(tree)[0]])\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.flatten_function","title":"flatten_function","text":"<pre><code>flatten_function(fn: Callable, layout: PyTree) -&gt; Callable\n</code></pre> <p>Wrap a function to flatten its input and output.</p> <p>This function takes a function and a layout, and returns a new function that accepts flattened input and returns also a flattened output.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to wrap.</p> required <code>layout</code> <code>PyTree</code> <p>The layout of the PyTree.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The wrapped function.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>def flatten_function(\n    fn: Callable,\n    layout: PyTree,\n) -&gt; Callable:\n    \"\"\"Wrap a function to flatten its input and output.\n\n    This function takes a function and a layout, and returns a new function that\n    accepts flattened input and returns also a flattened output.\n\n    Args:\n        fn: The function to wrap.\n        layout: The layout of the PyTree.\n\n    Returns:\n        The wrapped function.\n    \"\"\"\n    flatten, unflatten = create_pytree_flattener(layout)\n    return wrap_function(fn, input_fn=unflatten, output_fn=flatten)\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.create_pytree_flattener","title":"create_pytree_flattener","text":"<pre><code>create_pytree_flattener(tree: PyTree) -&gt; tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]\n</code></pre> <p>Create functions to flatten and unflatten a PyTree into and from a 1D array.</p> <p>The <code>flatten</code> function concatenates all leaves of the PyTree into a single vector. The <code>unflatten</code> function reconstructs the original PyTree from the flattened vector.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to derive the structure for flattening and unflattening.</p> required <p>Returns:</p> Type Description <code>tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]</code> <p>Tuple containing:</p> <ul> <li><code>flatten</code>: A function that flattens a PyTree into a 1D array.</li> <li><code>unflatten</code>: A function that reconstructs the PyTree from a 1D array.</li> </ul> Source code in <code>laplax/util/flatten.py</code> <pre><code>def create_pytree_flattener(\n    tree: PyTree,\n) -&gt; tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]:\n    \"\"\"Create functions to flatten and unflatten a PyTree into and from a 1D array.\n\n    The `flatten` function concatenates all leaves of the PyTree into a single\n    vector. The `unflatten` function reconstructs the original PyTree from the\n    flattened vector.\n\n    Args:\n        tree: A PyTree to derive the structure for flattening and unflattening.\n\n    Returns:\n        Tuple containing:\n\n            - `flatten`: A function that flattens a PyTree into a 1D array.\n            - `unflatten`: A function that reconstructs the PyTree from a 1D array.\n    \"\"\"\n    # Get shapes and tree def for unflattening\n    flat, tree_def = jax.tree.flatten(tree)\n    all_shapes = [leaf.shape for leaf in flat]\n\n    def _unflatten(arr: Array) -&gt; PyTree:\n        flat_vector_split = jnp.split(\n            arr, cumsum(math.prod(sh) for sh in all_shapes)[:-1]\n        )\n        return jax.tree.unflatten(\n            tree_def,\n            [\n                a.reshape(sh)\n                for a, sh in zip(flat_vector_split, all_shapes, strict=True)\n            ],\n        )\n\n    return full_flatten, _unflatten\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.create_partial_pytree_flattener","title":"create_partial_pytree_flattener","text":"<pre><code>create_partial_pytree_flattener(tree: PyTree) -&gt; tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]\n</code></pre> <p>Create functions to flatten and unflatten partial PyTrees into and from arrays.</p> <p>This function assumes that each leaf in the PyTree is a multi-dimensional array, where the last dimension represents column indices. The <code>flatten</code> function combines all rows across leaves into a single 2D array. The <code>unflatten</code> function reconstructs the PyTree from this 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to derive the structure for flattening and unflattening.</p> required <p>Returns:</p> Type Description <code>tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]</code> <p>Tuple containing:</p> <ul> <li><code>flatten</code>: A function that flattens a PyTree into a 2D array.</li> <li><code>unflatten</code>: A function that reconstructs the PyTree from a 2D array.</li> </ul> Source code in <code>laplax/util/flatten.py</code> <pre><code>def create_partial_pytree_flattener(\n    tree: PyTree,\n) -&gt; tuple[Callable[[PyTree], Array], Callable[[Array], PyTree]]:\n    \"\"\"Create functions to flatten and unflatten partial PyTrees into and from arrays.\n\n    This function assumes that each leaf in the PyTree is a multi-dimensional\n    array, where the last dimension represents column indices. The `flatten`\n    function combines all rows across leaves into a single 2D array. The\n    `unflatten` function reconstructs the PyTree from this 2D array.\n\n    Args:\n        tree: A PyTree to derive the structure for flattening and unflattening.\n\n    Returns:\n        Tuple containing:\n\n            - `flatten`: A function that flattens a PyTree into a 2D array.\n            - `unflatten`: A function that reconstructs the PyTree from a 2D array.\n    \"\"\"\n\n    def flatten(tree: PyTree) -&gt; jax.Array:\n        flat, _ = jax.tree_util.tree_flatten(tree)\n        return jnp.concatenate(\n            [leaf.reshape(-1, leaf.shape[-1]) for leaf in flat], axis=0\n        )\n\n    # Get shapes and tree def for unflattening\n    flat, tree_def = jax.tree_util.tree_flatten(tree)\n    all_shapes = [leaf.shape for leaf in flat]\n\n    def unflatten(arr: jax.Array) -&gt; PyTree:\n        flat_vector_split = jnp.split(\n            arr, cumsum(math.prod(sh[:-1]) for sh in all_shapes)[:-1], axis=0\n        )  # Ignore column indices in shape.\n        return jax.tree_util.tree_unflatten(\n            tree_def,\n            [\n                flat_vector_split[i].reshape(all_shapes[i])\n                for i in range(len(flat_vector_split))\n            ],\n        )\n\n    return flatten, unflatten\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.unravel_array_into_pytree","title":"unravel_array_into_pytree","text":"<pre><code>unravel_array_into_pytree(pytree: PyTree, axis: int, arr: Array) -&gt; PyTree\n</code></pre> <p>Unravel an array into a PyTree with a specified structure.</p> <p>This function splits and reshapes an array to match the structure of a given PyTree, with options to control the resulting shapes using the <code>axis</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>PyTree</code> <p>The PyTree defining the desired structure.</p> required <code>axis</code> <code>int</code> <p>The axis along which to split the array.</p> required <code>arr</code> <code>Array</code> <p>The array to be unraveled into the PyTree structure.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with the specified structure, populated with parts of the input array.</p> <p>This function follows the implementation in jax._src.api._unravel_array_into_pytree.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>def unravel_array_into_pytree(\n    pytree: PyTree,\n    axis: int,\n    arr: Array,\n) -&gt; PyTree:\n    \"\"\"Unravel an array into a PyTree with a specified structure.\n\n    This function splits and reshapes an array to match the structure of a given\n    PyTree, with options to control the resulting shapes using the `axis` parameter.\n\n    Args:\n        pytree: The PyTree defining the desired structure.\n        axis: The axis along which to split the array.\n        arr: The array to be unraveled into the PyTree structure.\n\n    Returns:\n        A PyTree with the specified structure, populated with parts of the input array.\n\n    This function follows the implementation in jax._src.api._unravel_array_into_pytree.\n    \"\"\"\n    leaves, treedef = jax.tree.flatten(pytree)\n    axis %= arr.ndim\n    shapes = [arr.shape[:axis] + l.shape + arr.shape[axis + 1 :] for l in leaves]\n    parts = jnp.split(arr, cumsum(math.prod(leaf.shape) for leaf in leaves[:-1]), axis)\n    reshaped_parts = [x.reshape(shape) for x, shape in zip(parts, shapes, strict=True)]\n\n    return jax.tree.unflatten(treedef, reshaped_parts)\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.wrap_function","title":"wrap_function","text":"<pre><code>wrap_function(fn: Callable, input_fn: Callable | None = None, output_fn: Callable | None = None, argnums: int = 0) -&gt; Callable\n</code></pre> <p>Wrap a function with input and output transformations.</p> <p>This utility wraps a function <code>fn</code>, applying an optional transformation to its inputs before execution and another transformation to its outputs after execution.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to be wrapped.</p> required <code>input_fn</code> <code>Callable | None</code> <p>A callable to transform the input arguments (default: identity).</p> <code>None</code> <code>output_fn</code> <code>Callable | None</code> <p>A callable to transform the output of the function (default: identity).</p> <code>None</code> <code>argnums</code> <code>int</code> <p>The index of the argument to be transformed by <code>input_fn</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The wrapped function with input and output transformations applied.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>@singledispatch\ndef wrap_function(\n    fn: Callable,\n    input_fn: Callable | None = None,\n    output_fn: Callable | None = None,\n    argnums: int = 0,\n) -&gt; Callable:\n    \"\"\"Wrap a function with input and output transformations.\n\n    This utility wraps a function `fn`, applying an optional transformation to its\n    inputs before execution and another transformation to its outputs after\n    execution.\n\n    Args:\n        fn: The function to be wrapped.\n        input_fn: A callable to transform the input arguments (default: identity).\n        output_fn: A callable to transform the output of the function\n            (default: identity).\n        argnums: The index of the argument to be transformed by `input_fn`.\n\n    Returns:\n        The wrapped function with input and output transformations applied.\n    \"\"\"\n\n    def wrapper(*args, **kwargs) -&gt; Any:\n        # Use the identity function if input_fn or output_fn is None\n        effective_input_fn = input_fn or identity\n        effective_output_fn = output_fn or identity\n\n        # Call the original function on transformed input\n        transformed_args = (\n            *args[:argnums],\n            effective_input_fn(args[argnums]),\n            *args[argnums + 1 :],\n        )\n        result = fn(*transformed_args, **kwargs)\n\n        # Apply the output transformation function\n        return effective_output_fn(result)\n\n    return wrapper\n</code></pre>"},{"location":"reference/util/flatten/#laplax.util.flatten.wrap_factory","title":"wrap_factory","text":"<pre><code>wrap_factory(factory: Callable, input_fn: Callable | None = None, output_fn: Callable | None = None) -&gt; Callable\n</code></pre> <p>Wrap a factory function to apply input and output transformations.</p> <p>This function wraps a factory, ensuring that any callable it produces is transformed with <code>wrap_function</code> to apply input and output transformations.</p> <p>Parameters:</p> Name Type Description Default <code>factory</code> <code>Callable</code> <p>The factory function that returns a callable.</p> required <code>input_fn</code> <code>Callable | None</code> <p>A callable to transform the input arguments (default: identity).</p> <code>None</code> <code>output_fn</code> <code>Callable | None</code> <p>A callable to transform the output of the function (default: identity).</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The wrapped factory that produces transformed callables.</p> Source code in <code>laplax/util/flatten.py</code> <pre><code>def wrap_factory(\n    factory: Callable,\n    input_fn: Callable | None = None,\n    output_fn: Callable | None = None,\n) -&gt; Callable:\n    \"\"\"Wrap a factory function to apply input and output transformations.\n\n    This function wraps a factory, ensuring that any callable it produces is\n    transformed with `wrap_function` to apply input and output transformations.\n\n    Args:\n        factory: The factory function that returns a callable.\n        input_fn: A callable to transform the input arguments (default: identity).\n        output_fn: A callable to transform the output of the function\n            (default: identity).\n\n    Returns:\n        The wrapped factory that produces transformed callables.\n    \"\"\"\n\n    def wrapped_factory(*args, **kwargs) -&gt; Callable:\n        fn = factory(*args, **kwargs)\n        return wrap_function(fn, input_fn, output_fn)\n\n    return wrapped_factory\n</code></pre>"},{"location":"reference/util/loader/","title":"laplax.util.loader","text":"<p>Utilities for handling DataLoaders/Iterables instead of single batches.</p>"},{"location":"reference/util/loader/#laplax.util.loader.DataLoaderMV","title":"DataLoaderMV","text":"Source code in <code>laplax/util/loader.py</code> <pre><code>class DataLoaderMV:\n    def __init__(\n        self,\n        mv: Callable,\n        loader: Iterable,\n        transform: Callable = input_target_split,\n        reduce: Callable = reduce_online_mean,\n        *,\n        verbose_logging: bool = False,\n        **kwargs: Kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the DataLoaderMV object.\n\n        Args:\n            mv: A callable that processes a single batch of data.\n            loader: An iterable yielding batches of data.\n            transform: A callable to transform each batch into the desired format\n                (default: `input_target_split`).\n            reduce: A callable to reduce results across batches\n                (default: `reduce_online_mean`).\n            verbose_logging: Whether to log progress using tqdm (default: False).\n            **kwargs: Additional keyword arguments (currently unused).\n        \"\"\"\n        del kwargs\n        self.mv = mv\n        self.loader = loader\n        self.transform = transform\n        self.reduce = reduce\n        self.input_transform = identity\n        self.output_transform = identity\n        self.verbose_logging = verbose_logging\n\n    def __call__(self, vec: Array) -&gt; Array | PyTree:\n        \"\"\"Process the input vector using the data loader and return the result.\n\n        Args:\n            vec: The input vector to process.\n\n        Returns:\n            The processed result as an Array or PyTree.\n        \"\"\"\n        return self.output_transform(\n            process_batches(\n                self.mv,\n                self.loader,\n                transform=self.transform,\n                reduce=self.reduce,\n                vec=self.input_transform(vec),\n                verbose_logging=self.verbose_logging,\n            )\n        )\n\n    def lower_func(self, func: Callable, **kwargs: Kwargs) -&gt; Array:\n        \"\"\"Apply a function to the data loader and return the result.\n\n        Args:\n            func: A callable to apply to the data loader.\n            **kwargs: Additional keyword arguments for the function.\n\n        Returns:\n            The result of applying the function to the data loader.\n        \"\"\"\n\n        def _body_fn(data):\n            return func(\n                wrap_function(\n                    partial(self.mv, data=data),\n                    input_fn=self.input_transform,\n                    output_fn=self.output_transform,\n                ),\n                **kwargs,\n            )\n\n        return process_batches(\n            _body_fn,\n            self.loader,\n            transform=self.transform,\n            reduce=self.reduce,\n            verbose_logging=self.verbose_logging,\n        )\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.DataLoaderMV.__init__","title":"__init__","text":"<pre><code>__init__(mv: Callable, loader: Iterable, transform: Callable = input_target_split, reduce: Callable = reduce_online_mean, *, verbose_logging: bool = False, **kwargs: Kwargs) -&gt; None\n</code></pre> <p>Initialize the DataLoaderMV object.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>Callable</code> <p>A callable that processes a single batch of data.</p> required <code>loader</code> <code>Iterable</code> <p>An iterable yielding batches of data.</p> required <code>transform</code> <code>Callable</code> <p>A callable to transform each batch into the desired format (default: <code>input_target_split</code>).</p> <code>input_target_split</code> <code>reduce</code> <code>Callable</code> <p>A callable to reduce results across batches (default: <code>reduce_online_mean</code>).</p> <code>reduce_online_mean</code> <code>verbose_logging</code> <code>bool</code> <p>Whether to log progress using tqdm (default: False).</p> <code>False</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments (currently unused).</p> <code>{}</code> Source code in <code>laplax/util/loader.py</code> <pre><code>def __init__(\n    self,\n    mv: Callable,\n    loader: Iterable,\n    transform: Callable = input_target_split,\n    reduce: Callable = reduce_online_mean,\n    *,\n    verbose_logging: bool = False,\n    **kwargs: Kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DataLoaderMV object.\n\n    Args:\n        mv: A callable that processes a single batch of data.\n        loader: An iterable yielding batches of data.\n        transform: A callable to transform each batch into the desired format\n            (default: `input_target_split`).\n        reduce: A callable to reduce results across batches\n            (default: `reduce_online_mean`).\n        verbose_logging: Whether to log progress using tqdm (default: False).\n        **kwargs: Additional keyword arguments (currently unused).\n    \"\"\"\n    del kwargs\n    self.mv = mv\n    self.loader = loader\n    self.transform = transform\n    self.reduce = reduce\n    self.input_transform = identity\n    self.output_transform = identity\n    self.verbose_logging = verbose_logging\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.DataLoaderMV.__call__","title":"__call__","text":"<pre><code>__call__(vec: Array) -&gt; Array | PyTree\n</code></pre> <p>Process the input vector using the data loader and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>vec</code> <code>Array</code> <p>The input vector to process.</p> required <p>Returns:</p> Type Description <code>Array | PyTree</code> <p>The processed result as an Array or PyTree.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def __call__(self, vec: Array) -&gt; Array | PyTree:\n    \"\"\"Process the input vector using the data loader and return the result.\n\n    Args:\n        vec: The input vector to process.\n\n    Returns:\n        The processed result as an Array or PyTree.\n    \"\"\"\n    return self.output_transform(\n        process_batches(\n            self.mv,\n            self.loader,\n            transform=self.transform,\n            reduce=self.reduce,\n            vec=self.input_transform(vec),\n            verbose_logging=self.verbose_logging,\n        )\n    )\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.DataLoaderMV.lower_func","title":"lower_func","text":"<pre><code>lower_func(func: Callable, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Apply a function to the data loader and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A callable to apply to the data loader.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The result of applying the function to the data loader.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def lower_func(self, func: Callable, **kwargs: Kwargs) -&gt; Array:\n    \"\"\"Apply a function to the data loader and return the result.\n\n    Args:\n        func: A callable to apply to the data loader.\n        **kwargs: Additional keyword arguments for the function.\n\n    Returns:\n        The result of applying the function to the data loader.\n    \"\"\"\n\n    def _body_fn(data):\n        return func(\n            wrap_function(\n                partial(self.mv, data=data),\n                input_fn=self.input_transform,\n                output_fn=self.output_transform,\n            ),\n            **kwargs,\n        )\n\n    return process_batches(\n        _body_fn,\n        self.loader,\n        transform=self.transform,\n        reduce=self.reduce,\n        verbose_logging=self.verbose_logging,\n    )\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.input_target_split","title":"input_target_split","text":"<pre><code>input_target_split(batch: tuple[Array, Array]) -&gt; Data\n</code></pre> <p>Split a batch into input and target components.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple[Array, Array]</code> <p>A tuple where the first element is the input data and the second element is the target data.</p> required <p>Returns:</p> Type Description <code>Data</code> <p>A dictionary containing:</p> <ul> <li>\"input\": Input data from the batch.</li> <li>\"target\": Target data from the batch.</li> </ul> Source code in <code>laplax/util/loader.py</code> <pre><code>def input_target_split(batch: tuple[Array, Array]) -&gt; Data:\n    \"\"\"Split a batch into input and target components.\n\n    Args:\n        batch: A tuple where the first element is the input data and the second\n            element is the target data.\n\n    Returns:\n        A dictionary containing:\n\n            - \"input\": Input data from the batch.\n            - \"target\": Target data from the batch.\n    \"\"\"\n    return {\"input\": batch[0], \"target\": batch[1]}\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.reduce_sum","title":"reduce_sum","text":"<pre><code>reduce_sum(res_new: Any, state: Any | None = None, *, keepdims: bool = True, axis: int = 0) -&gt; tuple[Any, Any]\n</code></pre> <p>Perform a reduction by summing results across a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>res_new</code> <code>Any</code> <p>The new result to add to the current state.</p> required <code>state</code> <code>Any | None</code> <p>The current accumulated state (default: None).</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to keep reduced dimensions (default: True).</p> <code>True</code> <code>axis</code> <code>int</code> <p>The axis along which to sum (default: 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[Any, Any]</code> <p>The updated state and the new accumulated sum.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def reduce_sum(\n    res_new: Any, state: Any | None = None, *, keepdims: bool = True, axis: int = 0\n) -&gt; tuple[Any, Any]:\n    \"\"\"Perform a reduction by summing results across a specified axis.\n\n    Args:\n        res_new: The new result to add to the current state.\n        state: The current accumulated state (default: None).\n        keepdims: Whether to keep reduced dimensions (default: True).\n        axis: The axis along which to sum (default: 0).\n\n    Returns:\n        The updated state and the new accumulated sum.\n    \"\"\"\n    summed = jax.tree.map(lambda x: jnp.sum(x, keepdims=keepdims, axis=axis), res_new)\n    if state is None:\n        return summed, summed\n    new_state = add(state, summed)\n    return new_state, new_state\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.reduce_add","title":"reduce_add","text":"<pre><code>reduce_add(res_new: Any, state: Any | None = None) -&gt; tuple[Any, Any]\n</code></pre> <p>Perform a reduction by adding results.</p> <p>Parameters:</p> Name Type Description Default <code>res_new</code> <code>Any</code> <p>The new result to add to the current state.</p> required <code>state</code> <code>Any | None</code> <p>The current accumulated state (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Any, Any]</code> <p>The updated state and the new accumulated sum.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def reduce_add(\n    res_new: Any,\n    state: Any | None = None,\n) -&gt; tuple[Any, Any]:\n    \"\"\"Perform a reduction by adding results.\n\n    Args:\n        res_new: The new result to add to the current state.\n        state: The current accumulated state (default: None).\n\n    Returns:\n        The updated state and the new accumulated sum.\n    \"\"\"\n    if state is None:\n        return res_new, res_new\n    new_state = add(res_new, state)\n    return new_state, new_state\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.concat","title":"concat","text":"<pre><code>concat(tree1: PyTree, tree2: PyTree, axis: int = 0) -&gt; PyTree\n</code></pre> <p>Concatenate two PyTrees along a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>tree1</code> <code>PyTree</code> <p>The first PyTree to concatenate.</p> required <code>tree2</code> <code>PyTree</code> <p>The second PyTree to concatenate.</p> required <code>axis</code> <code>int</code> <p>The axis along which to concatenate (default: 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree resulting from concatenating <code>tree1</code> and <code>tree2</code>.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def concat(\n    tree1: PyTree,\n    tree2: PyTree,\n    axis: int = 0,\n) -&gt; PyTree:\n    \"\"\"Concatenate two PyTrees along a specified axis.\n\n    Args:\n        tree1: The first PyTree to concatenate.\n        tree2: The second PyTree to concatenate.\n        axis: The axis along which to concatenate (default: 0).\n\n    Returns:\n        A PyTree resulting from concatenating `tree1` and `tree2`.\n    \"\"\"\n    return jax.tree.map(\n        lambda x, y: jax.numpy.concatenate([x, y], axis=axis), tree1, tree2\n    )\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.reduce_concat","title":"reduce_concat","text":"<pre><code>reduce_concat(res_new: Any, state: Any | None = None, *, axis: int = 0) -&gt; tuple[Any, Any]\n</code></pre> <p>Perform a reduction by concatenating results.</p> <p>Parameters:</p> Name Type Description Default <code>res_new</code> <code>Any</code> <p>The new result to concatenate with the current state.</p> required <code>state</code> <code>Any | None</code> <p>The current accumulated state (default: None).</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis along which to concatenate (default: 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[Any, Any]</code> <p>The updated state and the concatenated result.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def reduce_concat(\n    res_new: Any,\n    state: Any | None = None,\n    *,\n    axis: int = 0,\n) -&gt; tuple[Any, Any]:\n    \"\"\"Perform a reduction by concatenating results.\n\n    Args:\n        res_new: The new result to concatenate with the current state.\n        state: The current accumulated state (default: None).\n        axis: The axis along which to concatenate (default: 0).\n\n    Returns:\n        The updated state and the concatenated result.\n    \"\"\"\n    if state is None:\n        return res_new, res_new\n    new_state = concat(state, res_new, axis=axis)\n    return new_state, new_state\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.reduce_online_mean","title":"reduce_online_mean","text":"<pre><code>reduce_online_mean(res_new: Any, state: tuple | None = None) -&gt; tuple[Any, tuple]\n</code></pre> <p>Compute the online mean of results, maintaining a running count and sum.</p> <p>Parameters:</p> Name Type Description Default <code>res_new</code> <code>Any</code> <p>The new result to incorporate into the mean calculation.</p> required <code>state</code> <code>tuple | None</code> <p>A tuple containing the current count and running sum (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Any, tuple]</code> <p>The updated mean and the new state (count, running sum).</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def reduce_online_mean(\n    res_new: Any,\n    state: tuple | None = None,\n) -&gt; tuple[Any, tuple]:\n    \"\"\"Compute the online mean of results, maintaining a running count and sum.\n\n    Args:\n        res_new: The new result to incorporate into the mean calculation.\n        state: A tuple containing the current count and running sum (default: None).\n\n    Returns:\n        The updated mean and the new state (count, running sum).\n    \"\"\"\n    batch_size = jax.tree.map(lambda x: x.shape[0] if x.ndim &gt; 0 else 1, res_new)\n    batch_sum = jax.tree.map(\n        lambda x: jnp.sum(x, axis=0) if x.ndim &gt; 0 else jnp.sum(x),\n        res_new,\n    )\n\n    if state is None:\n        return jax.tree.map(operator.truediv, batch_sum, batch_size), (\n            batch_size,\n            batch_sum,\n        )\n\n    old_count, old_sum = state\n    total_count = jax.tree.map(operator.add, old_count, batch_size)\n    new_sum = add(old_sum, batch_sum)\n\n    current_mean = jax.tree.map(operator.truediv, new_sum, total_count)\n\n    return current_mean, (total_count, new_sum)\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.process_batches","title":"process_batches","text":"<pre><code>process_batches(function: Callable, data_loader: Iterable, transform: Callable, reduce: Callable, *args: Any, verbose_logging: bool = False, **kwargs: Kwargs) -&gt; Any\n</code></pre> <p>Process batches of data using a function, transformation, and reduction.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>A callable that processes a single batch of data.</p> required <code>data_loader</code> <code>Iterable</code> <p>An iterable yielding batches of data.</p> required <code>transform</code> <code>Callable</code> <p>A callable that transforms each batch into the desired format.</p> required <code>reduce</code> <code>Callable</code> <p>A callable that reduces results across batches.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments for the processing function.</p> <code>()</code> <code>verbose_logging</code> <code>bool</code> <p>Whether to log progress using tqdm (default: False).</p> <code>False</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for the processing function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The final result after processing all batches.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data loader is empty.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def process_batches(\n    function: Callable,\n    data_loader: Iterable,\n    transform: Callable,\n    reduce: Callable,\n    *args: Any,\n    verbose_logging: bool = False,\n    **kwargs: Kwargs,\n) -&gt; Any:\n    \"\"\"Process batches of data using a function, transformation, and reduction.\n\n    Args:\n        function: A callable that processes a single batch of data.\n        data_loader: An iterable yielding batches of data.\n        transform: A callable that transforms each batch into the desired format.\n        reduce: A callable that reduces results across batches.\n        *args: Additional positional arguments for the processing function.\n        verbose_logging: Whether to log progress using tqdm (default: False).\n        **kwargs: Additional keyword arguments for the processing function.\n\n    Returns:\n        The final result after processing all batches.\n\n    Raises:\n        ValueError: If the data loader is empty.\n    \"\"\"\n    state = None\n    result = None\n    for batch in tqdm(\n        data_loader, desc=\"Processing batches\", disable=not verbose_logging\n    ):\n        result = function(*args, data=transform(batch), **kwargs)\n        result, state = reduce(result, state)\n    if result is None:\n        msg = \"data loader was empty\"\n        raise ValueError(msg)\n    return result\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.execute_with_data_loader","title":"execute_with_data_loader","text":"<pre><code>execute_with_data_loader(function: Callable, data_loader: Iterable, transform: Callable = input_target_split, reduce: Callable = reduce_online_mean, *, jit: bool = False, **kwargs: Kwargs) -&gt; Any\n</code></pre> <p>Execute batch processing with a data loader.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>A callable that processes a single batch of data.</p> required <code>data_loader</code> <code>Iterable</code> <p>An iterable yielding batches of data.</p> required <code>transform</code> <code>Callable</code> <p>A callable to transform each batch into the desired format (default: <code>input_target_split</code>).</p> <code>input_target_split</code> <code>reduce</code> <code>Callable</code> <p>A callable to reduce results across batches (default: <code>reduce_online_mean</code>).</p> <code>reduce_online_mean</code> <code>jit</code> <code>bool</code> <p>Whether to JIT compile the processing function (default: False).</p> <code>False</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for the processing function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The final result after processing all batches.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def execute_with_data_loader(\n    function: Callable,\n    data_loader: Iterable,\n    transform: Callable = input_target_split,\n    reduce: Callable = reduce_online_mean,\n    *,\n    jit: bool = False,\n    **kwargs: Kwargs,\n) -&gt; Any:\n    \"\"\"Execute batch processing with a data loader.\n\n    Args:\n        function: A callable that processes a single batch of data.\n        data_loader: An iterable yielding batches of data.\n        transform: A callable to transform each batch into the desired format\n            (default: `input_target_split`).\n        reduce: A callable to reduce results across batches\n            (default: `reduce_online_mean`).\n        jit: Whether to JIT compile the processing function (default: False).\n        **kwargs: Additional keyword arguments for the processing function.\n\n    Returns:\n        The final result after processing all batches.\n    \"\"\"\n    fn = jax.jit(function) if jit else function\n    return process_batches(fn, data_loader, transform, reduce, **kwargs)\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader.wrap_function_with_data_loader","title":"wrap_function_with_data_loader","text":"<pre><code>wrap_function_with_data_loader(function: Callable, data_loader: Iterable, transform: Callable = input_target_split, reduce: Callable = reduce_online_mean, *, jit: bool = False) -&gt; Callable\n</code></pre> <p>Wrap a function to process batches with a data loader.</p> <p>This wrapper generates a callable that processes all batches from the data loader using the specified function, transformation, and reduction.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>A callable that processes a single batch of data.</p> required <code>data_loader</code> <code>Iterable</code> <p>An iterable yielding batches of data.</p> required <code>transform</code> <code>Callable</code> <p>A callable to transform each batch into the desired format (default: <code>input_target_split</code>).</p> <code>input_target_split</code> <code>reduce</code> <code>Callable</code> <p>A callable to reduce results across batches (default: <code>reduce_online_mean</code>).</p> <code>reduce_online_mean</code> <code>jit</code> <code>bool</code> <p>Whether to JIT compile the processing function (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A wrapped function for batch processing.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>def wrap_function_with_data_loader(\n    function: Callable,\n    data_loader: Iterable,\n    transform: Callable = input_target_split,\n    reduce: Callable = reduce_online_mean,\n    *,\n    jit: bool = False,\n) -&gt; Callable:\n    \"\"\"Wrap a function to process batches with a data loader.\n\n    This wrapper generates a callable that processes all batches from the data loader\n    using the specified function, transformation, and reduction.\n\n    Args:\n        function: A callable that processes a single batch of data.\n        data_loader: An iterable yielding batches of data.\n        transform: A callable to transform each batch into the desired format\n            (default: `input_target_split`).\n        reduce: A callable to reduce results across batches\n            (default: `reduce_online_mean`).\n        jit: Whether to JIT compile the processing function (default: False).\n\n    Returns:\n        A wrapped function for batch processing.\n    \"\"\"\n    fn = jax.jit(function) if jit else function\n\n    def wrapped(*args, **kwargs):\n        return process_batches(fn, data_loader, transform, reduce, *args, **kwargs)\n\n    return wrapped\n</code></pre>"},{"location":"reference/util/loader/#laplax.util.loader._","title":"_","text":"<pre><code>_(mv: DataLoaderMV, input_fn: Callable | None = None, output_fn: Callable | None = None, argnums: int = 0) -&gt; Callable\n</code></pre> <p>Apply wrap_function to DataLoaderMV.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A DataLoaderMV object representing the wrapped MV.</p> Source code in <code>laplax/util/loader.py</code> <pre><code>@wrap_function.register\ndef _(\n    mv: DataLoaderMV,\n    input_fn: Callable | None = None,\n    output_fn: Callable | None = None,\n    argnums: int = 0,\n) -&gt; Callable:\n    \"\"\"Apply wrap_function to DataLoaderMV.\n\n    Returns:\n        A DataLoaderMV object representing the wrapped MV.\n    \"\"\"\n    # Create new transforms without overwriting existing ones\n    new_input_transform = wrap_function(\n        mv.input_transform, input_fn=input_fn, argnums=argnums\n    )\n    new_output_transform = wrap_function(\n        mv.output_transform,\n        output_fn=output_fn,\n    )\n\n    new_mv = DataLoaderMV(\n        mv.mv, mv.loader, mv.transform, mv.reduce, verbose_logging=mv.verbose_logging\n    )\n    new_mv.input_transform = new_input_transform\n    new_mv.output_transform = new_output_transform\n\n    return new_mv\n</code></pre>"},{"location":"reference/util/mv/","title":"laplax.util.mv","text":"<p>Matrix-free array operations for matrix-vector products.</p>"},{"location":"reference/util/mv/#laplax.util.mv.diagonal","title":"diagonal","text":"<pre><code>diagonal(mv: Callable | ndarray, layout: Layout | None = None, *, mv_jittable: bool = True, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Compute the diagonal of a matrix represented by a matrix-vector product function.</p> <p>This function extracts the diagonal of a matrix using basis vectors and a matrix-vector product (MVP) function. If the input is already a dense matrix, its diagonal is directly computed.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>Callable | ndarray</code> <p>Either:</p> <ul> <li>A callable that implements the MVP, or</li> <li>A dense matrix (jax.Array) for which the diagonal is directly extracted.</li> </ul> required <code>layout</code> <code>Layout | None</code> <p>Specifies the structure of the matrix:</p> <ul> <li>int: The size of the matrix (for flat MVP functions).</li> <li>PyTree: A structure to generate basis vectors matching the matrix     dimensions.</li> <li>None: If <code>mv</code> is a dense matrix.</li> </ul> <code>None</code> <code>mv_jittable</code> <code>bool</code> <p>Whether to JIT compile the basis vector generator.</p> <code>True</code> <code>**kwargs</code> <code>Kwargs</code> <p>diagonal_batch_size: Batch size for applying the MVP function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>An array representing the diagonal of the matrix.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>layout</code> is not provided when <code>mv</code> is a callable.</p> Source code in <code>laplax/util/mv.py</code> <pre><code>@singledispatch\ndef diagonal(\n    mv: Callable | jnp.ndarray,\n    layout: Layout | None = None,\n    *,\n    mv_jittable: bool = True,\n    **kwargs: Kwargs,\n) -&gt; Array:\n    \"\"\"Compute the diagonal of a matrix represented by a matrix-vector product function.\n\n    This function extracts the diagonal of a matrix using basis vectors and a\n    matrix-vector product (MVP) function. If the input is already a dense matrix, its\n    diagonal is directly computed.\n\n    Args:\n        mv: Either:\n\n            - A callable that implements the MVP, or\n            - A dense matrix (jax.Array) for which the diagonal is directly extracted.\n        layout: Specifies the structure of the matrix:\n\n            - int: The size of the matrix (for flat MVP functions).\n            - PyTree: A structure to generate basis vectors matching the matrix\n                dimensions.\n            - None: If `mv` is a dense matrix.\n        mv_jittable: Whether to JIT compile the basis vector generator.\n        **kwargs:\n            diagonal_batch_size: Batch size for applying the MVP function.\n\n    Returns:\n        An array representing the diagonal of the matrix.\n\n    Raises:\n        TypeError: If `layout` is not provided when `mv` is a callable.\n    \"\"\"\n    if isinstance(mv, Callable) and layout is None:\n        msg = \"either size or tree needs to be present\"\n        raise TypeError(msg)\n\n    if isinstance(mv, jax.Array):\n        return jnp.diag(mv)\n\n    # Define basis vector generator based on layout type\n    if isinstance(layout, int):  # Integer layout defines size\n        size = layout\n\n        @jax.jit\n        def get_basis_vec(idx: int) -&gt; jax.Array:\n            zero_vec = jnp.zeros(size)\n            return zero_vec.at[idx].set(1.0)\n\n    else:  # PyTree layout\n        size = get_size(layout)\n\n        @jax.jit\n        def get_basis_vec(idx: int) -&gt; PyTree:\n            return basis_vector_from_index(idx, layout)\n\n    def diag_elem(i):\n        return util.tree.tree_vec_get(mv(get_basis_vec(i)), i)\n\n    if mv_jittable:\n        diag_elem = jax.jit(diag_elem)\n\n    return jax.lax.map(\n        diag_elem, jnp.arange(size), batch_size=kwargs.get(\"diagonal_batch_size\")\n    )\n</code></pre>"},{"location":"reference/util/mv/#laplax.util.mv.to_dense","title":"to_dense","text":"<pre><code>to_dense(mv: Callable, layout: Layout, **kwargs: Kwargs) -&gt; Array\n</code></pre> <p>Generate a dense matrix representation from a matrix-vector product function.</p> <p>Converts a matrix-vector product function into its equivalent dense matrix form by applying the function to identity-like basis vectors.</p> <p>Parameters:</p> Name Type Description Default <code>mv</code> <code>Callable</code> <p>A callable implementing the matrix-vector product function.</p> required <code>layout</code> <code>Layout</code> <p>Specifies the structure of the input:</p> <ul> <li>int: The size of the input dimension (flat vectors).</li> <li>PyTree: The structure for input to the MVP.</li> <li>None: Defaults to an identity-like structure.</li> </ul> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional options:</p> <ul> <li><code>to_dense_batch_size</code>: Batch size for applying the MVP function.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>A dense matrix representation of the MVP function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>layout</code> is neither an integer nor a PyTree structure.</p> Source code in <code>laplax/util/mv.py</code> <pre><code>@singledispatch\ndef to_dense(mv: Callable, layout: Layout, **kwargs: Kwargs) -&gt; Array:\n    \"\"\"Generate a dense matrix representation from a matrix-vector product function.\n\n    Converts a matrix-vector product function into its equivalent dense matrix form\n    by applying the function to identity-like basis vectors.\n\n    Args:\n        mv: A callable implementing the matrix-vector product function.\n        layout: Specifies the structure of the input:\n\n            - int: The size of the input dimension (flat vectors).\n            - PyTree: The structure for input to the MVP.\n            - None: Defaults to an identity-like structure.\n        **kwargs: Additional options:\n\n            - `to_dense_batch_size`: Batch size for applying the MVP function.\n\n    Returns:\n        A dense matrix representation of the MVP function.\n\n    Raises:\n        TypeError: If `layout` is neither an integer nor a PyTree structure.\n    \"\"\"\n    # Create the identity-like basis based on `layout`\n    if isinstance(layout, int):\n        identity = jnp.eye(layout)\n    elif isinstance(layout, PyTree):\n        identity = eye_like(layout)\n    else:\n        msg = \"`layout` must be an integer or a PyTree structure.\"\n        raise TypeError(msg)\n\n    return jax.tree.map(\n        jnp.transpose,\n        jax.lax.map(mv, identity, batch_size=kwargs.get(\"to_dense_batch_size\")),\n    )  # jax.lax.map shares along the first axis (rows instead of columns).\n</code></pre>"},{"location":"reference/util/ops/","title":"laplax.util.ops","text":"<p>Contains operations for flexible/adaptive compute.</p>"},{"location":"reference/util/ops/#laplax.util.ops.str_to_bool","title":"str_to_bool","text":"<pre><code>str_to_bool(value: str) -&gt; bool\n</code></pre> <p>Convert a string representation of a boolean to a boolean value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>A string representation of a boolean (\"True\" or \"False\").</p> required <p>Returns:</p> Type Description <code>bool</code> <p>The corresponding boolean value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string does not represent a valid boolean value.</p> Source code in <code>laplax/util/ops.py</code> <pre><code>def str_to_bool(value: str) -&gt; bool:\n    \"\"\"Convert a string representation of a boolean to a boolean value.\n\n    Args:\n        value: A string representation of a boolean (\"True\" or \"False\").\n\n    Returns:\n        The corresponding boolean value.\n\n    Raises:\n        ValueError: If the string does not represent a valid boolean value.\n    \"\"\"\n    valid_values = {\"True\": True, \"False\": False}\n    if value not in valid_values:\n        msg = \"invalid string representation of a boolean value\"\n        raise ValueError(msg)\n    return valid_values[value]\n</code></pre>"},{"location":"reference/util/ops/#laplax.util.ops.precompute_list","title":"precompute_list","text":"<pre><code>precompute_list(func: Callable, items: Iterable, precompute: bool | None = None, **kwargs: Kwargs) -&gt; Callable\n</code></pre> <p>Precompute results for a list of items or return the original function.</p> <p>If <code>option</code> is enabled, this function applies <code>func</code> to all items in <code>items</code> and stores the results for later retrieval. Otherwise, it returns <code>func</code> as-is.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to apply to each item in the list.</p> required <code>items</code> <code>Iterable</code> <p>An iterable of items to process.</p> required <code>precompute</code> <code>bool | None</code> <p>Determines whether to precompute results: - None: Use the default precompute setting. - bool: Specify directly whether to precompute.</p> <code>None</code> <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments, including: - precompute_list_batch_size: Batch size for precomputing results.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to retrieve precomputed elements by index, or the original <code>func</code> if precomputation is disabled.</p> Source code in <code>laplax/util/ops.py</code> <pre><code>def precompute_list(\n    func: Callable, items: Iterable, precompute: bool | None = None, **kwargs: Kwargs\n) -&gt; Callable:\n    \"\"\"Precompute results for a list of items or return the original function.\n\n    If `option` is enabled, this function applies `func` to all items in `items`\n    and stores the results for later retrieval. Otherwise, it returns `func` as-is.\n\n    Args:\n        func: The function to apply to each item in the list.\n        items: An iterable of items to process.\n        precompute: Determines whether to precompute results:\n            - None: Use the default precompute setting.\n            - bool: Specify directly whether to precompute.\n        **kwargs: Additional keyword arguments, including:\n            - precompute_list_batch_size: Batch size for precomputing results.\n\n    Returns:\n        A function to retrieve precomputed elements by index, or the original\n            `func` if precomputation is disabled.\n    \"\"\"\n    if precompute is None:\n        precompute = DEFAULT_PRECOMPUTE_LIST\n\n    if precompute:\n        precomputed = jax.lax.map(\n            func, items, batch_size=kwargs.get(\"precompute_list_batch_size\")\n        )\n\n        def get_element(index: int):\n            return jax.tree.map(operator.itemgetter(index), precomputed)\n\n        return get_element\n\n    return func\n</code></pre>"},{"location":"reference/util/tree/","title":"laplax.util.tree","text":"<p>Relevant tree operations.</p>"},{"location":"reference/util/tree/#laplax.util.tree.get_size","title":"get_size","text":"<pre><code>get_size(tree: PyTree) -&gt; int\n</code></pre> <p>Compute the total number of elements in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose total size is to be calculated.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The total number of elements across all leaves in the PyTree.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def get_size(tree: PyTree) -&gt; int:\n    \"\"\"Compute the total number of elements in a PyTree.\n\n    Args:\n        tree: A PyTree whose total size is to be calculated.\n\n    Returns:\n        The total number of elements across all leaves in the PyTree.\n    \"\"\"\n    flat, _ = jax.tree_util.tree_flatten(tree)\n    return sum(math.prod(arr.shape) for arr in flat)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.add","title":"add","text":"<pre><code>add(tree1: PyTree, tree2: PyTree) -&gt; PyTree\n</code></pre> <p>Add corresponding elements of two PyTrees.</p> <p>Parameters:</p> Name Type Description Default <code>tree1</code> <code>PyTree</code> <p>The first PyTree.</p> required <code>tree2</code> <code>PyTree</code> <p>The second PyTree.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree where each leaf is the element-wise sum of the leaves in <code>tree1</code> and <code>tree2</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def add(tree1: PyTree, tree2: PyTree) -&gt; PyTree:\n    \"\"\"Add corresponding elements of two PyTrees.\n\n    Args:\n        tree1: The first PyTree.\n        tree2: The second PyTree.\n\n    Returns:\n        A PyTree where each leaf is the element-wise sum of the leaves in\n            `tree1` and `tree2`.\n    \"\"\"\n    return jax.tree.map(jnp.add, tree1, tree2)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.neg","title":"neg","text":"<pre><code>neg(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Negate all elements of a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to negate.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with negated elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def neg(tree: PyTree) -&gt; PyTree:\n    \"\"\"Negate all elements of a PyTree.\n\n    Args:\n        tree: A PyTree to negate.\n\n    Returns:\n        A PyTree with negated elements.\n    \"\"\"\n    return jax.tree.map(jnp.negative, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.sub","title":"sub","text":"<pre><code>sub(tree1: PyTree, tree2: PyTree) -&gt; PyTree\n</code></pre> <p>Subtract corresponding elements of two PyTrees.</p> <p>Parameters:</p> Name Type Description Default <code>tree1</code> <code>PyTree</code> <p>The first PyTree.</p> required <code>tree2</code> <code>PyTree</code> <p>The second PyTree.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree where each leaf is the element-wise difference of the leaves in <code>tree1</code> and <code>tree2</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def sub(tree1: PyTree, tree2: PyTree) -&gt; PyTree:\n    \"\"\"Subtract corresponding elements of two PyTrees.\n\n    Args:\n        tree1: The first PyTree.\n        tree2: The second PyTree.\n\n    Returns:\n        A PyTree where each leaf is the element-wise difference of the leaves in\n            `tree1` and `tree2`.\n    \"\"\"\n    return add(tree1, neg(tree2))\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.mul","title":"mul","text":"<pre><code>mul(scalar: Float, tree: PyTree) -&gt; PyTree\n</code></pre> <p>Multiply all elements of a PyTree by a scalar.</p> <p>Parameters:</p> Name Type Description Default <code>scalar</code> <code>Float</code> <p>The scalar value to multiply by.</p> required <code>tree</code> <code>PyTree</code> <p>A PyTree to multiply.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree where each leaf is the element-wise product of the leaves in <code>tree</code> and <code>scalar</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def mul(scalar: Float, tree: PyTree) -&gt; PyTree:\n    \"\"\"Multiply all elements of a PyTree by a scalar.\n\n    Args:\n        scalar: The scalar value to multiply by.\n        tree: A PyTree to multiply.\n\n    Returns:\n        A PyTree where each leaf is the element-wise product of the leaves in\n            `tree` and `scalar`.\n    \"\"\"\n    return jax.tree.map(lambda x: scalar * x, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.sqrt","title":"sqrt","text":"<pre><code>sqrt(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Compute the square root of each element in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose elements are to be square-rooted.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with square-rooted elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def sqrt(tree: PyTree) -&gt; PyTree:\n    \"\"\"Compute the square root of each element in a PyTree.\n\n    Args:\n        tree: A PyTree whose elements are to be square-rooted.\n\n    Returns:\n        A PyTree with square-rooted elements.\n    \"\"\"\n    return jax.tree.map(jnp.sqrt, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.invert","title":"invert","text":"<pre><code>invert(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Invert all elements of a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to invert.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with inverted elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def invert(tree: PyTree) -&gt; PyTree:\n    \"\"\"Invert all elements of a PyTree.\n\n    Args:\n        tree: A PyTree to invert.\n\n    Returns:\n        A PyTree with inverted elements.\n    \"\"\"\n    return jax.tree.map(jnp.invert, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.mean","title":"mean","text":"<pre><code>mean(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree\n</code></pre> <p>Compute the mean of each element in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose elements are to be averaged.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for <code>jnp.mean</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with averaged elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def mean(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree:\n    \"\"\"Compute the mean of each element in a PyTree.\n\n    Args:\n        tree: A PyTree whose elements are to be averaged.\n        **kwargs: Additional keyword arguments for `jnp.mean`.\n\n    Returns:\n        A PyTree with averaged elements.\n    \"\"\"\n    return jax.tree.map(partial(jnp.mean, **kwargs), tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.std","title":"std","text":"<pre><code>std(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree\n</code></pre> <p>Compute the standard deviation of each element in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose elements are to be standard-deviated.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for <code>jnp.std</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with standard-deviated elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def std(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree:\n    \"\"\"Compute the standard deviation of each element in a PyTree.\n\n    Args:\n        tree: A PyTree whose elements are to be standard-deviated.\n        **kwargs: Additional keyword arguments for `jnp.std`.\n\n    Returns:\n        A PyTree with standard-deviated elements.\n    \"\"\"\n    return jax.tree.map(partial(jnp.std, **kwargs), tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.var","title":"var","text":"<pre><code>var(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree\n</code></pre> <p>Compute the variance of each element in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose elements are to be variance-ed.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for <code>jnp.var</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with variance-ed elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def var(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree:\n    \"\"\"Compute the variance of each element in a PyTree.\n\n    Args:\n        tree: A PyTree whose elements are to be variance-ed.\n        **kwargs: Additional keyword arguments for `jnp.var`.\n\n    Returns:\n        A PyTree with variance-ed elements.\n    \"\"\"\n    return jax.tree.map(partial(jnp.var, **kwargs), tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.cov","title":"cov","text":"<pre><code>cov(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree\n</code></pre> <p>Compute the covariance of each element in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose elements are to be covariance-ed.</p> required <code>**kwargs</code> <code>Kwargs</code> <p>Additional keyword arguments for <code>jnp.cov</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with covariance-ed elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def cov(tree: PyTree, **kwargs: Kwargs) -&gt; PyTree:\n    \"\"\"Compute the covariance of each element in a PyTree.\n\n    Args:\n        tree: A PyTree whose elements are to be covariance-ed.\n        **kwargs: Additional keyword arguments for `jnp.cov`.\n\n    Returns:\n        A PyTree with covariance-ed elements.\n    \"\"\"\n    return jax.tree.map(partial(jnp.cov, **kwargs), tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.tree_matvec","title":"tree_matvec","text":"<pre><code>tree_matvec(tree: PyTree, vector: Array) -&gt; PyTree\n</code></pre> <p>Multiply a PyTree by a vector.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to multiply.</p> required <code>vector</code> <code>Array</code> <p>A vector to multiply by.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with multiplied elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def tree_matvec(tree: PyTree, vector: Array) -&gt; PyTree:\n    \"\"\"Multiply a PyTree by a vector.\n\n    Args:\n        tree: A PyTree to multiply.\n        vector: A vector to multiply by.\n\n    Returns:\n        A PyTree with multiplied elements.\n    \"\"\"\n    # Flatten the vector\n    vec_flatten, vec_def = jax.tree.flatten(vector)\n    num_vec_flatten = len(vec_flatten)\n\n    # Array flattening and reshaping\n    arr_flatten, _ = jax.tree.flatten(tree)\n    arr_flatten = [\n        jnp.concatenate(\n            [\n                arr_flatten[i * num_vec_flatten + j].reshape(*vec_flatten[j].shape, -1)\n                for i in range(num_vec_flatten)\n            ],\n            axis=-1,\n        )\n        for j in range(num_vec_flatten)\n    ]\n\n    # Array, vector to correct shape\n    tree = jax.tree.unflatten(vec_def, arr_flatten)\n    vec_flatten = jnp.concatenate([v.reshape(-1) for v in vec_flatten])\n\n    # Apply matmul\n    return jax.tree.map(lambda p: p @ vec_flatten, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.tree_partialmatvec","title":"tree_partialmatvec","text":"<pre><code>tree_partialmatvec(tree: PyTree, vector: Array) -&gt; PyTree\n</code></pre> <p>Multiply a PyTree by a vector.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to multiply.</p> required <code>vector</code> <code>Array</code> <p>A vector to multiply by.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with multiplied elements.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def tree_partialmatvec(tree: PyTree, vector: Array) -&gt; PyTree:\n    \"\"\"Multiply a PyTree by a vector.\n\n    Args:\n        tree: A PyTree to multiply.\n        vector: A vector to multiply by.\n\n    Returns:\n        A PyTree with multiplied elements.\n    \"\"\"\n    return jax.tree.map(lambda arr: arr @ vector, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.ones_like","title":"ones_like","text":"<pre><code>ones_like(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Create a PyTree of ones with the same structure as the input tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose structure and shape will be used.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree of ones with the same structure and shape as <code>tree</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def ones_like(tree: PyTree) -&gt; PyTree:\n    \"\"\"Create a PyTree of ones with the same structure as the input tree.\n\n    Args:\n        tree: A PyTree whose structure and shape will be used.\n\n    Returns:\n        A PyTree of ones with the same structure and shape as `tree`.\n    \"\"\"\n    return jax.tree.map(jnp.ones_like, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.zeros_like","title":"zeros_like","text":"<pre><code>zeros_like(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Create a PyTree of zeros with the same structure as the input tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree whose structure and shape will be used.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree of zeros with the same structure and shape as <code>tree</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def zeros_like(tree: PyTree) -&gt; PyTree:\n    \"\"\"Create a PyTree of zeros with the same structure as the input tree.\n\n    Args:\n        tree: A PyTree whose structure and shape will be used.\n\n    Returns:\n        A PyTree of zeros with the same structure and shape as `tree`.\n    \"\"\"\n    return jax.tree.map(jnp.zeros_like, tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.randn_like","title":"randn_like","text":"<pre><code>randn_like(key: KeyType, tree: PyTree) -&gt; PyTree\n</code></pre> <p>Generate a PyTree of random normal values with the same structure as the input.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyType</code> <p>A JAX PRNG key.</p> required <code>tree</code> <code>PyTree</code> <p>A PyTree whose structure will be replicated.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree of random normal values.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def randn_like(key: KeyType, tree: PyTree) -&gt; PyTree:\n    \"\"\"Generate a PyTree of random normal values with the same structure as the input.\n\n    Args:\n        key: A JAX PRNG key.\n        tree: A PyTree whose structure will be replicated.\n\n    Returns:\n        A PyTree of random normal values.\n    \"\"\"\n    # Flatten the tree\n    leaves, treedef = jax.tree.flatten(tree)\n\n    # Split key\n    keys = jax.random.split(key, len(leaves))\n\n    # Generate random numbers\n    random_leaves = [\n        jax.random.normal(k, shape=leaf.shape)\n        for k, leaf in zip(keys, leaves, strict=True)\n    ]\n\n    return jax.tree.unflatten(treedef, random_leaves)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.normal_like","title":"normal_like","text":"<pre><code>normal_like(key: KeyType, mean: PyTree, scale_mv: Callable[[PyTree], PyTree]) -&gt; PyTree\n</code></pre> <p>Generate a PyTree of random normal values scaled and shifted by <code>mean</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyType</code> <p>A JAX PRNG key.</p> required <code>mean</code> <code>PyTree</code> <p>A PyTree representing the mean of the distribution.</p> required <code>scale_mv</code> <code>Callable[[PyTree], PyTree]</code> <p>A callable that scales a PyTree.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree of random normal values shifted by <code>mean</code>.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def normal_like(\n    key: KeyType,\n    mean: PyTree,\n    scale_mv: Callable[[PyTree], PyTree],\n) -&gt; PyTree:\n    \"\"\"Generate a PyTree of random normal values scaled and shifted by `mean`.\n\n    Args:\n        key: A JAX PRNG key.\n        mean: A PyTree representing the mean of the distribution.\n        scale_mv: A callable that scales a PyTree.\n\n    Returns:\n        A PyTree of random normal values shifted by `mean`.\n    \"\"\"\n    return add(mean, scale_mv(randn_like(key, mean)))\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.basis_vector_from_index","title":"basis_vector_from_index","text":"<pre><code>basis_vector_from_index(idx: int, tree: PyTree) -&gt; PyTree\n</code></pre> <p>Create a basis vector from an index in a PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the basis vector.</p> required <code>tree</code> <code>PyTree</code> <p>A PyTree whose structure will be used.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with a basis vector at the specified index.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def basis_vector_from_index(idx: int, tree: PyTree) -&gt; PyTree:\n    \"\"\"Create a basis vector from an index in a PyTree.\n\n    Args:\n        idx: The index of the basis vector.\n        tree: A PyTree whose structure will be used.\n\n    Returns:\n        A PyTree with a basis vector at the specified index.\n    \"\"\"\n    # Create a tree of zeros with the same structure\n    zeros = zeros_like(tree)\n\n    # Flatten the tree to get a list of arrays and the tree definition\n    flat, tree_def = jax.tree_util.tree_flatten(zeros)\n\n    # Compute the cumulative sizes of each array in the flat structure\n    sizes = jnp.array([math.prod(arr.shape) for arr in flat])\n\n    # Find the index of the array containing the idx\n    cum_sizes = jnp.cumsum(sizes)\n    k = jnp.searchsorted(cum_sizes, idx, side=\"right\")\n\n    # Compute the adjusted index within the identified array\n    idx_corr = idx - jnp.where(k &gt; 0, cum_sizes[k - 1], 0)\n\n    # Define a function to update the k-th array with the basis vector\n    def update_array(i, arr):\n        return jax.lax.cond(\n            i == k,\n            lambda: arr.flatten().at[idx_corr].set(1.0).reshape(arr.shape),\n            lambda: arr,\n        )\n\n    # Map the update function across the flattened list of arrays\n    updated_flat = list(starmap(update_array, enumerate(flat)))\n\n    # Reconstruct the tree with the updated flat structure\n    return jax.tree_util.tree_unflatten(tree_def, updated_flat)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.eye_like_with_basis_vector","title":"eye_like_with_basis_vector","text":"<pre><code>eye_like_with_basis_vector(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Create a PyTree where each element is a basis vector.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree defining the structure.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree of basis vectors.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def eye_like_with_basis_vector(tree: PyTree) -&gt; PyTree:\n    \"\"\"Create a PyTree where each element is a basis vector.\n\n    Args:\n        tree: A PyTree defining the structure.\n\n    Returns:\n        A PyTree of basis vectors.\n    \"\"\"\n    num_elems = get_size(tree)\n    return jax.lax.map(\n        partial(basis_vector_from_index, tree=tree), jnp.arange(num_elems)\n    )\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.eye_like","title":"eye_like","text":"<pre><code>eye_like(tree: PyTree) -&gt; PyTree\n</code></pre> <p>Create a PyTree equivalent of an identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree defining the structure.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree equivalent to an identity matrix.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def eye_like(tree: PyTree) -&gt; PyTree:\n    \"\"\"Create a PyTree equivalent of an identity matrix.\n\n    Args:\n        tree: A PyTree defining the structure.\n\n    Returns:\n        A PyTree equivalent to an identity matrix.\n    \"\"\"\n    return unravel_array_into_pytree(tree, 1, jnp.eye(get_size(tree)))\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.tree_slice","title":"tree_slice","text":"<pre><code>tree_slice(tree: PyTree, a: int, b: int) -&gt; PyTree\n</code></pre> <p>Slice each leaf of a PyTree along the first dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to slice.</p> required <code>a</code> <code>int</code> <p>The start index.</p> required <code>b</code> <code>int</code> <p>The end index.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>A PyTree with sliced leaves.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def tree_slice(tree: PyTree, a: int, b: int) -&gt; PyTree:\n    \"\"\"Slice each leaf of a PyTree along the first dimension.\n\n    Args:\n        tree: A PyTree to slice.\n        a: The start index.\n        b: The end index.\n\n    Returns:\n        A PyTree with sliced leaves.\n    \"\"\"\n    return jax.tree.map(operator.itemgetter(slice(a, b)), tree)\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.tree_vec_get","title":"tree_vec_get","text":"<pre><code>tree_vec_get(tree: PyTree, idx: int) -&gt; Any\n</code></pre> <p>Retrieve the element at the specified index from a flattened PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>PyTree</code> <p>A PyTree to retrieve the element from.</p> required <code>idx</code> <code>int</code> <p>The index of the element.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The element at the specified index.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def tree_vec_get(tree: PyTree, idx: int) -&gt; Any:\n    \"\"\"Retrieve the element at the specified index from a flattened PyTree.\n\n    Args:\n        tree: A PyTree to retrieve the element from.\n        idx: The index of the element.\n\n    Returns:\n        The element at the specified index.\n    \"\"\"\n    if isinstance(tree, jnp.ndarray):\n        return tree.reshape(-1)[idx]\n    # Column flat and get index\n    flat, _ = jax.tree_util.tree_flatten(tree)\n    flat = jnp.concatenate([f.reshape(-1) for f in flat])\n    return flat[idx]\n</code></pre>"},{"location":"reference/util/tree/#laplax.util.tree.allclose","title":"allclose","text":"<pre><code>allclose(tree1: PyTree, tree2: PyTree) -&gt; bool\n</code></pre> <p>Check whether all elements in two PyTrees are approximately equal.</p> <p>Parameters:</p> Name Type Description Default <code>tree1</code> <code>PyTree</code> <p>The first PyTree.</p> required <code>tree2</code> <code>PyTree</code> <p>The second PyTree.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all elements are approximately equal, otherwise False.</p> Source code in <code>laplax/util/tree.py</code> <pre><code>def allclose(tree1: PyTree, tree2: PyTree) -&gt; bool:\n    \"\"\"Check whether all elements in two PyTrees are approximately equal.\n\n    Args:\n        tree1: The first PyTree.\n        tree2: The second PyTree.\n\n    Returns:\n        True if all elements are approximately equal, otherwise False.\n    \"\"\"\n    return jax.tree.all(jax.tree.map(jnp.allclose, tree1, tree2))\n</code></pre>"},{"location":"reference/util/utils/","title":"laplax.util.utils","text":"<p>General utility functions.</p>"},{"location":"reference/util/utils/#laplax.util.utils.identity","title":"identity","text":"<pre><code>identity(x: Any) -&gt; Any\n</code></pre> <p>Identity function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Any input.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The input itself.</p> Source code in <code>laplax/util/utils.py</code> <pre><code>def identity(x: Any) -&gt; Any:\n    \"\"\"Identity function.\n\n    Args:\n        x: Any input.\n\n    Returns:\n        The input itself.\n    \"\"\"\n    return x\n</code></pre>"}]}