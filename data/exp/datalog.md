notes:
runexp_empfisher and runexp_laplax... were merged to run.py 

#### empirical_fisher.json
was generated by runexp_empfisher.py with parameterization:
fisher_fn = emp_fisher_inner, ell=[0, 1e-4, 1e-3, 1e-2, 0.1, 1], batch_sizes=[32, 512], num_repetitions=3

#### type1_fisher_10samples.json
was generated by runexp_empfisher.py with parameterization:
fisher_fn = type1_fisher_inner, ell=[0, 1e-4, 1e-3, 1e-2, 0.1, 1], batch_sizes=[32, 512], num_repetitions=3

#### unscaled_inner.json
was generated by runexp_empfisher.py with parameterization:
fisher_fn = unscaled_dot_product, ell=[0, 1e-5, 1e-4], batch_sizes=[32, 512], num_repetitions=3

#### ggn.json
was generated by (older version) runexp_laplax_different_batchsizes.py with parameterization:
This run used the vanilla dataloading version which was quite slow. Still, i cannot see why the dataloading with
pytorch dataloaders instead of just yielding the batches has (significantly) divergent results.
**Anyways in both runs, 1e-4 < ell < 1e-3 performs best.**
ell=[0, 1e-4, 1e-3, 1e-2, 0.1, 1], batch_sizes=[32, 512], num_repetitions=3


#### ggn_pytorch_dataloading.json
was generated by runexp_laplax_different_batchsizes.py with parameterization:
Also, this was an updated version of "runexp_laplax_different_batchsizes.py" in which we rely on pytorch dataloading.
ells = [0.0, 1e-5, 1e-4, 1e-3]
batch_sizes = [128]
num_repetitions = 5
Clearly 1e-5 < ell < 1e-4 performed best.


